{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNg/PeRSAo7iBG2zEjKnGIB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fabriziobasso/kaggle/blob/main/S3E25_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cleRFSI670J6"
      },
      "source": [
        "# **S3E25 Mohs_Hardness - KAGGLE Competition**\n",
        "\n",
        "Hardness, or the quantitative value of resistance to permanent or plastic deformation, plays a very crucial role in materials design in many applications, such as ceramic coatings and abrasives. Hardness testing is an especially useful method as it is non-destructive and simple to implement to gauge the plastic properties of a material. In this study, I proposed a machine, or statistical, learning approach to predict hardness in naturally occurring materials, which integrates atomic and electronic features from composition directly across a wide variety of mineral compositions and crystal systems. First, atomic and electronic features from the composition, such as van der Waals and covalent radii as well as the number of valence electrons, were extracted from the composition.\n",
        "\n",
        "In this study, the author trained a set of classifiers to understand whether compositional features can be used to predict the Mohs hardness of minerals of different chemical spaces, crystal structures, and crystal classes. The dataset for training and testing the classification models used in this study originated from experimental Mohs hardness data, their crystal classes, and chemical compositions of naturally occurring minerals reported in the Physical and Optical Properties of Minerals CRC Handbook of Chemistry and Physics and the American Mineralogist Crystal Structure Database. The database is composed of 369 uniquely named minerals. Due to the presence of multiple composition combinations for minerals referred to by the same name, the first step was to perform compositional permutations on these minerals. This produced a database of 622 minerals of unique compositions, comprising 210 monoclinic, 96 rhombohedral, 89 hexagonal, 80 tetragonal, 73 cubic, 50 orthorhombic, 22 triclinic, 1 trigonal, and 1 amorphous structure. An independent dataset was compiled to validate the model performance. The validation dataset contains the composition, crystal structure, and Mohs hardness values of 51 synthetic single crystals reported in the literature. The validation dataset includes 15 monoclinic, 7 tetragonal, 7 hexagonal, 6 orthorhombic, 4 cubic, and 3 rhombohedral crystal structures.\n",
        "\n",
        "In this study, the author constructed a database of compositional feature descriptors that characterize naturally occurring materials obtained directly from the Physical and Optical Properties of Minerals CRC Handbook45. This comprehensive compositional-based dataset allows us to train models that are able to predict hardness across a wide variety of mineral compositions and crystal classes. Each material in both the naturally occurring mineral and artificial single crystal datasets was represented by 11 atomic descriptors. The elemental features are the number of electrons, number of valence electrons, atomic number, Pauling electronegativity of the most common oxidation state, covalent atomic radii, van der Waals radii, and ionization energy of neutral.\n",
        "\n",
        "#### **Files:**\n",
        "* train.csv - the training dataset; Hardness is the continuous target\n",
        "* test.csv - the test dataset; your objective is to predict the value of Hardness\n",
        "* sample_submission.csv - a sample submission file in the correct format\n",
        "* Mineral_Dataset_Supplementary_Info.csv - Original Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_tuNI_58oDU"
      },
      "source": [
        "## 1.0 Workbook Set-up and Libraries:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiXajJZIpDRW"
      },
      "source": [
        "#### 1.0 Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4dbXrH095unC"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install tensorflow-addons\n",
        "#!pip install shap\n",
        "#!pip install eli5\n",
        "#!pip install tf-nightly\n",
        "#!pip install -U scikit-learn==1.2.0\n",
        "#!pip install catboost\n",
        "#!pip install haversine\n",
        "#!pip install pytorch-forecasting\n",
        "!pip install umap-learn\n",
        "#!pip install reverse_geocoder\n",
        "#!pip install --upgrade protobuf\n",
        "!pip install colorama\n",
        "!pip install imbalanced-learn\n",
        "!pip install optuna\n",
        "!pip install optuna-integration\n",
        "#!pip install pygam\n",
        "!pip install keras-tuner --upgrade\n",
        "#!pip install pycaret\n",
        "#!pip install lightning==2.0.1\n",
        "!pip install keras-nlp\n",
        "#!pip install MiniSom"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "81y_MZuE8zTt",
        "outputId": "ea829c2d-c834-4943-dbe9-2171e4fc1b98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Importing started...\n",
            "Using TensorFlow backend\n",
            "Done, All the required modules are imported. Time elapsed: 15.327619791030884 sec\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#importing modules\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import time\n",
        "t = time.time()\n",
        "\n",
        "print('Importing started...')\n",
        "\n",
        "# basic moduele\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "#from scipy import stats\n",
        "from random import randint\n",
        "import random\n",
        "import math\n",
        "import os\n",
        "import gc\n",
        "import pickle\n",
        "from glob import glob\n",
        "from IPython import display as ipd\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "from joblib import dump, load\n",
        "import sklearn as sk\n",
        "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
        "from functools import partial\n",
        "import itertools\n",
        "import joblib\n",
        "from itertools import combinations\n",
        "import IPython\n",
        "import statsmodels.api as sm\n",
        "import IPython.display\n",
        "\n",
        "# visualization moduels\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import matplotlib.gridspec as gridspec\n",
        "import matplotlib.patches as mpatches\n",
        "from matplotlib_venn import venn2_unweighted\n",
        "import seaborn as sns\n",
        "import missingno as msno\n",
        "import imblearn\n",
        "import scipy.stats as stats\n",
        "from scipy.special import boxcox, boxcox1p\n",
        "\n",
        "\n",
        "# Palette Setup\n",
        "colors = ['#FB5B68','#FFEB48','#2676A1','#FFBDB0',]\n",
        "colormap_0 = mpl.colors.LinearSegmentedColormap.from_list(\"\",colors)\n",
        "palette_1 = sns.color_palette(\"coolwarm\", as_cmap=True)\n",
        "palette_2 = sns.color_palette(\"YlOrBr\", as_cmap=True)\n",
        "palette_3 = sns.light_palette(\"red\", as_cmap=True)\n",
        "palette_4 = sns.color_palette(\"viridis\", as_cmap=True)\n",
        "palette_5 = sns.color_palette(\"rocket\", as_cmap=True)\n",
        "palette_6 = sns.color_palette(\"GnBu\", as_cmap=True)\n",
        "palette_7 = sns.color_palette(\"tab20c\", as_cmap=False)\n",
        "palette_8 = sns.color_palette(\"Set2\", as_cmap=False)\n",
        "\n",
        "palette_custom = ['#fbb4ae','#b3cde3','#ccebc5','#decbe4','#fed9a6','#ffffcc','#e5d8bd','#fddaec','#f2f2f2']\n",
        "palette_9 = sns.color_palette(palette_custom, as_cmap=False)\n",
        "\n",
        "sns.set_style(\"whitegrid\",{\"grid.linestyle\":\"--\", 'grid.linewidth':0.2, 'grid.alpha':0.5})\n",
        "#sns.set_theme(style=\"ticks\", context=\"notebook\")\n",
        "sns.despine(left=True, bottom=True, top=False, right=False)\n",
        "\n",
        "mpl.rcParams['axes.spines.left'] = True\n",
        "mpl.rcParams['axes.spines.right'] = False\n",
        "mpl.rcParams['axes.spines.top'] = False\n",
        "mpl.rcParams['axes.spines.bottom'] = True\n",
        "\n",
        "# Style Import\n",
        "from colorama import Style, Fore\n",
        "red = Style.BRIGHT + Fore.RED\n",
        "blu = Style.BRIGHT + Fore.BLUE\n",
        "mgt = Style.BRIGHT + Fore.MAGENTA\n",
        "gld = Style.BRIGHT + Fore.YELLOW\n",
        "res = Style.RESET_ALL\n",
        "\n",
        "# preprocessing modules\n",
        "from sklearn.model_selection import (train_test_split,\n",
        "                                     KFold,\n",
        "                                     StratifiedKFold,\n",
        "                                     cross_val_score,\n",
        "                                     GroupKFold,\n",
        "                                     GridSearchCV,\n",
        "                                     RepeatedStratifiedKFold)\n",
        "\n",
        "from sklearn.preprocessing import (LabelEncoder,\n",
        "                                   StandardScaler,\n",
        "                                   MinMaxScaler,\n",
        "                                   OrdinalEncoder,\n",
        "                                   RobustScaler,\n",
        "                                   PowerTransformer,\n",
        "                                   OneHotEncoder,\n",
        "                                   LabelEncoder,\n",
        "                                   OrdinalEncoder,\n",
        "                                   PolynomialFeatures)\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "\n",
        "\n",
        "# metrics\n",
        "from sklearn.metrics import (mean_squared_error,\n",
        "                             r2_score,\n",
        "                             mean_absolute_error,\n",
        "                             mean_absolute_percentage_error,\n",
        "                             classification_report,\n",
        "                             confusion_matrix,\n",
        "                             ConfusionMatrixDisplay,\n",
        "                             multilabel_confusion_matrix,\n",
        "                             accuracy_score,\n",
        "                             roc_auc_score,\n",
        "                             auc,\n",
        "                             roc_curve,\n",
        "                             log_loss)\n",
        "\n",
        "\n",
        "# modeling algos\n",
        "from sklearn.linear_model import (LogisticRegression,\n",
        "                                  Lasso,\n",
        "                                  ridge_regression,\n",
        "                                  LinearRegression,\n",
        "                                  Ridge,\n",
        "                                  RidgeCV,\n",
        "                                  ElasticNet,\n",
        "                                  BayesianRidge,\n",
        "                                  TweedieRegressor,\n",
        "                                  ARDRegression,\n",
        "                                  PoissonRegressor,\n",
        "                                  GammaRegressor)\n",
        "\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.isotonic import IsotonicRegression\n",
        "\n",
        "from sklearn.ensemble import (AdaBoostRegressor,\n",
        "                              RandomForestRegressor,\n",
        "                              RandomForestClassifier,\n",
        "                              VotingRegressor,\n",
        "                              GradientBoostingRegressor,\n",
        "                              StackingRegressor,\n",
        "                              HistGradientBoostingClassifier,\n",
        "                              ExtraTreesClassifier)\n",
        "\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "# Other Models\n",
        "#from pygam import LogisticGAM, s, te\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBRegressor, XGBClassifier\n",
        "import lightgbm as lgb\n",
        "from lightgbm import (LGBMRegressor,\n",
        "                      LGBMClassifier,\n",
        "                      early_stopping,\n",
        "                      record_evaluation,\n",
        "                      log_evaluation)\n",
        "\n",
        "#import catboost as cat\n",
        "#from catboost import CatBoost, CatBoostRegressor\n",
        "#from catboost import CatBoostClassifier\n",
        "\n",
        "#from catboost.utils import get_roc_curve\n",
        "\n",
        "from lightgbm import early_stopping\n",
        "# check installed version\n",
        "#import pycaret\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "#from minisom import MiniSom\n",
        "\n",
        "from sklearn.base import clone ## sklearn base models for stacked ensemble model\n",
        "from sklearn.calibration import CalibratedClassifierCV, CalibrationDisplay\n",
        "\n",
        "#Interpretiability of the model\n",
        "#import shap\n",
        "#import eli5\n",
        "#from eli5.sklearn import PermutationImportance\n",
        "\n",
        "\n",
        "## miss\n",
        "from sklearn.pipeline import (make_pipeline,\n",
        "                              Pipeline)\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow.keras.backend as K\n",
        "import tensorflow_addons as tfa\n",
        "from keras.utils import FeatureSpace\n",
        "import keras_nlp\n",
        "\n",
        "# Import libraries for Hypertuning\n",
        "import kerastuner as kt\n",
        "from kerastuner.tuners import RandomSearch, GridSearch, BayesianOptimization\n",
        "# Model Tuning tools:\n",
        "import optuna\n",
        "from optuna.integration import TFKerasPruningCallback\n",
        "from optuna.trial import TrialState\n",
        "from optuna.visualization import plot_intermediate_values\n",
        "from optuna.visualization import plot_optimization_history\n",
        "from optuna.visualization import plot_param_importances\n",
        "from optuna.visualization import plot_contour\n",
        "%matplotlib inline\n",
        "SEED = 1984\n",
        "N_SPLITS = 10\n",
        "\n",
        "print('Done, All the required modules are imported. Time elapsed: {} sec'.format(time.time()-t))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXoRiAxUiOlO",
        "outputId": "c48afca3-8144-45b8-ca4c-f0e67d63d379"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CHECK VERSIONS:\n",
            "sns: 0.12.2\n",
            "mpl: 3.7.1\n",
            "tensorflow: 2.15.0\n",
            "pandas: 1.5.3\n",
            "numpy: 1.23.5\n",
            "scikit-learn: 1.2.2\n",
            "statsmodels: 0.14.0\n",
            "missingno: 0.5.2\n",
            "Inbalance_Learning: 0.10.1\n",
            "XGBoost: 2.0.2\n"
          ]
        }
      ],
      "source": [
        "# Check Versions:\n",
        "print(\"CHECK VERSIONS:\")\n",
        "print(f\"sns: {sns.__version__}\")\n",
        "print(f\"mpl: {mpl.__version__}\")\n",
        "print(f\"tensorflow: {tf.__version__}\")\n",
        "print(f\"pandas: {pd.__version__}\")\n",
        "print(f\"numpy: {np.__version__}\")\n",
        "print(f\"scikit-learn: {sk.__version__}\")\n",
        "print(f\"statsmodels: {sm.__version__}\")\n",
        "print(f\"missingno: {msno.__version__}\")\n",
        "#print(f\"TF-addon: {tfa.__version__}\")\n",
        "print(f\"Inbalance_Learning: {imblearn.__version__}\")\n",
        "print(f\"XGBoost: {xgb.__version__}\")\n",
        "#print(f\"CatBoost: {cat.__version__}\")\n",
        "#print(f\"PyCaret: {pycaret.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIHRomd6iXg5"
      },
      "source": [
        "### **1.1 Utility Functions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umgqGNosjTMi"
      },
      "source": [
        "#### Graph Functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AMxcvC4qiSCK"
      },
      "outputs": [],
      "source": [
        "def summary(df):\n",
        "    print(f'data shape: {df.shape}')\n",
        "    summ = pd.DataFrame(df.dtypes, columns=['data type'])\n",
        "    summ['#missing'] = df.isnull().sum().values\n",
        "    summ['%missing'] = df.isnull().sum().values / len(df)* 100\n",
        "    summ['#unique'] = df.nunique().values\n",
        "    desc = pd.DataFrame(df.describe(include='all').transpose())\n",
        "    summ['min'] = desc['min'].values\n",
        "    summ['max'] = desc['max'].values\n",
        "    summ['median'] = desc['50%'].values\n",
        "    return summ\n",
        "\n",
        "\n",
        "def scatter_plot_high_corr_pair(data_set=None, target=None, variable_pairs=None):\n",
        "    num_pairs = len(variable_pairs)\n",
        "\n",
        "    num_rows = (num_pairs + 2) // 3\n",
        "    num_cols = min(num_pairs, 3)\n",
        "\n",
        "    fig = plt.figure(figsize=(15, 5 * num_rows))\n",
        "    spec = gridspec.GridSpec(num_rows, num_cols, width_ratios=[1]*num_cols, height_ratios=[1]*num_rows)\n",
        "\n",
        "    for i, pair in enumerate(variable_pairs):\n",
        "        x_var, y_var = pair\n",
        "        row_index = i // 3\n",
        "        col_index = i % 3\n",
        "\n",
        "        ax = plt.subplot(spec[row_index, col_index])\n",
        "\n",
        "        sns.scatterplot(x=x_var, y=y_var, hue=target, data=data_set, palette='viridis', ax=ax, s=10)\n",
        "        sns.regplot(x=x_var, y=y_var, data=data_set, scatter=False, color='#FA7F6F', ax=ax)\n",
        "\n",
        "        ax.set_title(f'{x_var} vs {y_var}')\n",
        "        ax.set_xlabel(x_var)\n",
        "        ax.set_ylabel(y_var)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def scatter_corr_plot(data_set=None, variables=None, target=None):\n",
        "    if data_set is None or variables is None or target is None:\n",
        "        raise ValueError(\"Please provide data_set and variables.\")\n",
        "\n",
        "    target = 'Hardness'\n",
        "\n",
        "    full_features = data_set.columns.tolist()\n",
        "    num_var = [column for column in data_set.columns if data_set[column].nunique() > 10]\n",
        "    cat_var = [column for column in data_set.columns if data_set[column].nunique() < 10]\n",
        "\n",
        "\n",
        "    num_rows = len(num_var)\n",
        "    num_cols = 3\n",
        "\n",
        "    total_plots = num_rows*num_cols\n",
        "    plt.figure(figsize=(14,num_rows*2.5))\n",
        "\n",
        "    for idx, col in enumerate(num_var):\n",
        "        plt.subplot(num_rows, num_cols, idx % total_plots + 1)\n",
        "        sns.scatterplot(x=col, y=target, data=data_set, color='#82B0D2',s=10)\n",
        "\n",
        "        sns.regplot(x=col, y=target, data=data_set, scatter=False, color='#FA7F6F')\n",
        "\n",
        "        corr_coef = data_set[target].corr(data_set[col])\n",
        "        plt.text(0.95, 0.95, f\"Corr: {corr_coef:.2f}\", transform=plt.gca().transAxes, ha='right', va='top')\n",
        "        mean_value = data_set[col].mean()\n",
        "\n",
        "        plt.axvline(x=mean_value, color='black', linestyle='--', linewidth=1, label='Mean Value')\n",
        "        plt.title(f\"{col} : {target}\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def correlation_plot(data_train=None, data_test=None, variables=None):\n",
        "    if data_train is None or data_test is None or variables is None:\n",
        "        raise ValueError(\"Please provide data_train, data_test and variables.\")\n",
        "    corr_matrix_train = data_train[variables].corr()\n",
        "    corr_matrix_test = data_test[variables].corr()\n",
        "\n",
        "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16, 10), gridspec_kw={'width_ratios': [0.80, 1.0]})\n",
        "    sns.heatmap(corr_matrix_train, annot=True, cmap=palette_1, fmt='.2f', linewidths=1, square=True, annot_kws={\"size\": 9}, ax=axes[0], cbar = False)\n",
        "    axes[0].set_title('Train Set', fontsize=15)\n",
        "\n",
        "    sns.heatmap(corr_matrix_test, annot=True, cmap=palette_1, fmt='.2f', linewidths=1, square=True, annot_kws={\"size\": 9}, ax=axes[1], yticklabels=False, cbar = True, cbar_kws={\"shrink\": .50})\n",
        "    axes[1].set_title('Test Set', fontsize=15)\n",
        "\n",
        "    plt.suptitle('Correlation Matrices of Train and Test Sets', fontsize=25, weight = 'bold',y=0.9)\n",
        "    for ax in axes:\n",
        "        ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha='right')\n",
        "    plt.subplots_adjust(top=1)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def scatter_plot_high_corr_pair(data_set=None, target=None, variable_pairs=None):\n",
        "    num_pairs = len(variable_pairs)\n",
        "\n",
        "    num_rows = (num_pairs + 2) // 3\n",
        "    num_cols = min(num_pairs, 3)\n",
        "\n",
        "    fig = plt.figure(figsize=(10, 3.5 * num_rows))\n",
        "    spec = gridspec.GridSpec(num_rows, num_cols, width_ratios=[1]*num_cols, height_ratios=[1]*num_rows)\n",
        "\n",
        "    for i, pair in enumerate(variable_pairs):\n",
        "        x_var, y_var = pair\n",
        "        row_index = i // 3\n",
        "        col_index = i % 3\n",
        "\n",
        "        ax = plt.subplot(spec[row_index, col_index])\n",
        "\n",
        "        sns.scatterplot(x=x_var, y=y_var, hue=target, data=data_set, palette=palette_1, ax=ax, s=10)\n",
        "        sns.regplot(x=x_var, y=y_var, data=data_set, scatter=False, color='#FA7F6F', ax=ax)\n",
        "\n",
        "        ax.set_title(f'{x_var} vs {y_var}', fontsize=10)\n",
        "        ax.set_xlabel(x_var, fontsize=7)\n",
        "        ax.set_ylabel(y_var, fontsize=7)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_boxplot_and_hist(data, variable):\n",
        "   #figure composed of two matplotlib.Axes objects (ax_box and ax_hist)\n",
        "   f, (ax_box, ax_hist) = plt.subplots( 2, 1, sharex =False, gridspec_kw ={\"height_ratios\": (0.50, 0.85)}, figsize=(8,4))\n",
        "   # assigning a graph to each ax\n",
        "   sns.boxplot( x = data[variable], ax = ax_box)\n",
        "   sns.histplot( data = data, x = variable, ax = ax_hist, bins=40)\n",
        "   # Remove x axis name for the boxplot\n",
        "   ax_box.set( xlabel ='')\n",
        "   ax_box.set_title(variable)\n",
        "   plt.tight_layout()\n",
        "\n",
        "\n",
        "def diagnostic_plots( df, variable):\n",
        "  plt.figure( figsize =(8,3))\n",
        "  plt.subplot( 1, 2, 1)\n",
        "  df[variable].hist( bins = 30)\n",
        "  plt.title( f\" Histogram of {variable}\")\n",
        "  plt.subplot( 1, 2, 2)\n",
        "  stats.probplot( df[ variable], dist =\"norm\", plot = plt)\n",
        "  plt.title( f\" Q-Q plot of {variable}\")\n",
        "  plt.show()\n",
        "  plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOo7obahqmNZ"
      },
      "source": [
        "#### Data Analysis Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "11kVuDgbqw5p"
      },
      "outputs": [],
      "source": [
        "def adversarial_validation(data_train, data_test, target='Hardness',is_train_test=\"True\"):\n",
        "\n",
        "    adv_train = data_train.drop(target, axis = 1)\n",
        "    adv_test = data_test.copy()\n",
        "    if is_train_test==False:\n",
        "      adv_test = adv_test.drop(target, axis = 1)\n",
        "\n",
        "    adv_train['is_test'] = 0\n",
        "    adv_test['is_test'] = 1\n",
        "    adv = pd.concat([adv_train, adv_test], ignore_index = True)\n",
        "    adv_shuffled = adv.sample(frac = 1)\n",
        "\n",
        "    adv_X = adv_shuffled.drop('is_test', axis = 1)\n",
        "    adv_y = adv_shuffled['is_test']\n",
        "\n",
        "    skf = StratifiedKFold(n_splits = 5, random_state = 42, shuffle = True)\n",
        "\n",
        "    val_scores = []\n",
        "    predictions = np.zeros(len(adv))\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(adv_X, adv_y)):\n",
        "        adv_lr = XGBClassifier(random_state = 42)\n",
        "        adv_lr.fit(adv_X.iloc[train_idx], adv_y.iloc[train_idx])\n",
        "\n",
        "        val_preds = adv_lr.predict_proba(adv_X.iloc[val_idx])[:,1]\n",
        "        predictions[val_idx] = val_preds\n",
        "        val_score = roc_auc_score(adv_y.iloc[val_idx], val_preds)\n",
        "        val_scores.append(val_score)\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(adv['is_test'], predictions)\n",
        "\n",
        "    plt.figure(figsize = (6, 6))\n",
        "    sns.lineplot(x=[0, 1], y=[0, 1], linestyle=\"--\", label=\"Indistinguishable Datasets\")\n",
        "    sns.lineplot(x=fpr, y=tpr, label=\"Adversarial Validation Classifier\")\n",
        "    plt.title(f'Train-Test Validation = {np.mean(val_scores):.5f}', weight = 'bold', size = 17)\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.show()\n",
        "\n",
        "def search_high_corr_pairs(data_set = None, variables =None, threshold = 0.7):\n",
        "    corr_matrix=data_set[variables].corr()\n",
        "    high_corr_pairs=[]\n",
        "\n",
        "    for i in range(len(corr_matrix.columns)):\n",
        "        for j in range(i+1,len(corr_matrix.columns)):\n",
        "            if abs(corr_matrix.iloc[i,j]) > threshold:\n",
        "                print(f'\\033[91m{corr_matrix.columns[i]}\\033[0m and \\033[91m{corr_matrix.columns[j]}\\033[0m are highly linearly correlated, indicating the possibility of collinearity.')\n",
        "                high_corr_pairs.append((corr_matrix.columns[i],corr_matrix.columns[j]))\n",
        "    return high_corr_pairs\n",
        "\n",
        "\n",
        "def transform_features(train,test):\n",
        "  train_ = train.copy()\n",
        "  test_ = test.copy()\n",
        "\n",
        "  feat_log_transf = [\"allelectrons_Total\",\"density_Total\",\"allelectrons_Average\",\"atomicweight_Average\",\n",
        "                     \"ionenergy_Average\",\"zaratio_Average\",\"density_Average\"]\n",
        "\n",
        "  feat_power_transf = [\"val_e_Average\",\"R_vdw_element_Average\",\"el_neg_chi_Average\"]\n",
        "\n",
        "  log_transformer = FunctionTransformer(lambda x: np.log(x+1.0))\n",
        "  power_transformer = FunctionTransformer(lambda x: np.power(x,2.0))\n",
        "\n",
        "  for feat in feat_log_transf:\n",
        "    train_[feat] = log_transformer.transform(train_[feat])\n",
        "    test_[feat] = log_transformer.transform(test_[feat])\n",
        "\n",
        "  for feat in feat_power_transf:\n",
        "    train_[feat] = power_transformer.transform(train_[feat])\n",
        "    test_[feat] = power_transformer.transform(test_[feat])\n",
        "\n",
        "  return train_, test_\n",
        "\n",
        "\n",
        "def replace_zeros(train,test):\n",
        "  train_ = train.copy()\n",
        "  test_ = test.copy()\n",
        "\n",
        "  train_.replace({0:np.nan},inplace=True)\n",
        "  test_.replace({0:np.nan},inplace=True)\n",
        "\n",
        "  train_[\"nan_marker\"] = train_.isna().any(axis=1).astype(\"int\")\n",
        "  test_[\"nan_marker\"] = test_.isna().any(axis=1).astype(\"int\")\n",
        "\n",
        "  total = pd.concat([train_, test_], ignore_index=True)\n",
        "  total.drop(\"Hardness\",axis=1,inplace=True)\n",
        "\n",
        "  total_col = list(total.columns)\n",
        "\n",
        "  imputer = IterativeImputer(estimator=BayesianRidge(), max_iter=50, random_state = 0)\n",
        "  imputer.fit(total[total_col])\n",
        "  train_[total_col] = imputer.transform(train_[total_col])\n",
        "  test_[total_col] = imputer.transform(test_[total_col])\n",
        "\n",
        "  return train_, test_\n",
        "\n",
        "def clip_values(train,test):\n",
        "  train_ = train.copy()\n",
        "  test_ = test.copy()\n",
        "\n",
        "  train_[\"allelectrons_Total\"] = train_[\"allelectrons_Total\"].clip(lower=1.9, upper=None, axis=None, inplace=False)\n",
        "  test_[\"allelectrons_Total\"] = test_[\"allelectrons_Total\"].clip(lower=1.9, upper=None, axis=None, inplace=False)\n",
        "\n",
        "  train_[\"allelectrons_Average\"] = train_[\"allelectrons_Average\"].clip(lower=1.7, upper=None, axis=None, inplace=False)\n",
        "  test_[\"allelectrons_Average\"] = test_[\"allelectrons_Average\"].clip(lower=1.7, upper=None, axis=None, inplace=False)\n",
        "\n",
        "  train_[\"val_e_Average\"] = train_[\"val_e_Average\"].clip(lower=2.0, upper=None, axis=None, inplace=False)\n",
        "  test_[\"val_e_Average\"] = test_[\"val_e_Average\"].clip(lower=2.0, upper=None, axis=None, inplace=False)\n",
        "\n",
        "  train_[\"atomicweight_Average\"] = train_[\"atomicweight_Average\"].clip(lower=2.0, upper=None, axis=None, inplace=False)\n",
        "  test_[\"atomicweight_Average\"] = test_[\"atomicweight_Average\"].clip(lower=2.0, upper=None, axis=None, inplace=False)\n",
        "\n",
        "  train_[\"density_Total\"] = train_[\"density_Total\"].clip(lower=0.01, upper=None, axis=None, inplace=False)\n",
        "  test_[\"density_Total\"] = test_[\"density_Total\"].clip(lower=0.01, upper=None, axis=None, inplace=False)\n",
        "\n",
        "  train_[\"el_neg_chi_Average\"] = train_[\"el_neg_chi_Average\"].clip(lower=1.0, upper=None, axis=None, inplace=False)\n",
        "  test_[\"el_neg_chi_Average\"] = test_[\"el_neg_chi_Average\"].clip(lower=1.0, upper=None, axis=None, inplace=False)\n",
        "\n",
        "  train_[\"R_vdw_element_Average\"] = train_[\"R_vdw_element_Average\"].clip(lower=1.2, upper=None, axis=None, inplace=False)\n",
        "  test_[\"R_vdw_element_Average\"] = test_[\"R_vdw_element_Average\"].clip(lower=1.2, upper=None, axis=None, inplace=False)\n",
        "\n",
        "  train_[\"R_cov_element_Average\"] = train_[\"R_cov_element_Average\"].clip(lower=0.2,upper=2.0,axis=None,inplace=False)\n",
        "  test_[\"R_cov_element_Average\"] = test_[\"R_cov_element_Average\"].clip(lower=0.2,upper=2.0,axis=None,inplace=False)\n",
        "\n",
        "  train_[\"zaratio_Average\"] = train_[\"zaratio_Average\"].clip(lower=0.3,upper=0.60, axis=None, inplace=False)\n",
        "  test_[\"zaratio_Average\"] = test_[\"zaratio_Average\"].clip(lower=0.3,upper=0.60, axis=None, inplace=False)\n",
        "\n",
        "  train_[\"density_Average\"] = train_[\"density_Average\"].clip(lower=0.01,upper=None, axis=None, inplace=False)\n",
        "  test_[\"density_Average\"] = test_[\"density_Average\"].clip(lower=0.01,upper=None, axis=None, inplace=False)\n",
        "\n",
        "  train_[\"ionenergy_Average\"] = train_[\"ionenergy_Average\"].clip(lower=2.1,upper=None, axis=None, inplace=False)\n",
        "  test_[\"ionenergy_Average\"] = test_[\"ionenergy_Average\"].clip(lower=2.1,upper=None, axis=None, inplace=False)\n",
        "\n",
        "\n",
        "  return train_, test_\n",
        "\n",
        "def prepare_dataset(train,test):\n",
        "  train_ = train.copy()\n",
        "  test_ = test.copy()\n",
        "\n",
        "  train_,test_ = transform_features(train_,test_)\n",
        "  train_,test_ = replace_zeros(train_,test_)\n",
        "  train_,test_ = clip_values(train_,test_)\n",
        "\n",
        "  return train_, test_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STsF4sfaDxbj"
      },
      "source": [
        "### **1.2 Connect Drives**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nucekKRDxbj",
        "outputId": "02ce5e88-9666-44de-dc67-346634da3369"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xwI9dKRDxbj"
      },
      "source": [
        "Connect to Google Drive:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "XnZHOmpDDxbj"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Connect to Colab:\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_vZlWHwDxbj",
        "outputId": "36bee34b-9be3-43c4-daa3-57d12c9ff74b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S3E25_Mohs_Hardness already exists\n",
            "/content/drive/MyDrive/Exercises/Studies_Structured_Data/Models/S3E25_Mohs_Hardness already exists\n",
            "/content/drive/MyDrive/Exercises/Studies_Structured_Data/Models/S3E25_Mohs_Hardness/neural_networks/ already exists\n"
          ]
        }
      ],
      "source": [
        "folder_data = \"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S3E25_Mohs_Hardness\"\n",
        "models_folders = \"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Models/S3E25_Mohs_Hardness\"\n",
        "folders_nn = \"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Models/S3E25_Mohs_Hardness/neural_networks/\"\n",
        "\n",
        "list_directories = [folder_data,models_folders,folders_nn]\n",
        "\n",
        "for path in list_directories:\n",
        "  try:\n",
        "      os.mkdir(path)\n",
        "  except OSError as error:\n",
        "      print(f\"{path} already exists\")\n",
        "\n",
        "\n",
        "os.chdir(folder_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.0 Upload Data"
      ],
      "metadata": {
        "id": "h1lby8JzbZjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv(\"train_final.csv\",index_col=0)\n",
        "test = pd.read_csv(\"test_final.csv\",index_col=0)\n",
        "\n",
        "float_cols = train.select_dtypes(\"float\").columns\n",
        "int_cols = train.select_dtypes(\"int\").columns\n",
        "\n",
        "train[float_cols]=train[float_cols].astype(\"float32\")\n",
        "train[int_cols]=train[int_cols].astype(\"int32\")\n",
        "\n",
        "test[float_cols]=test[float_cols].astype(\"float32\")\n",
        "test[int_cols]=test[int_cols].astype(\"int32\")"
      ],
      "metadata": {
        "id": "2nLaTB9kbQyy"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2tGDG-AbQ2B",
        "outputId": "36dce29f-98d0-4aa3-cfa1-e2b5ad335f56"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 11006 entries, 0 to 11005\n",
            "Data columns (total 25 columns):\n",
            " #   Column                     Non-Null Count  Dtype  \n",
            "---  ------                     --------------  -----  \n",
            " 0   allelectrons_Total         11006 non-null  float32\n",
            " 1   density_Total              11006 non-null  float32\n",
            " 2   allelectrons_Average       11006 non-null  float32\n",
            " 3   val_e_Average              11006 non-null  float32\n",
            " 4   ionenergy_Average          11006 non-null  float32\n",
            " 5   el_neg_chi_Average         11006 non-null  float32\n",
            " 6   R_vdw_element_Average      11006 non-null  float32\n",
            " 7   R_cov_element_Average      11006 non-null  float32\n",
            " 8   zaratio_Average            11006 non-null  float32\n",
            " 9   density_Average            11006 non-null  float32\n",
            " 10  Hardness                   11006 non-null  float32\n",
            " 11  nan_marker                 11006 non-null  int32  \n",
            " 12  awa_cat                    11006 non-null  int32  \n",
            " 13  grouped_atomicweight       11006 non-null  int32  \n",
            " 14  density_per_valence        11006 non-null  float32\n",
            " 15  combined_feature           11006 non-null  float32\n",
            " 16  atomicweight_density_diff  11006 non-null  float32\n",
            " 17  tree_feat_v57              11006 non-null  float32\n",
            " 18  tree_feat_v84              11006 non-null  float32\n",
            " 19  tree_feat_v3               11006 non-null  float32\n",
            " 20  tree_feat_v60              11006 non-null  float32\n",
            " 21  tree_feat_v33              11006 non-null  float32\n",
            " 22  tree_feat_v94              11006 non-null  float32\n",
            " 23  tree_feat_v56              11006 non-null  float32\n",
            " 24  tree_feat_v58              11006 non-null  float32\n",
            "dtypes: float32(22), int32(3)\n",
            "memory usage: 1.1 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.0 Data Loading"
      ],
      "metadata": {
        "id": "Z-YvbZVmiS35"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dataframe_to_dataset(dataframe, shuffle=False, batch_size=32):\n",
        "    dataframe = dataframe.copy()\n",
        "    labels = dataframe.pop(\"Hardness\")\n",
        "    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
        "    if shuffle:\n",
        "      ds = ds.shuffle(buffer_size=len(dataframe))\n",
        "    ds = ds.batch(batch_size)\n",
        "    ds = ds.prefetch(batch_size)\n",
        "    return ds"
      ],
      "metadata": {
        "id": "ilPD9km6bQtR"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = dataframe_to_dataset(train)\n",
        "test_ds = dataframe_to_dataset(test)"
      ],
      "metadata": {
        "id": "CjtV_DKdbQp_"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x, y in dataframe_to_dataset(train, batch_size=1).take(1):\n",
        "    print(f\"Input: {x}\")\n",
        "    print(f\"Target: {y}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JUtuM_ybQnR",
        "outputId": "40114e2a-b979-43d4-99f5-dc722b7db290"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: {'allelectrons_Total': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([4.1743875], dtype=float32)>, 'density_Total': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([2.358776], dtype=float32)>, 'allelectrons_Average': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([2.76001], dtype=float32)>, 'val_e_Average': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([23.04], dtype=float32)>, 'ionenergy_Average': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([2.4646292], dtype=float32)>, 'el_neg_chi_Average': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([6.990736], dtype=float32)>, 'R_vdw_element_Average': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([3.013696], dtype=float32)>, 'R_cov_element_Average': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.928], dtype=float32)>, 'zaratio_Average': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.3456089], dtype=float32)>, 'density_Average': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.0718952], dtype=float32)>, 'nan_marker': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, 'awa_cat': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([3], dtype=int32)>, 'grouped_atomicweight': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, 'density_per_valence': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.6124908], dtype=float32)>, 'combined_feature': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.8724668], dtype=float32)>, 'atomicweight_density_diff': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.6731774], dtype=float32)>, 'tree_feat_v57': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([5.675158], dtype=float32)>, 'tree_feat_v84': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([5.4419246], dtype=float32)>, 'tree_feat_v3': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([3.1333334], dtype=float32)>, 'tree_feat_v60': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([5.1926827], dtype=float32)>, 'tree_feat_v33': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([5.3754153], dtype=float32)>, 'tree_feat_v94': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([5.7042108], dtype=float32)>, 'tree_feat_v56': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([5.32505], dtype=float32)>, 'tree_feat_v58': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([5.3026023], dtype=float32)>}\n",
            "Target: [2.5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds_with_no_labels = train_ds.map(lambda x, _: x)\n",
        "\n",
        "\n",
        "def example_feature_space(dataset, feature_space, feature_names):\n",
        "    feature_space.adapt(dataset)\n",
        "    for x in dataset.take(1):\n",
        "        inputs = {feature_name: x[feature_name] for feature_name in feature_names}\n",
        "        preprocessed_x = feature_space(inputs)\n",
        "        print(f\"Input: {[{k:v.numpy()} for k, v in inputs.items()]}\")\n",
        "        print(\n",
        "            f\"Preprocessed output: {[{k:v.numpy()} for k, v in preprocessed_x.items()]}\"\n",
        "        )"
      ],
      "metadata": {
        "id": "bySF_HXLbQkg"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.awa_cat.unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJo1M72a2cxJ",
        "outputId": "5e4e0519-48d5-4c5e-a6c2-16041a3d1b24"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 3, 2, 4, 0, 5], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_space = FeatureSpace(\n",
        "    features={\n",
        "        \"awa_cat\": FeatureSpace.integer_hashed(num_bins=6, output_mode=\"one_hot\")\n",
        "    },\n",
        "    output_mode=\"dict\",\n",
        ")\n",
        "example_feature_space(train_ds_with_no_labels, feature_space, [\"awa_cat\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-GliHYqbQhn",
        "outputId": "d6863d69-eaea-45fe-a027-d9376c9aa555"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: [{'awa_cat': 0}]\n",
            "Preprocessed output: [{'awa_cat': array([0., 1., 0., 0., 0., 0.], dtype=float32)}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_space = FeatureSpace(\n",
        "    features={\n",
        "        \"awa_cat\": FeatureSpace.integer_categorical(num_oov_indices=0, output_mode=\"one_hot\"),\n",
        "        \"grouped_atomicweight\": FeatureSpace.integer_categorical(num_oov_indices=0, output_mode=\"one_hot\")\n",
        "    },\n",
        "    crosses=[\n",
        "        FeatureSpace.cross(feature_names=(\"awa_cat\", \"grouped_atomicweight\"), crossing_dim=8)],\n",
        "    output_mode=\"dict\",\n",
        ")\n",
        "example_feature_space(train_ds_with_no_labels, feature_space, [\"awa_cat\",\"grouped_atomicweight\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJbBo-HGbQev",
        "outputId": "9a7e0985-5c1e-4801-8140-3d246f894a89"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: [{'awa_cat': 1}, {'grouped_atomicweight': 0}]\n",
            "Preprocessed output: [{'awa_cat': array([0., 0., 0., 1., 0., 0.], dtype=float32)}, {'grouped_atomicweight': array([1., 0.], dtype=float32)}, {'awa_cat_X_grouped_atomicweight': array([1., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "float_feat = list(float_cols)\n",
        "int_feat = list(int_cols)\n",
        "float_feat.remove(\"Hardness\")"
      ],
      "metadata": {
        "id": "CxQvY2-FbQY5"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_space_dict = FeatureSpace(\n",
        "                                  features={**{a:FeatureSpace.integer_categorical(num_oov_indices=0, output_mode=\"one_hot\") for a in int_feat},**{a:FeatureSpace.float_normalized() for a in float_feat}},\n",
        "                                  # Specify feature cross with a custom crossing dim.\n",
        "                                  crosses=[FeatureSpace.cross(feature_names=(\"awa_cat\",\"grouped_atomicweight\"), crossing_dim=8, output_mode=\"one_hot\")],\n",
        "                                  output_mode=\"dict\"\n",
        "                                  )\n",
        "\n",
        "feature_space_conc = FeatureSpace(\n",
        "                                  features={**{a:FeatureSpace.integer_categorical(num_oov_indices=0, output_mode=\"one_hot\") for a in int_feat},**{a:FeatureSpace.float_normalized() for a in float_feat}},\n",
        "                                  # Specify feature cross with a custom crossing dim.\n",
        "                                  crosses=[FeatureSpace.cross(feature_names=(\"awa_cat\",\"grouped_atomicweight\"), crossing_dim=8, output_mode=\"one_hot\")],\n",
        "                                  output_mode=\"concat\"\n",
        "                                  )"
      ],
      "metadata": {
        "id": "OzlO7JXlbQb4"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "train_ds_with_no_labels = train_ds.map(lambda x, _: x)\n",
        "feature_space_dict.adapt(train_ds_with_no_labels)\n",
        "feature_space_conc.adapt(train_ds_with_no_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2F99chf9-YnL",
        "outputId": "0b2c7526-7195-4009-e8ed-0faf2ceceb73"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1min 11s, sys: 1.76 s, total: 1min 12s\n",
            "Wall time: 1min 15s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for x, _ in train_ds.take(1):\n",
        "    preprocessed_x = feature_space_conc(x)\n",
        "    print(\"preprocessed_x.shape:\", preprocessed_x.shape)\n",
        "    print(\"preprocessed_x.dtype:\", preprocessed_x.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMBRBRL5EAF1",
        "outputId": "12035404-0f84-4623-d0bd-603cfdc02fb8"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "preprocessed_x.shape: (32, 39)\n",
            "preprocessed_x.dtype: <dtype: 'float32'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_train_ds_dict = train_ds.map(lambda x, y: (feature_space_dict(x), y), num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
        "preprocessed_test_ds_dict = test_ds.map(lambda x, y: (feature_space_dict(x), y), num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "preprocessed_train_ds_conc = train_ds.map(lambda x, y: (feature_space_conc(x), y), num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
        "preprocessed_test_ds_conc = test_ds.map(lambda x, y: (feature_space_conc(x), y), num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "4j5TiL1dGSJI"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x, y in preprocessed_train_ds_dict.take(1):\n",
        "    print(\"preprocessed_x.shape:\", x.keys())\n",
        "    print(\"y.dtype:\", y.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGERgwVJGz4a",
        "outputId": "3257e80e-8538-443e-e2d0-822ee397fb10"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "preprocessed_x.shape: dict_keys(['R_cov_element_Average', 'R_vdw_element_Average', 'allelectrons_Average', 'allelectrons_Total', 'atomicweight_density_diff', 'awa_cat', 'combined_feature', 'density_Average', 'density_Total', 'density_per_valence', 'el_neg_chi_Average', 'grouped_atomicweight', 'ionenergy_Average', 'nan_marker', 'tree_feat_v3', 'tree_feat_v33', 'tree_feat_v56', 'tree_feat_v57', 'tree_feat_v58', 'tree_feat_v60', 'tree_feat_v84', 'tree_feat_v94', 'val_e_Average', 'zaratio_Average', 'awa_cat_X_grouped_atomicweight'])\n",
            "y.dtype: <dtype: 'float32'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for x, y in preprocessed_train_ds_conc.take(1):\n",
        "    print(\"preprocessed_x.shape:\", x.shape)\n",
        "    print(\"y.dtype:\", y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zFWvNgfLs29",
        "outputId": "4140ab19-7f6b-42c0-9ac5-12fcdeaf65ce"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "preprocessed_x.shape: (32, 39)\n",
            "y.dtype: (32,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_features_dict = feature_space_dict.get_encoded_features()\n",
        "print(encoded_features_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRxLqzXMNvWl",
        "outputId": "40fd48a4-7b32-4e37-e01d-504c607936fd"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'R_cov_element_Average': <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'float_normalized_113_preprocessor')>, 'R_vdw_element_Average': <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'float_normalized_112_preprocessor')>, 'allelectrons_Average': <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'float_normalized_108_preprocessor')>, 'allelectrons_Total': <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'float_normalized_106_preprocessor')>, 'atomicweight_density_diff': <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'float_normalized_118_preprocessor')>, 'awa_cat': <KerasTensor: shape=(None, 6) dtype=float32 (created by layer 'category_encoding_8')>, 'combined_feature': <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'float_normalized_117_preprocessor')>, 'density_Average': <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'float_normalized_115_preprocessor')>, 'density_Total': <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'float_normalized_107_preprocessor')>, 'density_per_valence': <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'float_normalized_116_preprocessor')>, 'el_neg_chi_Average': <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'float_normalized_111_preprocessor')>, 'grouped_atomicweight': <KerasTensor: shape=(None, 2) dtype=float32 (created by layer 'category_encoding_9')>, 'ionenergy_Average': <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'float_normalized_110_preprocessor')>, 'nan_marker': <KerasTensor: shape=(None, 2) dtype=float32 (created by layer 'category_encoding_10')>, 'tree_feat_v3': <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'float_normalized_121_preprocessor')>, 'tree_feat_v33': <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'float_normalized_123_preprocessor')>, 'tree_feat_v56': <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'float_normalized_125_preprocessor')>, 'tree_feat_v57': <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'float_normalized_119_preprocessor')>, 'tree_feat_v58': <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'float_normalized_126_preprocessor')>, 'tree_feat_v60': <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'float_normalized_122_preprocessor')>, 'tree_feat_v84': <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'float_normalized_120_preprocessor')>, 'tree_feat_v94': <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'float_normalized_124_preprocessor')>, 'val_e_Average': <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'float_normalized_109_preprocessor')>, 'zaratio_Average': <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'float_normalized_114_preprocessor')>, 'awa_cat_X_grouped_atomicweight': <KerasTensor: shape=(None, 8) dtype=float32 (created by layer 'category_encoding_11')>}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.0 MODELS\n",
        "-----------------------"
      ],
      "metadata": {
        "id": "v-YGyDrxQ7jv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Experiment Run Function**"
      ],
      "metadata": {
        "id": "nK2z_R7KRGRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001\n",
        "dropout_rate = 0.1\n",
        "batch_size = 64\n",
        "num_epochs = 50\n",
        "\n",
        "hidden_units = [32, 32]\n",
        "\n",
        "\n",
        "def run_experiment(model,train_data_file,val_data_file,test_data_file):\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "        loss=keras.losses.MeanAbsoluteError(),\n",
        "        metrics=[keras.metrics.MeanAbsoluteError()],\n",
        "    )\n",
        "\n",
        "    train_dataset = dataframe_to_dataset(train_data_file, batch_size, shuffle=True)\n",
        "    valid_dataset = dataframe_to_dataset(test_data_file, batch_size, shuffle=False)\n",
        "    test_dataset = dataframe_to_dataset(test_data_file, batch_size, shuffle=False)\n",
        "\n",
        "    print(\"Start training the model...\")\n",
        "    history = model.fit(train_dataset, epochs=num_epochs)\n",
        "    print(\"Model training finished\")\n",
        "\n",
        "    _, accuracy = model.evaluate(test_dataset, verbose=0)\n",
        "\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")"
      ],
      "metadata": {
        "id": "RoOg-ziCRCSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Baseline:"
      ],
      "metadata": {
        "id": "tmCA8PswQ--X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_features_conc = feature_space_conc.get_encoded_features()\n",
        "print(encoded_features_conc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_TrWI2iPEzu",
        "outputId": "3b80654e-d412-4888-c75a-13599792facd"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KerasTensor(type_spec=TensorSpec(shape=(None, 39), dtype=tf.float32, name=None), name='concatenate/concat:0', description=\"created by layer 'concatenate'\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_baseline_model(data_format=\"conc\"):\n",
        "  if data_format==\"conc\":\n",
        "    encoded_features_conc = feature_space_conc.get_encoded_features()\n",
        "\n",
        "  elif data_format==\"dict\":\n",
        "    encoded_features_dict = feature_space_dict.get_encoded_features()\n",
        "\n",
        "  x = tf.keras.layers.Dense(64, activation=\"relu\")(encoded_features_conc)\n",
        "  x = tf.keras.layers.Dropout(0.5)(x)\n",
        "  output = tf.keras.layers.Dense(1, activation=\"linear\")(x)\n",
        "\n",
        "  model = tf.keras.Model(inputs=encoded_features_conc, outputs=output)\n",
        "  model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "v_Sxvd2QPNV7"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_baseline_model()\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WG0iXPP7Phcg",
        "outputId": "80e20b84-e2e3-420c-986b-487a73f739b5"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, 39)]              0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 64)                2560      \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2625 (10.25 KB)\n",
            "Trainable params: 2625 (10.25 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "krU00faCPoss"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}