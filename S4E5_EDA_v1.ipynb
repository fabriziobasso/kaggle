{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fabriziobasso/kaggle/blob/main/S4E5_EDA_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3YFwEWGhqJz"
      },
      "source": [
        "# Regression with an Flood Dataset\n",
        "Playground Series - Season 4, Episode 5\n",
        "\n",
        "\n",
        "####**Dataset Description**\n",
        "The dataset for this competition (both train and test) was generated from a deep learning model trained on the Abalone dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance.\n",
        "\n",
        "Files\n",
        "**train.csv** - the training dataset; Rings is the integer target\n",
        "**test.csv** - the test dataset; your objective is to predict the value of Rings for each row\n",
        "**sample_submission.csv** - a sample submission file in the correct format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ehbj4ZxqabOF"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "run_n = 0\n",
        "if run_n==0:\n",
        "  #!pip install tensorflow-addons\n",
        "  #!pip install shap\n",
        "  #!pip install eli5\n",
        "  #!pip install tf-nightly\n",
        "  #!pip install -U scikit-learn==1.2.0\n",
        "  !pip install catboost\n",
        "  #!pip install haversine\n",
        "  #!pip install pytorch-forecasting\n",
        "  #!pip install umap-learn\n",
        "  #!pip install reverse_geocoder\n",
        "  #!pip install --upgrade protobuf\n",
        "  !pip install colorama\n",
        "  #!pip install imbalanced-learn\n",
        "  !pip install optuna\n",
        "  !pip install optuna-integration\n",
        "  #!pip install pygam\n",
        "  !pip install keras-tuner --upgrade\n",
        "  #!pip install pycaret\n",
        "  #!pip install lightning==2.0.1\n",
        "  !pip install keras-nlp\n",
        "  #!pip install MiniSom\n",
        "  !pip install category_encoders\n",
        "  !pip install BorutaShap\n",
        "  !pip install feature-engine\n",
        "  #!pip install scikit-learn==1.4.1\n",
        "  !pip install scikit-lego"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81y_MZuE8zTt"
      },
      "outputs": [],
      "source": [
        "#importing modules\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import time\n",
        "t = time.time()\n",
        "\n",
        "from IPython.display import display_html, clear_output;\n",
        "clear_output();\n",
        "\n",
        "print('Importing started...')\n",
        "\n",
        "# basic moduele\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from copy import deepcopy\n",
        "import re\n",
        "#from scipy import stats\n",
        "from random import randint\n",
        "from prettytable import PrettyTable\n",
        "import random\n",
        "import math\n",
        "import os\n",
        "import gc\n",
        "import pickle\n",
        "from glob import glob\n",
        "from IPython import display as ipd\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "from joblib import dump, load\n",
        "import sklearn as sk\n",
        "#from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
        "from functools import partial\n",
        "import itertools\n",
        "import joblib\n",
        "from itertools import combinations\n",
        "import IPython\n",
        "import statsmodels.api as sm\n",
        "import IPython.display\n",
        "#from umap import UMAP\n",
        "\n",
        "# visualization moduels\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import matplotlib.gridspec as gridspec\n",
        "import matplotlib.patches as mpatches\n",
        "from matplotlib_venn import venn2_unweighted\n",
        "import seaborn as sns\n",
        "import missingno as msno\n",
        "#import imblearn\n",
        "import scipy.stats as stats\n",
        "from scipy.special import boxcox, boxcox1p\n",
        "\n",
        "\n",
        "# Palette Setup\n",
        "colors = ['#FB5B68','#FFEB48','#2676A1','#FFBDB0',]\n",
        "colormap_0 = mpl.colors.LinearSegmentedColormap.from_list(\"\",colors)\n",
        "palette_1 = sns.color_palette(\"coolwarm\", as_cmap=True)\n",
        "palette_2 = sns.color_palette(\"YlOrBr\", as_cmap=True)\n",
        "palette_3 = sns.light_palette(\"red\", as_cmap=True)\n",
        "palette_4 = sns.color_palette(\"viridis\", as_cmap=True)\n",
        "palette_5 = sns.color_palette(\"rocket\", as_cmap=True)\n",
        "palette_6 = sns.color_palette(\"GnBu\", as_cmap=True)\n",
        "palette_7 = sns.color_palette(\"tab20c\", as_cmap=False)\n",
        "palette_8 = sns.color_palette(\"Set2\", as_cmap=False)\n",
        "\n",
        "palette_custom = ['#fbb4ae','#b3cde3','#ccebc5','#decbe4','#fed9a6','#ffffcc','#e5d8bd','#fddaec','#f2f2f2']\n",
        "palette_9 = sns.color_palette(palette_custom, as_cmap=False)\n",
        "\n",
        "sns.set_style(\"whitegrid\",{\"grid.linestyle\":\"--\", 'grid.linewidth':0.2, 'grid.alpha':0.5})\n",
        "#sns.set_theme(style=\"ticks\", context=\"notebook\")\n",
        "sns.despine(left=True, bottom=True, top=False, right=False)\n",
        "\n",
        "mpl.rcParams['axes.spines.left'] = True\n",
        "mpl.rcParams['axes.spines.right'] = False\n",
        "mpl.rcParams['axes.spines.top'] = False\n",
        "mpl.rcParams['axes.spines.bottom'] = True\n",
        "\n",
        "# Style Import\n",
        "from colorama import Style, Fore\n",
        "red = Style.BRIGHT + Fore.RED\n",
        "blu = Style.BRIGHT + Fore.BLUE\n",
        "mgt = Style.BRIGHT + Fore.MAGENTA\n",
        "gld = Style.BRIGHT + Fore.YELLOW\n",
        "res = Style.RESET_ALL\n",
        "\n",
        "# preprocessing modules\n",
        "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, cross_val_score, cross_validate, GroupKFold, GridSearchCV, RepeatedStratifiedKFold, cross_val_predict\n",
        "\n",
        "from sklearn.preprocessing import (LabelEncoder,\n",
        "                                   StandardScaler,\n",
        "                                   MinMaxScaler,\n",
        "                                   OrdinalEncoder,\n",
        "                                   RobustScaler,\n",
        "                                   PowerTransformer,\n",
        "                                   OneHotEncoder,\n",
        "                                   LabelEncoder,\n",
        "                                   OrdinalEncoder,\n",
        "                                   QuantileTransformer,\n",
        "                                   PolynomialFeatures)\n",
        "\n",
        "from sklearn.decomposition import PCA, TruncatedSVD\n",
        "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
        "\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "\n",
        "\n",
        "# metrics\n",
        "from sklearn.metrics import (mean_squared_error,\n",
        "                             r2_score,\n",
        "                             mean_absolute_error,\n",
        "                             mean_absolute_percentage_error,\n",
        "                             classification_report,\n",
        "                             confusion_matrix,\n",
        "                             ConfusionMatrixDisplay,\n",
        "                             multilabel_confusion_matrix,\n",
        "                             accuracy_score,\n",
        "                             roc_auc_score,\n",
        "                             auc,\n",
        "                             roc_curve,\n",
        "                             log_loss,\n",
        "                             make_scorer)\n",
        "\n",
        "\n",
        "# modeling algos\n",
        "from sklearn.linear_model import (LogisticRegression,\n",
        "                                  Lasso,\n",
        "                                  ridge_regression,\n",
        "                                  LinearRegression,\n",
        "                                  Ridge,\n",
        "                                  RidgeCV,\n",
        "                                  ElasticNet,\n",
        "                                  BayesianRidge,\n",
        "                                  HuberRegressor,\n",
        "                                  TweedieRegressor,\n",
        "                                  ARDRegression,\n",
        "                                  PoissonRegressor,\n",
        "                                  GammaRegressor)\n",
        "\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.isotonic import IsotonicRegression\n",
        "\n",
        "from sklearn.ensemble import (AdaBoostRegressor,\n",
        "                              AdaBoostClassifier,\n",
        "                              RandomForestRegressor,\n",
        "                              RandomForestClassifier,\n",
        "                              VotingRegressor,\n",
        "                              GradientBoostingRegressor,\n",
        "                              GradientBoostingClassifier,\n",
        "                              StackingRegressor,\n",
        "                              HistGradientBoostingClassifier,\n",
        "                              HistGradientBoostingRegressor,\n",
        "                              ExtraTreesClassifier)\n",
        "\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.impute import SimpleImputer, KNNImputer\n",
        "\n",
        "#from category_encoders import MEstimateEncoder, CatBoostEncoder, OrdinalEncoder\n",
        "\n",
        "# Other Models\n",
        "#from pygam import LogisticGAM, s, te\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBRegressor, XGBClassifier\n",
        "import lightgbm as lgb\n",
        "from lightgbm import (LGBMRegressor,\n",
        "                      LGBMClassifier,\n",
        "                      early_stopping,\n",
        "                      record_evaluation,\n",
        "                      log_evaluation)\n",
        "\n",
        "#import catboost as cat\n",
        "from catboost import CatBoost, CatBoostRegressor\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "#from catboost.utils import get_roc_curve\n",
        "\n",
        "from lightgbm import early_stopping\n",
        "# check installed version\n",
        "#import pycaret\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "#from minisom import MiniSom\n",
        "\n",
        "from sklearn.base import clone ## sklearn base models for stacked ensemble model\n",
        "from sklearn.calibration import CalibratedClassifierCV, CalibrationDisplay\n",
        "\n",
        "#Interpretiability of the model\n",
        "#import shap\n",
        "#import eli5\n",
        "#from eli5.sklearn import PermutationImportance\n",
        "\n",
        "\n",
        "## miss\n",
        "from sklearn.pipeline import (make_pipeline,\n",
        "                              Pipeline)\n",
        "\n",
        "\n",
        "import feature_engine as fe\n",
        "from feature_engine.creation import MathFeatures\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow.keras.backend as K\n",
        "#import tensorflow_addons as tfa\n",
        "from keras.utils import FeatureSpace\n",
        "#import keras_nlp\n",
        "from keras.utils import FeatureSpace, plot_model\n",
        "\n",
        "# Import libraries for Hypertuning\n",
        "import kerastuner as kt\n",
        "from kerastuner.tuners import RandomSearch, GridSearch, BayesianOptimization\n",
        "# Model Tuning tools:\n",
        "import optuna\n",
        "from optuna.integration import TFKerasPruningCallback\n",
        "from optuna.trial import TrialState\n",
        "from optuna.visualization import plot_intermediate_values\n",
        "from optuna.visualization import plot_optimization_history\n",
        "from optuna.visualization import plot_param_importances\n",
        "from optuna.visualization import plot_contour\n",
        "# Feature selection\n",
        "from BorutaShap import BorutaShap\n",
        "%matplotlib inline\n",
        "SEED = 1984\n",
        "N_SPLITS = 10\n",
        "\n",
        "# Personal Library\n",
        "\n",
        "print('Done, All the required modules are imported. Time elapsed: {} sec'.format(time.time()-t))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NHXgby13oDV"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# Setting rc parameters in seaborn for plots and graphs-\n",
        "# Reference - https://matplotlib.org/stable/tutorials/introductory/customizing.html:-\n",
        "# To alter this, refer to matplotlib.rcParams.keys()\n",
        "\n",
        "sns.set({\"axes.facecolor\"       : \"#ffffff\",\n",
        "         \"figure.facecolor\"     : \"#ffffff\",\n",
        "         \"axes.edgecolor\"       : \"#000000\",\n",
        "         \"grid.color\"           : \"#ffffff\",\n",
        "         \"font.family\"          : ['Cambria'],\n",
        "         \"axes.labelcolor\"      : \"#000000\",\n",
        "         \"xtick.color\"          : \"#000000\",\n",
        "         \"ytick.color\"          : \"#000000\",\n",
        "         \"grid.linewidth\"       : 0.75,\n",
        "         \"grid.linestyle\"       : \"--\",\n",
        "         \"axes.titlecolor\"      : '#0099e6',\n",
        "         'axes.titlesize'       : 8.5,\n",
        "         'axes.labelweight'     : \"bold\",\n",
        "         'legend.fontsize'      : 7.0,\n",
        "         'legend.title_fontsize': 7.0,\n",
        "         'font.size'            : 7.5,\n",
        "         'xtick.labelsize'      : 7.5,\n",
        "         'ytick.labelsize'      : 7.5,\n",
        "        });\n",
        "\n",
        "# Color printing\n",
        "def PrintColor(text:str, color = Fore.BLUE, style = Style.BRIGHT):\n",
        "    \"Prints color outputs using colorama using a text F-string\";\n",
        "    print(style + color + text + Style.RESET_ALL);\n",
        "\n",
        "# Making sklearn pipeline outputs as dataframe:-\n",
        "pd.set_option('display.max_columns', 100);\n",
        "pd.set_option('display.max_rows', 50);\n",
        "\n",
        "sns.set_style(\"whitegrid\",{\"grid.linestyle\":\"--\", 'grid.linewidth':0.2, 'grid.alpha':0.5})\n",
        "#sns.set_theme(style=\"ticks\", context=\"notebook\")\n",
        "sns.despine(left=True, bottom=True, top=False, right=False)\n",
        "\n",
        "mpl.rcParams['axes.spines.left'] = True\n",
        "mpl.rcParams['axes.spines.right'] = False\n",
        "mpl.rcParams['axes.spines.top'] = False\n",
        "mpl.rcParams['axes.spines.bottom'] = True\n",
        "\n",
        "print();\n",
        "gc.collect();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXoRiAxUiOlO"
      },
      "outputs": [],
      "source": [
        "# Check Versions:\n",
        "print(\"CHECK VERSIONS:\")\n",
        "print(f\"sns: {sns.__version__}\")\n",
        "print(f\"mpl: {mpl.__version__}\")\n",
        "print(f\"tensorflow: {tf.__version__}\")\n",
        "print(f\"pandas: {pd.__version__}\")\n",
        "print(f\"numpy: {np.__version__}\")\n",
        "print(f\"scikit-learn: {sk.__version__}\")\n",
        "print(f\"statsmodels: {sm.__version__}\")\n",
        "print(f\"missingno: {msno.__version__}\")\n",
        "#print(f\"TF-addon: {tfa.__version__}\")\n",
        "#print(f\"Inbalance_Learning: {imblearn.__version__}\")\n",
        "print(f\"XGBoost: {xgb.__version__}\")\n",
        "#print(f\"CatBoost: {cat.__version__}\")\n",
        "#print(f\"PyCaret: {pycaret.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1K1-Yuv-syUc"
      },
      "outputs": [],
      "source": [
        "print(f\"scikit-learn: {sk.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0eLGb3fVilrA"
      },
      "outputs": [],
      "source": [
        "# Configuration class:-\n",
        "class CFG:\n",
        "    \"\"\"\n",
        "    Configuration class for parameters and CV strategy for tuning and training\n",
        "    Some parameters may be unused here as this is a general configuration class\n",
        "    \"\"\";\n",
        "\n",
        "    # Data preparation:-\n",
        "    version_nb         = 4;\n",
        "    test_req           = \"N\";\n",
        "    test_sample_frac   = 0.025;\n",
        "    gpu_switch         = \"OFF\";\n",
        "    state              = 42;\n",
        "    target             = [\"Rings\"];\n",
        "    episode            = 5;\n",
        "    season             = 4;\n",
        "    path               = f\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S{season}E{episode}_Flood\";\n",
        "    orig_path          = f\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S{season}E{episode}_Plates/flood.csv\";\n",
        "    public_subs_path   = None;\n",
        "\n",
        "    dtl_preproc_req    = \"Y\";\n",
        "    adv_cv_req         = \"N\";\n",
        "    ftre_plots_req     = 'Y';\n",
        "    ftre_imp_req       = \"Y\";\n",
        "\n",
        "    # Data transforms and scaling:-\n",
        "    conjoin_orig_data  = \"Y\";\n",
        "    drop_nulls         = \"N\";\n",
        "    sec_ftre_req       = \"Y\";\n",
        "    scale_req          = \"N\";\n",
        "    # NOTE---Keep a value here even if scale_req = N, this is used for linear models:-\n",
        "    scl_method         = \"Z\";\n",
        "    enc_method         = 'Label';\n",
        "    OH_cols            = [\"MTRANS\"];\n",
        "    tgt_mapper         = {0 : 0,\n",
        "                          1 : 1,\n",
        "                          };\n",
        "\n",
        "    # Model Training:-\n",
        "    baseline_req       = \"N\";\n",
        "    pstprcs_oof        = \"N\";\n",
        "    pstprcs_train      = \"N\";\n",
        "    pstprcs_test       = \"N\";\n",
        "    ML                 = \"Y\";\n",
        "\n",
        "    pseudo_lbl_req     = \"N\";\n",
        "    pseudolbl_up       = 0.975;\n",
        "    pseudolbl_low      = 0.00;\n",
        "\n",
        "    use_orig_allfolds  = \"N\";\n",
        "    n_splits           = 3 if test_req == \"Y\" else 10;\n",
        "    n_repeats          = 1 ;\n",
        "    nbrnd_erly_stp     = 75;\n",
        "    mdlcv_mthd         = 'RSKF';\n",
        "\n",
        "    # Ensemble:-\n",
        "    ensemble_req       = \"Y\";\n",
        "    hill_climb_req     = \"N\";\n",
        "    optuna_req         = \"Y\";\n",
        "    LAD_req            = \"N\";\n",
        "    enscv_mthd         = \"RSKF\";\n",
        "    metric_obj         = 'maximize';\n",
        "    ntrials            = 10 if test_req == \"Y\" else 150;\n",
        "\n",
        "    # Global variables for plotting:-\n",
        "    grid_specs = {'visible': True, 'which': 'both', 'linestyle': '--',\n",
        "                           'color': 'lightgrey', 'linewidth': 0.75};\n",
        "    title_specs = {'fontsize': 9, 'fontweight': 'bold', 'color': 'tab:blue'};\n",
        "\n",
        "#print();\n",
        "PrintColor(f\"--> Configuration done!\\n\");\n",
        "gc.collect();\n",
        "\n",
        "CFG.ntrials"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIRkouhrYMg-"
      },
      "source": [
        "### 0.0 Connect Drives:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nucekKRDxbj"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xwI9dKRDxbj"
      },
      "source": [
        "Connect to Google Drive:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XnZHOmpDDxbj"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Connect to Colab:\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzy1u60Ta75I"
      },
      "outputs": [],
      "source": [
        "folder_script = models_folders = \"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Scripts/S4E5_Flood\"\n",
        "os.chdir(folder_script)\n",
        "import data_analysis as da\n",
        "\n",
        "da.info_on_functions()\n",
        "\n",
        "run_graph=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXLeB_aPKxby"
      },
      "outputs": [],
      "source": [
        "da.seed_everything(seed=42, tensorflow_init=True,pytorch_init=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5eUI141AxR6"
      },
      "source": [
        "#### Control Action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5n_wi-uA1Sq"
      },
      "outputs": [],
      "source": [
        "run_ridge=0\n",
        "run_lgb = 0\n",
        "run_xgb=0\n",
        "run_cat=0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvtFu2ayEo3E"
      },
      "source": [
        "### 0.1 Functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UF9u5s7ft9Oj"
      },
      "outputs": [],
      "source": [
        "from numpy import array, random, arange\n",
        "\n",
        "def sort_rows(df,features_original):\n",
        "\n",
        "    df_copy = df[features_original].copy()\n",
        "\n",
        "    df_copy = np.sort(df_copy,axis=1)\n",
        "\n",
        "    return df_copy\n",
        "\n",
        "def adjust_row_means_vectorized_low(df,features_original, to_remove=2):\n",
        "    df_copy = df[features_original].copy()\n",
        "    df_copy[\"adj_mean\"]=df_copy.mean(axis=1)\n",
        "\n",
        "    means = df_copy.mean(axis=1)\n",
        "    medians = df_copy.median(axis=1)\n",
        "\n",
        "    # Remove the two highest values if mean > median\n",
        "    where_high = means > medians\n",
        "    df_copy.loc[where_high,\"adj_mean\"] = np.sort(df_copy.loc[where_high,:],axis=1)[:,:-to_remove].mean(axis=1)\n",
        "\n",
        "    # Remove the two lowest values if mean <= median\n",
        "    where_low = means < medians\n",
        "    df_copy.loc[where_low,\"adj_mean\"] = np.sort(df_copy.loc[where_low,:],axis=1)[:,to_remove:].mean(axis=1)\n",
        "\n",
        "    return df_copy[\"adj_mean\"].values\n",
        "\n",
        "def adjust_row_means_vectorized(df,features_original, to_remove=7):\n",
        "    df_copy = df[features_original].copy()\n",
        "    df_copy[\"adj_mean\"]=np.sort(df_copy,axis=1)[:,:to_remove].mean(axis=1)\n",
        "\n",
        "    return df_copy[\"adj_mean\"].values\n",
        "\n",
        "def xicor(X, Y, ties=True):\n",
        "    random.seed(42)\n",
        "    n = len(X)-1\n",
        "    order = array([i[0] for i in sorted(enumerate(X), key=lambda x: x[1])])\n",
        "    if ties:\n",
        "        l = array([sum(y >= Y[order]) for y in Y[order]])\n",
        "        r = l.copy()\n",
        "        for j in range(n):\n",
        "            if sum([r[j] == r[i] for i in range(n)]) > 1:\n",
        "                tie_index = array([r[j] == r[i] for i in range(n)])\n",
        "                r[tie_index] = random.choice(r[tie_index] - arange(0, sum([r[j] == r[i] for i in range(n)])), sum(tie_index), replace=False)\n",
        "        return 1 - n*sum( abs(r[1:] - r[:n-1]) ) / (2*sum(l*(n - l)))\n",
        "    else:\n",
        "        r = array([sum(y >= Y[order]) for y in Y[order]])\n",
        "        return 1 - 3 * sum( abs(r[1:] - r[:n-1]) ) / (n**2 - 1)\n",
        "\n",
        "def sum_three_smallest_numpy(df,num):\n",
        "    values = df.values\n",
        "\n",
        "    smallest = 22-np.sort(values, axis=1)[:, :num].mean(axis=1)\n",
        "    largest = np.sort(values, axis=1)[:, -num:].mean(axis=1)\n",
        "    rt = largest-smallest\n",
        "    return rt\n",
        "\n",
        "\n",
        "def plot_regression_scatter(y_true, y_pred):\n",
        "  plt.figure(figsize=(8, 6))\n",
        "  plt.scatter(y_true, y_pred)\n",
        "  plt.plot(y_true, y_true, 'r--')  # Perfect prediction line\n",
        "  plt.xlabel('Target Values')\n",
        "  plt.ylabel('Predicted Values')\n",
        "  plt.title('Regression Output vs. Target with Reference')\n",
        "  plt.show()\n",
        "\n",
        "def create_sub_files(df_results, scaler, oof_results=[], experiment_name = \"\", folder_data = f\"/content/drive/MyDrive/Exercises/Studies_Structured_Data//Data/S4E5_Flood\",replace_duplicates=True):\n",
        "\n",
        "  os.chdir(folder_data)\n",
        "\n",
        "  sub = pd.read_csv(\"sample_submission.csv\", index_col=\"id\")\n",
        "  if replace_duplicates==True:\n",
        "    replacements = pd.read_csv(\"test_duplicated.csv\",index_col=0)\n",
        "\n",
        "\n",
        "  res_no_reb = sub.copy()\n",
        "  res_reb = sub.copy()\n",
        "\n",
        "  test_results_df_no_reb = pd.DataFrame(index=sub.index, columns=sub.columns, data=df_results)\n",
        "  res_no_reb = test_results_df_no_reb\n",
        "  res_no_reb_sub = pd.DataFrame(index=sub.index, columns=sub.columns, data=scaler.inverse_transform(res_no_reb))\n",
        "\n",
        "  res_with_replacement = res_no_reb_sub.copy()\n",
        "  res_with_replacement.loc[replacements.index]=replacements.values\n",
        "\n",
        "  res_no_reb_sub.to_csv(f\"Results/{experiment_name}.csv\")\n",
        "  res_with_replacement.to_csv(f\"Results/{experiment_name}_test_with_replacements.csv\")\n",
        "\n",
        "  res_no_reb.to_csv(f\"results_ensemble/{experiment_name}_test.csv\")\n",
        "  res_with_replacement.to_csv(f\"results_ensemble/{experiment_name}_test_with_replacements.csv\")\n",
        "\n",
        "  if len(oof_results)!=0:\n",
        "    oof_results_df = pd.DataFrame(columns=sub.columns, data=oof_results)\n",
        "    oof_results_df.to_csv(f\"results_ensemble/{experiment_name}_train.csv\")\n",
        "\n",
        "  return res_no_reb_sub\n",
        "\n",
        "def plot_training_session(history):\n",
        "  # Plot training and validation loss scores\n",
        "  # against the number of epochs.\n",
        "  plt.figure(figsize=(8, 6))\n",
        "  plt.plot(history.history['loss'], label='Train')\n",
        "  plt.plot(history.history['val_loss'], label='Validation')\n",
        "  plt.grid(linestyle='--')\n",
        "  plt.ylabel('val_loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.title('Train-Validation Scores', pad=13)\n",
        "  plt.legend(loc='upper right');\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLfOC7BeEor2"
      },
      "outputs": [],
      "source": [
        "class add_cluster_features(BaseEstimator, TransformerMixin):\n",
        "\n",
        "    def __init__(self, num_clusters=8, random_state=None, features=None, target=\"target\", name=\"x_cluster\"):\n",
        "\n",
        "        self.clustes=num_clusters\n",
        "        self.random_state = random_state\n",
        "        self.features = features\n",
        "        self._name = name\n",
        "        self.target=target\n",
        "        self.strategy = KMeans(n_clusters=self.clustes, random_state=self.random_state)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # only numerical columns:\n",
        "\n",
        "        temp = X.loc[:, self.features].copy()\n",
        "\n",
        "        self.strategy.fit(temp)\n",
        "\n",
        "        return self\n",
        "\n",
        "\n",
        "    def transform(self, X):\n",
        "        Xt = X.copy()\n",
        "\n",
        "        Xt.loc[:, self._name] = self.strategy.predict(Xt.loc[:, self.features])\n",
        "\n",
        "        return Xt\n",
        "\n",
        "    def fit_transform(self, X, y):\n",
        "        Xt = X.copy()\n",
        "        self.fit(Xt, y)\n",
        "        return self.transform(Xt)\n",
        "\n",
        "    def elbow_test(self, X, y, k=(3,12),metric=\"distortion\"):\n",
        "        Xt = X[self.features].copy()\n",
        "        model = KMeans()\n",
        "        visualizer = KElbowVisualizer(self.strategy, k=k,metric=metric)\n",
        "        visualizer.fit(Xt)        # Fit the data to the visualizer\n",
        "        visualizer.show()        # Finalize and render the figure\n",
        "\n",
        "\n",
        "###########################################################################\n",
        "splits=3\n",
        "n_repeats=3\n",
        "seed=42\n",
        "\n",
        "skf = RepeatedStratifiedKFold(n_splits = splits, n_repeats=n_repeats,\n",
        "                              random_state = seed,\n",
        "                              #shuffle = True\n",
        "                              )\n",
        "\n",
        "skf = KFold(\n",
        "            n_splits = 5,\n",
        "            random_state = seed,\n",
        "            shuffle = True\n",
        "            )\n",
        "\n",
        "# Containers for results\n",
        "oof, test_pred = {}, {}\n",
        "\n",
        "def cross_validate_tuning(model, trial, features, train, test, target_feat=\"FloodProbability\", n_repeats=1,\n",
        "                          rs_list=[17,3,78,18,20,42,38,25,1978,1981], pruning=True,es=True,\n",
        "                          model_type=\"lgbm\"):\n",
        "\n",
        "    \"\"\"Compute out-of-fold and test predictions for a given model.\n",
        "\n",
        "    Out-of-fold and test predictions are stored in the global variables\n",
        "    oof and test_pred, respectively.\n",
        "\n",
        "    If n_repeats > 1, the model is trained several times with different seeds.\n",
        "\n",
        "    All predictions are clipped to the interval [1, 29].\n",
        "    \"\"\"\n",
        "    scores = []\n",
        "    oof_preds = np.full_like(train[target_feat], np.nan, dtype=float)\n",
        "\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
        "    for fold, (idx_tr, idx_va) in enumerate(kf.split(train, train[target_feat])):\n",
        "        X_tr = train.iloc[idx_tr][features]\n",
        "        X_va = train.iloc[idx_va][features]\n",
        "        y_tr = train.iloc[idx_tr][target_feat]\n",
        "        y_va = train.iloc[idx_va][target_feat]\n",
        "\n",
        "        y_pred = np.zeros_like(y_va, dtype=float)\n",
        "        for i in range(n_repeats):\n",
        "            print(f\"Running Fold {fold} - Experiment {i}\")\n",
        "            m = clone(model)\n",
        "\n",
        "            try:\n",
        "                m.set_params(random_state=rs_list[i])\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "\n",
        "            if 'lgbm' in model_type:\n",
        "                fit_params={\"eval_set\":(X_va,y_va),\n",
        "                            \"callbacks\":[]}\n",
        "                if pruning==True:\n",
        "                    fit_params[\"callbacks\"].append(optuna.integration.LightGBMPruningCallback(trial, metric=\"l2\"))\n",
        "                if es==True:\n",
        "                    fit_params[\"callbacks\"].append(early_stopping(stopping_rounds=51))\n",
        "\n",
        "                m.fit(X_tr, y_tr,**fit_params)\n",
        "\n",
        "            elif 'xgb' in model_type:\n",
        "                fit_params={\"eval_set\":[(X_va,y_va)],\n",
        "                            \"callbacks\":[]}\n",
        "\n",
        "                if pruning==True:\n",
        "                    fit_params[\"callbacks\"].append(optuna.integration.XGBoostPruningCallback(trial, \"validation_0-rmse\"))\n",
        "                if es==True:\n",
        "                    fit_params[\"callbacks\"].append(xgb.callback.EarlyStopping(rounds=51, save_best=True))\n",
        "\n",
        "\n",
        "                m.fit(X_tr, y_tr, verbose=False, **fit_params)\n",
        "\n",
        "            elif 'cat' in model_type:\n",
        "                fit_params={\"eval_set\":[(X_va,y_va)],\"callbacks\":[]}\n",
        "                if pruning==True:\n",
        "                  fit_params[\"callbacks\"].append(optuna.integration.CatBoostPruningCallback(trial, \"RMSE\"))\n",
        "\n",
        "                m.fit(X_tr, y_tr,**fit_params)\n",
        "\n",
        "\n",
        "            else:\n",
        "                m.fit(X_tr, y_tr)\n",
        "\n",
        "            y_pred += m.predict(X_va)\n",
        "        y_pred /= n_repeats\n",
        "\n",
        "        score = mean_squared_error(y_va, y_pred)\n",
        "        R2 = r2_score(y_va, y_pred)\n",
        "        print(f\"# Fold {fold}: MSE={score:.5f}, R2={R2:.5f}\")\n",
        "        scores.append(score)\n",
        "        oof_preds[idx_va] = y_pred\n",
        "    print(f\"{Fore.GREEN}# Overall MSE Mean: {np.array(scores).mean():.5f}{Style.RESET_ALL}\")\n",
        "    final_score=np.array(scores).mean()\n",
        "\n",
        "    return final_score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7NAELqI3mqB"
      },
      "source": [
        "## 1.0 Upload Dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4X6zp8Ii34Z"
      },
      "outputs": [],
      "source": [
        "dataset_manager = da.load_data.read_data(to_drop=[],base=\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/\", exp_name=\"S4E5_Flood\", old_dataset=\"flood.csv\")\n",
        "train_df = dataset_manager.train_dataset\n",
        "test_df = dataset_manager.test_dataset\n",
        "old_df = dataset_manager.old\n",
        "\n",
        "old_df = old_df.reset_index()\n",
        "dataset_manager.old=old_df\n",
        "old_df = dataset_manager.old"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p79lRH0NnAex"
      },
      "outputs": [],
      "source": [
        "train_df.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qlpXcUchPBB"
      },
      "outputs": [],
      "source": [
        "test_df.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jdvmc5V8udS6"
      },
      "outputs": [],
      "source": [
        "old_df.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0l7CkFG3sIf"
      },
      "source": [
        "### 1.1 Preliminary Analsys:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jws9OaoShSBV"
      },
      "outputs": [],
      "source": [
        "dataset_manager.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "516esx_qqSpL"
      },
      "outputs": [],
      "source": [
        "dataset_manager.summary(on=\"test\",cmap=\"Reds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OV9ciovw8CMD"
      },
      "outputs": [],
      "source": [
        "dataset_manager.summary(on=\"old\",cmap=\"Reds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EnwyYGr782MR"
      },
      "outputs": [],
      "source": [
        "old_df = dataset_manager.old\n",
        "old_df.sample(10)\n",
        "#old_df.rename(columns={\"Whole_weight\":\"Whole weight\",\"Shucked_weight\":\"Whole weight.1\",\"Viscera_weight\":\"Whole weight.2\",\"Shell_weight\":\"Shell weight\"},inplace=True)\n",
        "\n",
        "#dataset_manager.old = old_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmnkUdwEjat0"
      },
      "source": [
        "### 1.2 Adversial Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2KAuQgeMYhX"
      },
      "outputs": [],
      "source": [
        "train_no_nan = train_df.drop(columns=\"FloodProbability\")\n",
        "old_no_nan = old_df.drop(columns=\"FloodProbability\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBCTmMSPXLxa"
      },
      "outputs": [],
      "source": [
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTj4Jeoohzaj"
      },
      "outputs": [],
      "source": [
        "new_train = dataset_manager.merge_train_old()\n",
        "new_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jfoG9deZTfm6"
      },
      "outputs": [],
      "source": [
        "dataset_manager.train = new_train\n",
        "train_df = dataset_manager.train_dataset\n",
        "train_df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkBHle2Tjfcq"
      },
      "source": [
        "### 1.3 NaN Review:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjajsRQBh2N8"
      },
      "outputs": [],
      "source": [
        "dataset_manager.check_nana()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W97gAI9Jjh6H"
      },
      "source": [
        "### 1.4 Check for Duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5fvezkqYlEw"
      },
      "outputs": [],
      "source": [
        "dataset_manager.define_target([\"FloodProbability\"])\n",
        "dataset_manager.check_duplicates();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4Ute_O5vBJm"
      },
      "source": [
        "## 2.0 Target Feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FiC9N8_dxfOW"
      },
      "outputs": [],
      "source": [
        "target_groups = train_df.groupby([\"FloodProbability\"])[[\"FloodProbability\"]].count().div(len(train_df))\n",
        "target_groups*100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLhT88CcsFjJ"
      },
      "outputs": [],
      "source": [
        "target_groups = train_df.copy()\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(data=target_groups, x='FloodProbability', kde=True,\n",
        "             stat=\"percent\"\n",
        "              )\n",
        "plt.title('Distribution of FloodProbability', fontsize=15)\n",
        "plt.xlabel('FloodProbability')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(True)\n",
        "#plt.yscale('log')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZoVTNAOCbag7"
      },
      "outputs": [],
      "source": [
        "train_df.FloodProbability.min(),train_df.FloodProbability.max(),"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9rSXrE2CTLu"
      },
      "source": [
        "**Notes**\n",
        "\n",
        "- Target Feature restricted between 0.285 and 0.725"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7Em8cuPCkjt"
      },
      "source": [
        "## 3.0 **EDA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D74uPZDUcEl7"
      },
      "outputs": [],
      "source": [
        "run_graph=1\n",
        "features = list(test_df.columns)\n",
        "len(features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AquGV-hDF_7"
      },
      "source": [
        "### 3.1 Categorical Features:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5HVXtkAb2P-"
      },
      "outputs": [],
      "source": [
        "if run_graph==0:\n",
        "  from matplotlib.ticker import MaxNLocator\n",
        "\n",
        "  _, axs = plt.subplots(5, 4, figsize=(12, 12))\n",
        "  for col, ax in zip(features, axs.ravel()):\n",
        "      vc = train_df[col].value_counts() / len(train_df)\n",
        "      ax.bar(vc.index, vc, color=\"tomato\")\n",
        "      #vc = test_df[col].value_counts() / len(test_df)\n",
        "      #ax.bar(vc.index, vc, alpha=0.6)\n",
        "      ax.set_title(col)\n",
        "      ax.xaxis.set_major_locator(MaxNLocator(integer=True)) # only integer labels\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztcCHBbDXuS4"
      },
      "source": [
        "#### **Add Average Score**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7EMBKne1oJeA"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import hmean, gmean\n",
        "features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klAMBKGXXzC4"
      },
      "outputs": [],
      "source": [
        "df_sorted_train = sort_rows(train_df,features)\n",
        "df_sorted_test = sort_rows(test_df,features)\n",
        "\n",
        "train_df[\"col_11\"] = df_sorted_train[:,11]\n",
        "test_df[\"col_11\"] = df_sorted_test[:,11]\n",
        "\n",
        "train_df[\"col_12\"] = df_sorted_train[:,12]\n",
        "test_df[\"col_12\"] = df_sorted_test[:,12]\n",
        "\n",
        "train_df[\"col_8\"] = df_sorted_train[:,8]\n",
        "test_df[\"col_8\"] = df_sorted_test[:,8]\n",
        "\n",
        "train_df[\"col_8_12\"] = df_sorted_train[:,8]*df_sorted_train[:,12]\n",
        "test_df[\"col_8_12\"] = df_sorted_test[:,8]*df_sorted_test[:,12]\n",
        "\n",
        "train_df[\"average_score\"] = train_df[features].mean(axis=1)\n",
        "test_df[\"average_score\"] = test_df[features].mean(axis=1)\n",
        "\n",
        "train_df[\"median_score\"] = train_df[features].median(axis=1)\n",
        "test_df[\"median_score\"] = test_df[features].median(axis=1)\n",
        "\n",
        "train_df[\"hmean\"] = hmean((train_df[features]+1),axis=1)\n",
        "test_df[\"hmean\"] = hmean((test_df[features]+1),axis=1)\n",
        "\n",
        "#train_df[\"gmean\"] = gmean(train_df[features]+1,axis=1)#.prod(axis=1)**(1/len(features))\n",
        "#test_df[\"gmean\"] = gmean(test_df[features]+1,axis=1)#.prod(axis=1)**(1/len(features))\n",
        "\n",
        "#train_df[\"risk_factors\"] = (train_df[features] > 4).sum(axis=1).values\n",
        "#test_df[\"risk_factors\"] = (test_df[features] > 4).sum(axis=1).values\n",
        "\n",
        "#train_df[\"3_factors\"] = train_df[[\"MonsoonIntensity\",\"DamsQuality\",\"DeterioratingInfrastructure\"]].mean(axis=1)\n",
        "#test_df[\"3_factors\"] = test_df[[\"MonsoonIntensity\",\"DamsQuality\",\"DeterioratingInfrastructure\"]].mean(axis=1)\n",
        "\n",
        "#train_df['q_delta'] = train_df[features].quantile(0.80, axis=1)-train_df[features].quantile(0.20, axis=1)\n",
        "#test_df['q_delta'] = test_df[features].quantile(0.80, axis=1)-test_df[features].quantile(0.20, axis=1)\n",
        "\n",
        "#train_df['std'] = train_df[features].std(axis=1)\n",
        "#test_df['std'] = test_df[features].std(axis=1)\n",
        "\n",
        "#train_df['max'] = train_df[features].max(axis=1)\n",
        "#test_df['max'] = test_df[features].max(axis=1)\n",
        "\n",
        "#train_df['min'] = train_df[features].min(axis=1)\n",
        "#test_df['min'] = test_df[features].min(axis=1)\n",
        "\n",
        "train_df[\"delta\"] = train_df[features].max(axis=1) - 22 + train_df[features].min(axis=1)\n",
        "test_df[\"delta\"] = test_df[features].max(axis=1) - 22 + test_df[features].min(axis=1)\n",
        "\n",
        "train_df[\"compound_effect\"] = ((train_df[features]-6)**3).sum(axis=1).clip(upper=2000)#**(1/3)\n",
        "test_df[\"compound_effect\"] = ((train_df[features]-6)**3).sum(axis=1).clip(upper=2000)#**(1/3)\n",
        "\n",
        "train_df[\"risk_mitigation_factors\"] = (train_df[features] > 9).sum(axis=1).values-(train_df[features] < 5).sum(axis=1).values\n",
        "test_df[\"risk_mitigation_factors\"] = (test_df[features] > 9).sum(axis=1).values-(test_df[features] < 5).sum(axis=1).values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dFefHDSg9oV"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import pearsonr, spearmanr, kendalltau\n",
        "\n",
        "#df_results = pd.DataFrame(index=[2,3,4,5,6,7,8,9,10,11], columns=[\"Pearson\",\"Spearman\"])\n",
        "\n",
        "#for i in [2,3,4,5,6,7,8,9,10,11,12,13]:\n",
        "\n",
        "#  try_val = (train_df[features] > i).sum(axis=1).values\n",
        "#  y = train_df.FloodProbability.values\n",
        "#  df_results.loc[i,\"Spearman\"] = spearmanr(try_val, y)[0]\n",
        "#  df_results.loc[i,\"Pearson\"] = pearsonr(try_val, y)[0]\n",
        "\n",
        "\n",
        "#df_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXA3NZLqW95E"
      },
      "outputs": [],
      "source": [
        "#result_corr = pd.DataFrame(columns=[\"Pearsons\", \"Spearman\",\"kendalltau\"])#\n",
        "\n",
        "#for a,b in tqdm(combinations_2):\n",
        "\n",
        "#  try_val = (train_df[a]*train_df[b]).values\n",
        "#  y = train_df.FloodProbability.values\n",
        "\n",
        "#  result_corr.loc[f\"{a}_{b}\",\"Pearsons\"] = pearsonr(try_val, y)[0]\n",
        "#  result_corr.loc[f\"{a}_{b}\",\"Spearman\"] = spearmanr(try_val, y)[0]\n",
        "#  result_corr.loc[f\"{a}_{b}\",\"kendalltau\"] = kendalltau(try_val, y)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iu9_SWhIVpSN"
      },
      "outputs": [],
      "source": [
        "#result_corr = result_corr.sort_values(by=\"kendalltau\", ascending=False)\n",
        "#result_corr.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahKvztBDfmIz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_6IXZBUa8ZS"
      },
      "outputs": [],
      "source": [
        "#y = train_df.FloodProbability.values\n",
        "#VERIFY = ((train_df[\"TopographyDrainage\"]+1)*(train_df[\"DeterioratingInfrastructure\"]+1)*(train_df[\"MonsoonIntensity\"]+1)*\\\n",
        "# (train_df[\"DamsQuality\"]+1)*(train_df[\"RiverManagement\"]+1)*(train_df[\"Siltation\"]+1)*(train_df[\"PopulationScore\"]+1)*(train_df[\"Deforestation\"]+1)*\\\n",
        "#  (train_df[\"Urbanization\"]+1)*(train_df[\"DrainageSystems\"]+1)*(train_df[\"Encroachments\"]+1)*(train_df[\"IneffectiveDisasterPreparedness\"]+1))**(1/7)\n",
        "#print(VERIFY.max())\n",
        "#pearsonr(VERIFY, y)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7TxMcabsxUx"
      },
      "outputs": [],
      "source": [
        "#y = train_df.FloodProbability.values\n",
        "#VERIFY = (train_df[features]+1).prod(axis=1)**(1/20)\n",
        "#VERIFY = VERIFY[features].prod(axis=1)\n",
        "#VERIFY=VERIFY.drop(columns=features,axis=1)**(1/13)\n",
        "#print(VERIFY.max())\n",
        "#pearsonr(VERIFY, y)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "loCMRCHYat1J"
      },
      "outputs": [],
      "source": [
        "#result_corr = result_corr.sort_values(by=\"Pearsons\", ascending=False)\n",
        "#result_corr.head(10)\n",
        "#VERIFY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "piGxDHJ9uDd0"
      },
      "outputs": [],
      "source": [
        "#result_corr = pd.DataFrame(columns=[\"Pearsons\", \"Spearman\",\"kendalltau\"])\n",
        "\n",
        "#for a,b,c in tqdm(combinations_3):\n",
        "\n",
        "#  try_val = (train_df[a]+train_df[b]+train_df[c]).values\n",
        "#  y = train_df.FloodProbability.values\n",
        "\n",
        "#  result_corr.loc[f\"{a}_{b}_{c}\",\"Pearsons\"] = pearsonr(try_val, y)[0]\n",
        "#  result_corr.loc[f\"{a}_{b}_{c}\",\"Spearman\"] = spearmanr(try_val, y)[0]\n",
        "#  result_corr.loc[f\"{a}_{b}_{c}\",\"kendalltau\"] = kendalltau(try_val, y)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6YaFQKVu-59"
      },
      "outputs": [],
      "source": [
        "#result_corr = result_corr.sort_values(by=\"Pearsons\", ascending=False)\n",
        "#result_corr.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWhkxeKc7cYN"
      },
      "source": [
        "### 3.2 Correlation Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mc-ClnOkcK8n"
      },
      "outputs": [],
      "source": [
        "mask = np.zeros_like(train_df.corr(), dtype=bool)\n",
        "mask[np.triu_indices_from(mask)] = True\n",
        "np.fill_diagonal(mask, False)\n",
        "\n",
        "fig, axs = plt.subplots(1,1,figsize=(18,8))\n",
        "g = sns.heatmap(train_df.corr(),\n",
        "                annot=True,\n",
        "                annot_kws={\"fontsize\": 9},\n",
        "                fmt='.2f',\n",
        "                linewidths=0.5,\n",
        "                cmap='RdBu',\n",
        "                mask=mask,\n",
        "                ax=axs# the mask has been included here\n",
        "                )\n",
        "axs.grid(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7P_PQJy5V-ZK"
      },
      "outputs": [],
      "source": [
        "if run_graph==0:\n",
        "  plt.scatter(train_df[\"average_score\"], train_df[\"gmean\"], s=5, cmap='coolwarm', c=train_df[\"FloodProbability\"])\n",
        "  plt.xlabel('average_score')\n",
        "  plt.ylabel('FloodProbability')\n",
        "  plt.grid(linestyle='--')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0Bml55w3EnL"
      },
      "outputs": [],
      "source": [
        "if run_graph==0:\n",
        "  gb_fp = train_df.groupby([\"adj_average\"])[[\"FloodProbability\"]].mean().reset_index()\n",
        "\n",
        "  plt.scatter(gb_fp[\"adj_average\"], gb_fp[\"FloodProbability\"], s=7, cmap='coolwarm')\n",
        "  plt.xlabel('average_score')\n",
        "  plt.ylabel('FloodProbability')\n",
        "  plt.grid(linestyle='--')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ns-CSgVw9t96"
      },
      "source": [
        "### 3.3 Sorted Rows Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUtkHMDx4G5C"
      },
      "outputs": [],
      "source": [
        "to_drop = train_df[(train_df.FloodProbability==0.285)].index\n",
        "to_drop\n",
        "#train_df = train_df.drop(\"ISO\",axis=1)\n",
        "tot_features = test_df.columns\n",
        "train_df[(train_df.FloodProbability==0.285)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wh66P9hb510-"
      },
      "outputs": [],
      "source": [
        "df_sorted_test = sort_rows(test_df,features)\n",
        "cols_sorted = [f\"col_{i}\" for i in range(df_sorted_test.shape[1])]\n",
        "df_sorted_test = pd.DataFrame(data=df_sorted_test, columns=cols_sorted, index=test_df.index)\n",
        "df_sorted_test[\"FloodProbability\"] =np.nan\n",
        "df_sorted_test[\"col_12_11\"]=df_sorted_test[\"col_12\"]*df_sorted_test[\"col_8\"]\n",
        "\n",
        "df_sorted_train = sort_rows(train_df,features)\n",
        "cols_sorted = [f\"col_{i}\" for i in range(df_sorted_train.shape[1])]\n",
        "df_sorted_train = pd.DataFrame(data=df_sorted_train, columns=cols_sorted)\n",
        "df_sorted_train[\"FloodProbability\"] =train_df[\"FloodProbability\"].values\n",
        "df_sorted_train[\"col_12_11\"]=df_sorted_train[\"col_12\"]*df_sorted_train[\"col_8\"]\n",
        "\n",
        "df_sorted = pd.concat([df_sorted_train,df_sorted_test],axis=0)\n",
        "\n",
        "display(df_sorted.head())\n",
        "display(df_sorted.tail())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKcuQzaP_LKs"
      },
      "outputs": [],
      "source": [
        "if run_graph==1:\n",
        "  fig, axs = plt.subplots(2,1,figsize=(10,14))\n",
        "  axs = np.ravel(axs)\n",
        "\n",
        "  col_x=12\n",
        "  col_y=8\n",
        "\n",
        "  axs[0].scatter(df_sorted_train[f\"col_{col_x}\"], df_sorted_train[f\"col_{col_y}\"], s=16, cmap='viridis', c=df_sorted_train[\"FloodProbability\"])\n",
        "  axs[0].set_xlabel(f\"col_{col_x}\")\n",
        "  axs[0].set_ylabel(f\"col_{col_y}\")\n",
        "  axs[0].grid(linestyle='--')\n",
        "  axs[0].legend()\n",
        "\n",
        "  mask = np.zeros_like(df_sorted_train.corr(), dtype=bool)\n",
        "  mask[np.triu_indices_from(mask)] = True\n",
        "  np.fill_diagonal(mask, False)\n",
        "\n",
        "  g = sns.heatmap(df_sorted_train.corr(),\n",
        "                  annot=True,\n",
        "                  annot_kws={\"fontsize\": 9},\n",
        "                  fmt='.2f',\n",
        "                  linewidths=0.5,\n",
        "                  cmap='RdBu',\n",
        "                  mask=mask,\n",
        "                  ax=axs[1]# the mask has been included here\n",
        "                  )\n",
        "  axs[1].grid(False);"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_sorted.describe().T"
      ],
      "metadata": {
        "id": "-CFVz5LgZiXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlLPgy0AGjKU"
      },
      "source": [
        "#### Verify Duplicates:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ec9OXa_GGsfj"
      },
      "outputs": [],
      "source": [
        "duplicates = df_sorted[df_sorted.duplicated()]\n",
        "duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KTxEzn3uSAJJ"
      },
      "outputs": [],
      "source": [
        "dup_verify = df_sorted[(df_sorted.col_0==2)&(df_sorted.col_1==2)&(df_sorted.col_2==3)&(df_sorted.col_3==3)&(df_sorted.col_4==3)&(df_sorted.col_5==4)&\\\n",
        "                       (df_sorted.col_6==4)&(df_sorted.col_7==4)&(df_sorted.col_8==5)&(df_sorted.col_9==5)&(df_sorted.col_10==5)&(df_sorted.col_11==5)&\\\n",
        "                       (df_sorted.col_12==5)&(df_sorted.col_13==5)&(df_sorted.col_14==6)&(df_sorted.col_15==6)&(df_sorted.col_16==6)&(df_sorted.col_17==7)&\\\n",
        "                       (df_sorted.col_18==8)&(df_sorted.col_19==9)]\n",
        "\n",
        "dup_verify[dup_verify.FloodProbability.isna()==False].FloodProbability.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1hRxwWkZiyx"
      },
      "outputs": [],
      "source": [
        "mean_sorted_values = df_sorted_train.groupby([\"col_0\",\"col_1\",\"col_2\",\"col_3\",\"col_4\",\"col_5\",\"col_6\",\"col_7\",\"col_8\",\"col_9\",\"col_10\",\"col_11\",\"col_12\",\"col_13\",\"col_14\",\"col_15\",\n",
        "                         \"col_16\",\"col_17\",\"col_18\",\"col_19\"], as_index=False)[\"FloodProbability\"].agg([\"mean\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5c5xWt_tb4Z3"
      },
      "outputs": [],
      "source": [
        "tomerge_df = df_sorted_test.drop(\"FloodProbability\",axis=1)\n",
        "tomerge_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSq21FjkcYcc"
      },
      "outputs": [],
      "source": [
        "#tomerge_df=tomerge_df.merge(mean_sorted_values,left_on=list(tomerge_df.columns),right_on=list(tomerge_df.columns),how='left',suffixes=('_x', '_y'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDWXEZA7dxkz"
      },
      "outputs": [],
      "source": [
        "#tomerge_df.index=df_sorted_test.index\n",
        "#tomerge_df.rename({\"mean\":\"FloodProbability\"},axis=1,inplace=True)\n",
        "#tomerge_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4Y47TcUfvLm"
      },
      "outputs": [],
      "source": [
        "#duplicated_test_forecast = tomerge_df[[\"FloodProbability\"]].dropna()\n",
        "#duplicated_test_forecast.to_csv(\"test_duplicated.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjzHE-Y5aghJ"
      },
      "source": [
        "### 3.3 Single Features Analysis:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPuurvq_amSG"
      },
      "source": [
        "**MonsoonIntensity**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUhjTUg58nDc"
      },
      "outputs": [],
      "source": [
        "if run_graph==0:\n",
        "  fig, axs = plt.subplots(1,1,figsize=(23,4))\n",
        "  sns.boxplot(data=train_df, x=\"MonsoonIntensity\", y=\"FloodProbability\", palette=\"viridis\", ax=axs);\n",
        "  axs.grid(linestyle=\"--\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXWfRMnXcDvl"
      },
      "source": [
        "**TopographyDrainage**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_cADqVgWJot"
      },
      "outputs": [],
      "source": [
        "if run_graph==0:\n",
        "  fig, axs = plt.subplots(1,1,figsize=(23,4))\n",
        "  sns.boxplot(data=train_df, x=\"TopographyDrainage\", y=\"FloodProbability\", palette=\"autumn\", ax=axs);\n",
        "  axs.grid(linestyle=\"--\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTGG36htcSit"
      },
      "source": [
        "**RiverManagement**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmEteSlYcSi3"
      },
      "outputs": [],
      "source": [
        "if run_graph==0:\n",
        "  fig, axs = plt.subplots(1,1,figsize=(23,4))\n",
        "  sns.boxplot(data=train_df, x=\"RiverManagement\", y=\"FloodProbability\", palette=\"RdYlBu_r\", ax=axs);\n",
        "  axs.grid(linestyle=\"--\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjJcOfqIdX8r"
      },
      "source": [
        "**Deforestation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECFFrhZzdX8s"
      },
      "outputs": [],
      "source": [
        "if run_graph==0:\n",
        "  fig, axs = plt.subplots(1,1,figsize=(23,4))\n",
        "  sns.boxplot(data=train_df, x=\"Deforestation\", y=\"FloodProbability\", palette=\"RdYlBu_r\", ax=axs);\n",
        "  axs.grid(linestyle=\"--\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bqRPe9ao8ie"
      },
      "source": [
        "**Urbanization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HoI3DhL2cLq0"
      },
      "outputs": [],
      "source": [
        "if run_graph==0:\n",
        "  fig, axs = plt.subplots(1,1,figsize=(23,4))\n",
        "  sns.boxplot(data=train_df, x=\"Urbanization\", y=\"FloodProbability\", palette=\"RdYlBu_r\", ax=axs);\n",
        "  axs.grid(linestyle=\"--\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCGl6nyhpgmz"
      },
      "source": [
        "**ClimateChange**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKi_buuNpB1T"
      },
      "outputs": [],
      "source": [
        "if run_graph==0:\n",
        "  fig, axs = plt.subplots(1,1,figsize=(23,4))\n",
        "  sns.boxplot(data=train_df, x=\"ClimateChange\", y=\"FloodProbability\", palette=\"RdYlBu_r\", ax=axs);\n",
        "  axs.grid(linestyle=\"--\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYykFfdXRaYi"
      },
      "source": [
        "**DamsQuality**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1djxfo1RaYk"
      },
      "outputs": [],
      "source": [
        "if run_graph==0:\n",
        "  fig, axs = plt.subplots(1,1,figsize=(23,4))\n",
        "  sns.boxplot(data=train_df, x=\"DamsQuality\", y=\"FloodProbability\", palette=\"RdYlBu_r\", ax=axs);\n",
        "  axs.grid(linestyle=\"--\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fy91fRVIRtDn"
      },
      "source": [
        "**Siltation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4I8yO9OpRtDn"
      },
      "outputs": [],
      "source": [
        "if run_graph==0:\n",
        "  fig, axs = plt.subplots(1,1,figsize=(23,4))\n",
        "  sns.boxplot(data=train_df, x=\"Siltation\", y=\"FloodProbability\", palette=\"RdYlBu_r\", ax=axs);\n",
        "  axs.grid(linestyle=\"--\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nRAmuukRsoO"
      },
      "source": [
        "**AgriculturalPractices**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UW7eW7vMRsoO"
      },
      "outputs": [],
      "source": [
        "if run_graph==0:\n",
        "  fig, axs = plt.subplots(1,1,figsize=(23,4))\n",
        "  sns.boxplot(data=train_df, x=\"AgriculturalPractices\", y=\"FloodProbability\", palette=\"RdYlBu_r\", ax=axs);\n",
        "  axs.grid(linestyle=\"--\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKmIs0sxRsR8"
      },
      "source": [
        "**Encroachments**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6RiWO9yRsR9"
      },
      "outputs": [],
      "source": [
        "if run_graph==0:\n",
        "  fig, axs = plt.subplots(1,1,figsize=(23,4))\n",
        "  sns.boxplot(data=train_df, x=\"Encroachments\", y=\"FloodProbability\", palette=\"RdYlBu_r\", ax=axs);\n",
        "  axs.grid(linestyle=\"--\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bulb2whiRsAf"
      },
      "source": [
        "**IneffectiveDisasterPreparedness**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhWRk8XxRsAf"
      },
      "outputs": [],
      "source": [
        "if run_graph==0:\n",
        "  fig, axs = plt.subplots(1,1,figsize=(23,4))\n",
        "  sns.boxplot(data=train_df, x=\"DamsQuality\", y=\"IneffectiveDisasterPreparedness\", palette=\"RdYlBu_r\", ax=axs);\n",
        "  axs.grid(linestyle=\"--\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMxWJB0MRrqC"
      },
      "source": [
        "**DrainageSystems**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ntMaDDKRrqD"
      },
      "outputs": [],
      "source": [
        "if run_graph==0:\n",
        "  fig, axs = plt.subplots(1,1,figsize=(23,4))\n",
        "  sns.boxplot(data=train_df, x=\"DrainageSystems\", y=\"FloodProbability\", palette=\"RdYlBu_r\", ax=axs);\n",
        "  axs.grid(linestyle=\"--\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHukDxqBV56X"
      },
      "source": [
        "**CoastalVulnerability**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cBdCWF50V56e"
      },
      "outputs": [],
      "source": [
        "if run_graph==0:\n",
        "  fig, axs = plt.subplots(1,1,figsize=(23,4))\n",
        "  sns.boxplot(data=train_df, x=\"CoastalVulnerability\", y=\"FloodProbability\", palette=\"RdYlBu_r\", ax=axs);\n",
        "  axs.grid(linestyle=\"--\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiXStCVoV6Wr"
      },
      "source": [
        "**Landslides**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09Pm7SjoV6Wr"
      },
      "outputs": [],
      "source": [
        "if run_graph==0:\n",
        "  fig, axs = plt.subplots(1,1,figsize=(23,4))\n",
        "  sns.boxplot(data=train_df, x=\"Landslides\", y=\"FloodProbability\", palette=\"RdYlBu_r\", ax=axs);\n",
        "  axs.grid(linestyle=\"--\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ID26h5KKV6l-"
      },
      "source": [
        "**Watersheds**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3hiEoVDV6l-"
      },
      "outputs": [],
      "source": [
        "if run_graph==0:\n",
        "  fig, axs = plt.subplots(1,1,figsize=(23,4))\n",
        "  sns.boxplot(data=train_df, x=\"Watersheds\", y=\"FloodProbability\", palette=\"RdYlBu_r\", ax=axs);\n",
        "  axs.grid(linestyle=\"--\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8RL-CK9V618"
      },
      "source": [
        "**DeterioratingInfrastructure**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8FUTXvqV619"
      },
      "outputs": [],
      "source": [
        "if run_graph==0:\n",
        "  fig, axs = plt.subplots(1,1,figsize=(23,4))\n",
        "  sns.boxplot(data=train_df, x=\"DeterioratingInfrastructure\", y=\"FloodProbability\", palette=\"RdYlBu_r\", ax=axs);\n",
        "  axs.grid(linestyle=\"--\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QP1aqBmDV7DE"
      },
      "source": [
        "**PopulationScore**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DgK61soV7DE"
      },
      "outputs": [],
      "source": [
        "if run_graph==0:\n",
        "  fig, axs = plt.subplots(1,1,figsize=(23,4))\n",
        "  sns.boxplot(data=train_df, x=\"PopulationScore\", y=\"FloodProbability\", palette=\"RdYlBu_r\", ax=axs);\n",
        "  axs.grid(linestyle=\"--\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQ2xo4KdWaz0"
      },
      "source": [
        "**WetlandLoss**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JfRS4Q6Waz_"
      },
      "outputs": [],
      "source": [
        "if run_graph==0:\n",
        "  fig, axs = plt.subplots(1,1,figsize=(23,4))\n",
        "  sns.boxplot(data=train_df, x=\"WetlandLoss\", y=\"FloodProbability\", palette=\"RdYlBu_r\", ax=axs);\n",
        "  axs.grid(linestyle=\"--\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbu4wjE8WbUe"
      },
      "source": [
        "**PopulationScore**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JRqGM3lWbUf"
      },
      "outputs": [],
      "source": [
        "if run_graph==0:\n",
        "  fig, axs = plt.subplots(1,1,figsize=(23,4))\n",
        "  sns.boxplot(data=train_df, x=\"PopulationScore\", y=\"FloodProbability\", palette=\"RdYlBu_r\", ax=axs);\n",
        "  axs.grid(linestyle=\"--\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0l4RE4Y5XQl"
      },
      "source": [
        "## 4.0 Transform Features:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFDgCMkq5f9L"
      },
      "source": [
        "**TARGET**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qr3NAgTtRqXg"
      },
      "outputs": [],
      "source": [
        "y_scaler = MinMaxScaler()\n",
        "\n",
        "y = train_df[[\"FloodProbability\"]].copy()\n",
        "y_scaled = y_scaler.fit_transform(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yfgzxlw4ZWp"
      },
      "outputs": [],
      "source": [
        "train_df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNwmq6or6TnK"
      },
      "source": [
        "**FEATURES**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5lrjjH6RqUF"
      },
      "outputs": [],
      "source": [
        "features = ['MonsoonIntensity', 'TopographyDrainage', 'RiverManagement',\n",
        "       'Deforestation', 'Urbanization', 'ClimateChange', 'DamsQuality',\n",
        "       'Siltation', 'AgriculturalPractices', 'Encroachments',\n",
        "       'IneffectiveDisasterPreparedness', 'DrainageSystems',\n",
        "       'CoastalVulnerability', 'Landslides', 'Watersheds',\n",
        "       'DeterioratingInfrastructure', 'PopulationScore', 'WetlandLoss',\n",
        "       'InadequatePlanning', 'PoliticalFactors', 'col_11',\n",
        "       'col_12', 'col_8', 'average_score', 'median_score', 'hmean', 'delta',\n",
        "       'compound_effect', 'risk_mitigation_factors']\n",
        "\n",
        "to_scale = ['MonsoonIntensity', 'TopographyDrainage', 'RiverManagement',\n",
        "       'Deforestation', 'Urbanization', 'ClimateChange', 'DamsQuality',\n",
        "       'Siltation', 'AgriculturalPractices', 'Encroachments',\n",
        "       'IneffectiveDisasterPreparedness', 'DrainageSystems',\n",
        "       'CoastalVulnerability', 'Landslides', 'Watersheds',\n",
        "       'DeterioratingInfrastructure', 'PopulationScore', 'WetlandLoss',\n",
        "       'InadequatePlanning', 'PoliticalFactors', 'average_score',\n",
        "       'median_score', 'hmean', 'delta','compound_effect', 'risk_mitigation_factors']\n",
        "\n",
        "x_scaler = StandardScaler()\n",
        "X_train = train_df[features].copy()\n",
        "X_test = test_df[features].copy()\n",
        "\n",
        "X_train[to_scale] =  x_scaler.fit_transform(X_train[to_scale])\n",
        "X_test[to_scale] =  x_scaler.transform(X_test[to_scale])\n",
        "\n",
        "X_train_df = pd.DataFrame(index=train_df.index, columns=features, data=X_train)\n",
        "X_test_df = pd.DataFrame(index=test_df.index, columns=features, data=X_test)\n",
        "\n",
        "X_train_df[\"FloodProbability\"] = y_scaled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AuUf8OCh4Fy2"
      },
      "outputs": [],
      "source": [
        "X_train_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJl4vM30gWjb"
      },
      "outputs": [],
      "source": [
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gelVnyf8xMa"
      },
      "source": [
        "## 5.0 Models:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLFh7qF_g-Gl"
      },
      "source": [
        "### **LINEAR**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k84rxQBK95_l"
      },
      "source": [
        "### 5.1 RidgeRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOnTmZhfRqQ7"
      },
      "outputs": [],
      "source": [
        "def ridge_objective(trial):\n",
        "    params = {\n",
        "          'alpha':         trial.suggest_float('alpha', 0.00001, 10.0, log=True)\n",
        "            }\n",
        "\n",
        "\n",
        "    optuna_model = Ridge(**params)\n",
        "\n",
        "    skf = KFold(n_splits = 5,random_state = 42, shuffle = True)\n",
        "\n",
        "    optuna_score = cross_val_score(estimator=optuna_model, X=X_train, y=y_scaled, scoring=\"r2\", cv=skf, n_jobs=-1, verbose=0)\n",
        "\n",
        "    return np.mean(optuna_score)\n",
        "\n",
        "ridge_study = optuna.create_study(\n",
        "                                 direction = 'maximize', study_name=\"Ridge_v0\",\n",
        "                                 sampler = optuna.samplers.TPESampler(seed=42)\n",
        "                                 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_sjldyYRqOF"
      },
      "outputs": [],
      "source": [
        "if run_ridge==2:\n",
        "  ridge_study.optimize(ridge_objective, 31,  show_progress_bar=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2T3IrMrRsdg"
      },
      "outputs": [],
      "source": [
        "if run_ridge==2:\n",
        "  trial = ridge_study.best_trial\n",
        "  print('R2: {}'.format(trial.value))\n",
        "  print(\"Best hyperparameters: {}\".format(trial.params))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyFRlmSdKGIQ"
      },
      "source": [
        "**Best Score**\n",
        "\n",
        "- R2: 0.8490257434096522\n",
        "- Best hyperparameters: {'alpha': 1.9862905191888807}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSW12xI3RJjj"
      },
      "source": [
        "### 5.1 HuberRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRUjETI0RJj0"
      },
      "outputs": [],
      "source": [
        "def huber_objective(trial):\n",
        "    params = {\n",
        "          'alpha':         trial.suggest_float('alpha', 1e-4, 10.0, log=True),\n",
        "          \"epsilon\":       trial.suggest_float('epsilon', 1.35, 10.0, step=0.05)\n",
        "            }\n",
        "\n",
        "\n",
        "    optuna_model = HuberRegressor(**params)\n",
        "\n",
        "    skf = KFold(n_splits = 5,random_state = 42, shuffle = True)\n",
        "\n",
        "    optuna_score = cross_val_score(estimator=optuna_model, X=X_train, y=y_scaled, scoring=\"r2\", cv=skf, n_jobs=-1, verbose=0)\n",
        "\n",
        "    return np.mean(optuna_score)\n",
        "\n",
        "huber_study = optuna.create_study(\n",
        "                                 direction = 'maximize', study_name=\"huber_v1\",\n",
        "                                 sampler = optuna.samplers.TPESampler(seed=42)\n",
        "                                 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODZIG3vgRJj0"
      },
      "outputs": [],
      "source": [
        "if run_ridge==2:\n",
        "  huber_study.optimize(huber_objective, 31, show_progress_bar=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Qk2eoB2RqLM"
      },
      "outputs": [],
      "source": [
        "if run_ridge==2:\n",
        "  trial = huber_study.best_trial\n",
        "  print('R2: {}'.format(trial.value))\n",
        "  print(\"Best hyperparameters: {}\".format(trial.params))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZS0V32kKVBe"
      },
      "source": [
        "**Best Score**\n",
        "\n",
        "- R2: 0.8490258066852565\n",
        "- Best hyperparameters: {'alpha': 0.3671905753537766, 'epsilon': 4.5}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnAfLyNZYqLg"
      },
      "outputs": [],
      "source": [
        "0.8490258066852565\n",
        "0.8490257434096522\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKIkSGovNlQy"
      },
      "source": [
        "### 5.3 RANSACRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FGOuJ5QZY8Q"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import RANSACRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sdsw7zxsNsl3"
      },
      "outputs": [],
      "source": [
        "def ransac_objective(trial):\n",
        "    params = {\n",
        "          'min_samples':     trial.suggest_float('min_samples', 0.25, 0.8, step=0.025),\n",
        "            }\n",
        "\n",
        "\n",
        "    optuna_model = RANSACRegressor(estimator=Ridge(alpha=2.0), loss=\"squared_error\",**params)\n",
        "\n",
        "    skf = KFold(n_splits = 5,random_state = 42, shuffle = True)\n",
        "\n",
        "    optuna_score = cross_val_score(estimator=optuna_model, X=X_train, y=y_scaled, scoring=\"r2\", cv=skf, n_jobs=-1, verbose=0)\n",
        "\n",
        "    return np.mean(optuna_score)\n",
        "\n",
        "RANSAC_study = optuna.create_study(\n",
        "                                 direction = 'maximize', study_name=\"RANSACRegressor_v1\",\n",
        "                                 sampler = optuna.samplers.TPESampler(seed=42)\n",
        "                                 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXYeIhSySBgT"
      },
      "outputs": [],
      "source": [
        "if run_ridge==2:\n",
        "  RANSAC_study.optimize(ransac_objective, 10, show_progress_bar=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KasOfW0GSBgd"
      },
      "outputs": [],
      "source": [
        "if run_ridge==2:\n",
        "  trial = RANSAC_study.best_trial\n",
        "  print('R2: {}'.format(trial.value))\n",
        "  print(\"Best hyperparameters: {}\".format(trial.params))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKlvsi-2yYNS"
      },
      "source": [
        "**Best Score**\n",
        "\n",
        "- R2: 0.8490258798048684\n",
        "- Best hyperparameters: {'min_samples': 0.45}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCvzP6VEzjUe"
      },
      "source": [
        "### 5.4 ElasticNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KNNLjmc0R8Bh"
      },
      "outputs": [],
      "source": [
        "def elasticNet_objective(trial):\n",
        "    params = {\n",
        "          'alpha':     trial.suggest_float('alpha', 1e-5, 100.0, log=True),\n",
        "          'l1_ratio':  trial.suggest_float('l1_ratio', 0.05, 0.95, step=0.025)\n",
        "            }\n",
        "\n",
        "\n",
        "    optuna_model = ElasticNet(random_state=42,**params)\n",
        "\n",
        "    skf = KFold(n_splits = 5,random_state = 42, shuffle = True)\n",
        "\n",
        "    optuna_score = cross_val_score(estimator=optuna_model, X=X_train, y=y_scaled, scoring=\"r2\", cv=skf, n_jobs=-1, verbose=0)\n",
        "\n",
        "    return np.mean(optuna_score)\n",
        "\n",
        "eleastic_study = optuna.create_study(\n",
        "                                 direction = 'maximize', study_name=\"EleasticRegressor_v1\",\n",
        "                                 sampler = optuna.samplers.TPESampler(seed=42)\n",
        "                                 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCEVXn0W0wG4"
      },
      "outputs": [],
      "source": [
        "if run_ridge==2:\n",
        "  eleastic_study.optimize(elasticNet_objective, 51, show_progress_bar=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLW1JLQE1DB0"
      },
      "outputs": [],
      "source": [
        "if run_ridge==2:\n",
        "  trial = eleastic_study.best_trial\n",
        "  print('R2: {}'.format(trial.value))\n",
        "  print(\"Best hyperparameters: {}\".format(trial.params))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMHAilVt_Vr5"
      },
      "source": [
        "**Best Score**\n",
        "\n",
        "- R2: 0.8490257428157406\n",
        "- Best hyperparameters: {'alpha': 1.3468130933650185e-05, 'l1_ratio': 0.05}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PU8YObEJ2nTX"
      },
      "outputs": [],
      "source": [
        "0.8490258066852565\n",
        "0.8490257434096522\n",
        "0.8490258798048684\n",
        "0.8490257428157406"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_tTY88phDpg"
      },
      "source": [
        "### **TREE-BASED**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIw-eg6eFOLP"
      },
      "source": [
        "### 5.5 LightGBM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVB1hRbUFUBO"
      },
      "outputs": [],
      "source": [
        "def lgbm_objective(trial):\n",
        "    params = {\n",
        "            'num_leaves':         trial.suggest_int('num_leaves', 100, 111, step=1),\n",
        "            'min_child_samples':  trial.suggest_int('min_child_samples', 60, 70, step=1),\n",
        "            'min_child_weight' :  trial.suggest_float(\"min_child_weight\", 1e-2, 0.1, log=True),\n",
        "            \"reg_alpha\" :         trial.suggest_float(\"reg_alpha\", 1e-4, 0.001, log=True),\n",
        "            \"reg_lambda\" :        trial.suggest_float(\"reg_lambda\", 1e-3, 0.01, log=True),\n",
        "            \"max_depth\" :         trial.suggest_int('max_depth', 7, 11, step=1),\n",
        "            'bagging_freq' :      trial.suggest_int('bagging_freq', 5, 9),\n",
        "            'max_bin' :           trial.suggest_int('max_bin', 401, 501, step=5),\n",
        "            'feature_fraction':   trial.suggest_float(\"feature_fraction\", 0.85, 1.0, step=0.01),\n",
        "            \"learning_rate\" :     0.01,\n",
        "            'n_estimators':       2_000,\n",
        "            'random_state':       42,\n",
        "            'device':             \"cpu\"\n",
        "            }\n",
        "\n",
        "    if params[\"bagging_freq\"]>0:\n",
        "      params['bagging_fraction'] =  trial.suggest_float(\"bagging_fraction\", 0.75, 0.85, step=0.01)\n",
        "\n",
        "\n",
        "    optuna_model = LGBMRegressor(**params)\n",
        "\n",
        "    optuna_score = cross_validate_tuning(optuna_model, trial, features=features, train=X_train_df, test=X_test_df,\n",
        "                                         target_feat=\"FloodProbability\", n_repeats=1, pruning=True, es=True, model_type=\"lgbm\")\n",
        "\n",
        "    return np.mean(optuna_score)\n",
        "\n",
        "lgbm_study = optuna.create_study(\n",
        "                                 direction = 'minimize', study_name=\"lgbm_opt_all\",\n",
        "                                 sampler = optuna.samplers.TPESampler(seed=42),\n",
        "                                 pruner=optuna.pruners.MedianPruner(n_warmup_steps=31)\n",
        "                                 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qTana0-RGnNd"
      },
      "outputs": [],
      "source": [
        "if run_lgb==2:\n",
        "  lgbm_study.optimize(lgbm_objective, 51, show_progress_bar=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opZvWYZpL7_F"
      },
      "outputs": [],
      "source": [
        "if run_lgb==2:\n",
        "  trial = lgbm_study.best_trial\n",
        "  print('MSE: {}'.format(trial.value))\n",
        "  print(\"Best hyperparameters: {}\".format(trial.params))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XniqKPMpH6ui"
      },
      "outputs": [],
      "source": [
        "if run_lgb==2:\n",
        "  fig = optuna.visualization.plot_optimization_history(lgbm_study)\n",
        "  fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L73C1f1dXRh1"
      },
      "outputs": [],
      "source": [
        "if run_lgb==2:\n",
        "  fig = optuna.visualization.plot_param_importances(lgbm_study)\n",
        "  fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTYksvhuy6bV"
      },
      "source": [
        "**Experiment 1**\n",
        "\n",
        "- MSE: 0.0017314917811725675\n",
        "- Best hyperparameters:\n",
        "\n",
        "  {'num_leaves': 108, 'min_child_samples': 60, 'min_child_weight': 0.009869114516750276, 'reg_alpha': 0.005445305113175027, 'reg_lambda': 0.0011418268475368125, 'max_depth': 8, 'bagging_freq': 5, 'max_bin': 481, 'feature_fraction': 1.0, 'bagging_fraction': 0.8200000000000001}\n",
        "\n",
        "**Experiment 2**\n",
        "\n",
        "- MSE: 0.0017313802521396822\n",
        "- Best hyperparameters:\n",
        "\n",
        " {'num_leaves': 105, 'min_child_samples': 65, 'min_child_weight': 0.01598146044340705, 'reg_alpha': 0.00045943610934533695, 'reg_lambda': 0.004382282365789647, 'max_depth': 9, 'bagging_freq': 9, 'max_bin': 426, 'feature_fraction': 0.94, 'bagging_fraction': 0.8}\n",
        "\n",
        "**Experiment 3**\n",
        "\n",
        "- MSE: 0.0017312030127051945\n",
        "- Best hyperparameters:\n",
        "\n",
        " {'num_leaves': 108, 'min_child_samples': 63, 'min_child_weight': 0.036102158869668395, 'reg_alpha': 0.0004888277171563409, 'reg_lambda': 0.004923185364540501, 'max_depth': 9, 'bagging_freq': 7, 'max_bin': 501, 'feature_fraction': 1.0, 'bagging_fraction': 0.75}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mN0YkB7RdIlp"
      },
      "source": [
        "### 5.6 XGBRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "byYkfkhBdIl0"
      },
      "outputs": [],
      "source": [
        "def xgb_objective(trial):\n",
        "    params = {'grow_policy':        \"lossguide\", #trial.suggest_categorical('grow_policy', ['lossguide', \"depthwise\"]),\n",
        "              'objective':          'reg:squarederror',\n",
        "              'tree_method':        'hist',\n",
        "              'device':             \"cpu\",\n",
        "              'enable_categorical': True,\n",
        "              'verbosity':          0,\n",
        "              'n_estimators' :      2000,\n",
        "              'eta' :               0.0025,\n",
        "              'booster' :           \"gbtree\", #trial.suggest_categorical('booster', [\"gbtree\", \"dart\"]),\n",
        "              'max_depth' :         trial.suggest_int('max_depth', 5, 12),\n",
        "              'subsample' :         trial.suggest_float('subsample', .75, 0.95, step=0.01),\n",
        "              'colsample_bylevel':  trial.suggest_float('colsample_bylevel', .60, 0.70, step=0.01),\n",
        "              'gamma' :             trial.suggest_int('gamma', 0, 5),\n",
        "              'min_child_weight' :  trial.suggest_float('min_child_weight', 5, 15, step=0.25),\n",
        "              'reg_lambda' :        trial.suggest_float('reg_lambda', 1e-2, 1.0, log = True),\n",
        "              'reg_alpha' :         trial.suggest_float('reg_alpha', 1e-2, 1.0, log = True),\n",
        "              'max_bin' :           trial.suggest_int('max_bin', 401, 526, step=5),\n",
        "              }\n",
        "\n",
        "\n",
        "    optuna_model = XGBRegressor(**params)\n",
        "\n",
        "    optuna_score = cross_validate_tuning(optuna_model, trial, features=features, train=X_train_df, test=X_test_df,\n",
        "                                         target_feat=\"FloodProbability\", n_repeats=1, pruning=True, es=True, model_type=\"xgb\")\n",
        "\n",
        "    return np.mean(optuna_score)\n",
        "\n",
        "xgb_study = optuna.create_study(\n",
        "                                 direction = 'minimize', study_name=\"xgb_opt_all\",\n",
        "                                 sampler = optuna.samplers.TPESampler(seed=42),\n",
        "                                 pruner=optuna.pruners.MedianPruner(n_warmup_steps=31)\n",
        "                                 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydc_iiFddIl0"
      },
      "outputs": [],
      "source": [
        "if run_xgb==2:\n",
        "  xgb_study.optimize(xgb_objective, 51, show_progress_bar=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDx_b2JGdIl0"
      },
      "outputs": [],
      "source": [
        "if run_xgb==2:\n",
        "  trial = xgb_study.best_trial\n",
        "  print('MSE: {}'.format(trial.value))\n",
        "  print(\"Best hyperparameters: {}\".format(trial.params))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jh8aU2OYdIl0"
      },
      "outputs": [],
      "source": [
        "if run_xgb==2:\n",
        "  fig = optuna.visualization.plot_optimization_history(xgb_study)\n",
        "  fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KJQW0YNdIl1"
      },
      "outputs": [],
      "source": [
        "if run_xgb==2:\n",
        "  fig = optuna.visualization.plot_param_importances(xgb_study)\n",
        "  fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7LlY0YTdIl1"
      },
      "source": [
        "- MSE: 0.0017334538488432368\n",
        "- Best hyperparameters:\n",
        "{'max_depth': 8, 'subsample': 0.8500000000000001, 'colsample_bylevel': 0.65, 'gamma': 0, 'min_child_weight': 10.695814388584468, 'reg_lambda': 0.14948338043917855, 'reg_alpha': 0.025900915752743607, 'max_bin': 476}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qm8P9FatuB-"
      },
      "source": [
        "### 5.7 CatBoostRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQxxUdMytuCM"
      },
      "outputs": [],
      "source": [
        "def cat_objective(trial):\n",
        "    cb_params = {'grow_policy':         \"SymmetricTree\",\n",
        "                 'loss_function':       'RMSE',\n",
        "                 'eval_metric':         'RMSE',\n",
        "                 'task_type':           'CPU',\n",
        "                 'boosting_type':        'Plain',\n",
        "                 'verbose':              0,\n",
        "                 'n_estimators' :        1000,\n",
        "                 'learning_rate' :       0.015,\n",
        "                 'early_stopping_rounds': 101,\n",
        "                 'boost_from_average':   True,\n",
        "                 'colsample_bylevel'     : trial.suggest_float(\"colsample_bylevel\", 0.4, 1.0, step=0.01), #\n",
        "                 'max_depth'             : trial.suggest_int('max_depth', 5, 16), #\n",
        "                 'l2_leaf_reg'           : trial.suggest_float(\"l2_leaf_reg\", 0.001, 10.0, log=True), #\n",
        "                 'min_data_in_leaf'      : trial.suggest_int('min_data_in_leaf', 35, 60),\n",
        "                 'random_strength'       : trial.suggest_float(\"random_strength\", 1.0, 10.0, step=0.1),\n",
        "                 'max_bin'               : trial.suggest_int('max_bin', 201, 601, step=10),\n",
        "                 \"bootstrap_type\"        : trial.suggest_categorical(\"bootstrap_type\", [\"Bayesian\", \"Bernoulli\"]),\n",
        "                 #\"bagging_temperature\"   : trial.suggest_float('bagging_temperature', 4, 7),\n",
        "                }\n",
        "\n",
        "\n",
        "\n",
        "    if cb_params[\"bootstrap_type\"] == \"Bayesian\":\n",
        "        cb_params[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0, 10)\n",
        "    elif cb_params[\"bootstrap_type\"] == \"Bernoulli\":\n",
        "        cb_params[\"subsample\"] = trial.suggest_float(\"subsample\", 0.2, 1, log=True)\n",
        "\n",
        "    optuna_model = CatBoostRegressor(**cb_params)\n",
        "\n",
        "    optuna_score = cross_validate_tuning(optuna_model, trial, features=features, train=X_train_df, test=X_test_df,\n",
        "                                         target_feat=\"FloodProbability\", n_repeats=1, pruning=True, es=True, model_type=\"cat\")\n",
        "\n",
        "    return np.mean(optuna_score)\n",
        "\n",
        "cat_study = optuna.create_study(\n",
        "                                 direction = 'minimize', study_name=\"cat_opt_all\",\n",
        "                                 sampler = optuna.samplers.TPESampler(seed=42),\n",
        "                                 pruner=optuna.pruners.MedianPruner(n_warmup_steps=31)\n",
        "                                 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZvwonc8tuCO"
      },
      "outputs": [],
      "source": [
        "if run_cat==2:\n",
        "  cat_study.optimize(cat_objective, 51, show_progress_bar=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-GZoFr_tuCP"
      },
      "outputs": [],
      "source": [
        "if run_cat==2:\n",
        "  trial = cat_study.best_trial\n",
        "  print('MSE: {}'.format(trial.value))\n",
        "  print(\"Best hyperparameters: {}\".format(trial.params))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6VBLAvK9tuCP"
      },
      "outputs": [],
      "source": [
        "if run_cat==2:\n",
        "  fig = optuna.visualization.plot_optimization_history(cat_study)\n",
        "  fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zzz1Ye99tuCQ"
      },
      "outputs": [],
      "source": [
        "if run_cat==2:\n",
        "  fig = optuna.visualization.plot_param_importances(cat_study)\n",
        "  fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ihx1pm5DtuCQ"
      },
      "source": [
        "- MSE: 0.0017419265884232565\n",
        "- Best hyperparameters:\n",
        "\n",
        "{'colsample_bylevel': 0.91, 'max_depth': 16, 'l2_leaf_reg': 2.893709030040562, 'min_data_in_leaf': 50, 'random_strength': 1.0, 'max_bin': 401, 'bootstrap_type': 'Bernoulli', 'subsample': 0.8772046302417569}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eUVAN6uhH7O"
      },
      "source": [
        "### **NEURAL NETWORK**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ik1EXI1eI10b"
      },
      "source": [
        "#### Training Function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Dl-qSSwyVCj"
      },
      "outputs": [],
      "source": [
        "X_test_df[\"FloodProbability\"] = 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0N-glkxw0vQp"
      },
      "outputs": [],
      "source": [
        "class dense_block(tf.keras.layers.Layer):\n",
        "    def __init__(self, units, dropout_rate=0.25, activation=\"relu\", kr=0, name=\"drb\"):   #tf.keras.regularizers.L2(l2=0.01)\n",
        "        super(dense_block, self).__init__()\n",
        "        self.units = units\n",
        "\n",
        "        self.linear_dense = tf.keras.layers.Dense(units, name=f\"lin_dense_0_{name}\")\n",
        "        self.project = tf.keras.layers.Dense(units, name=f\"lin_dense_prj_{name}\")\n",
        "\n",
        "        self.batchnorm_0 = tf.keras.layers.BatchNormalization(name=f\"bn_0_{name}\")\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout_rate,name=f\"do_0_{name}\")\n",
        "\n",
        "        if activation==\"gelu\":\n",
        "          self.activation_0 = tf.keras.activations.gelu\n",
        "          self.dense_0 = tf.keras.layers.Dense(units, kernel_initializer=\"he_normal\", kernel_regularizer = tf.keras.regularizers.L2(l2=kr), name=f\"dense_0_{name}\")\n",
        "        if activation==\"relu\":\n",
        "          self.activation_0 = tf.keras.activations.relu\n",
        "          self.dense_0 = tf.keras.layers.Dense(units, kernel_initializer=\"he_normal\", kernel_regularizer = tf.keras.regularizers.L2(l2=kr), name=f\"dense_0_{name}\")\n",
        "        if activation==\"elu\":\n",
        "          self.activation_0 = tf.keras.activations.elu\n",
        "          self.dense_0 = tf.keras.layers.Dense(units, kernel_initializer=\"he_normal\", kernel_regularizer = tf.keras.regularizers.L2(l2=kr), name=f\"dense_0_{name}\")\n",
        "        if activation==\"swish\":\n",
        "          self.activation_0 = tf.keras.activations.swish\n",
        "          self.dense_0 = tf.keras.layers.Dense(units, kernel_initializer=\"he_normal\", kernel_regularizer = tf.keras.regularizers.L2(l2=kr), name=f\"dense_0_{name}\")\n",
        "        if activation==\"selu\":\n",
        "          self.activation_0 = tf.keras.activations.selu\n",
        "#          self.dropout = tf.keras.layers.AlphaDropout(dropout_rate,name=f\"alphado_0_{name}\")\n",
        "          self.dense_0 = tf.keras.layers.Dense(units, kernel_initializer=\"lecun_normal\", kernel_regularizer = tf.keras.regularizers.L2(l2=kr), name=f\"dense_0_{name}\")\n",
        "        if activation==\"leaky_relu\":\n",
        "          self.activation_0 = tf.keras.layers.LeakyReLU()\n",
        "          self.dense_0 = tf.keras.layers.Dense(units, kernel_initializer=\"he_normal\", kernel_regularizer = tf.keras.regularizers.L2(l2=kr), name=f\"dense_0_{name}\")\n",
        "        if activation==\"prelu\":\n",
        "          self.activation_0 = tf.keras.layers.PReLU()\n",
        "          self.dense_0 = tf.keras.layers.Dense(units, kernel_initializer=\"he_normal\", kernel_regularizer = tf.keras.regularizers.L2(l2=kr), name=f\"dense_0_{name}\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "\n",
        "        x = self.dense_0(inputs)\n",
        "        x = self.batchnorm_0(x)\n",
        "        x = self.activation_0(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class dense_residual_block(tf.keras.layers.Layer):\n",
        "    def __init__(self, units, dropout_rate=0.25, activation=\"relu\", kr=0, attention=False, name=\"drb\", norm=\"batch\"):   #tf.keras.regularizers.L2(l2=0.01)\n",
        "        super(dense_residual_block, self).__init__()\n",
        "        self.units = units\n",
        "\n",
        "        self.linear_dense = tf.keras.layers.Dense(units, name=f\"lin_dense_0_{name}\")\n",
        "        self.project = tf.keras.layers.Dense(units, name=f\"lin_dense_prj_{name}\")\n",
        "\n",
        "        if norm==\"batch\":\n",
        "          self.batchnorm_0 = tf.keras.layers.BatchNormalization(name=f\"bn_0_{name}\")\n",
        "          self.batchnorm_1 = tf.keras.layers.BatchNormalization(name=f\"bn_1_{name}\")\n",
        "          self.batchnorm_prj = tf.keras.layers.BatchNormalization(name=f\"bn_prj_{name}\")  #LayerNormalization()\n",
        "\n",
        "        if norm==\"layer\":\n",
        "          self.batchnorm_0 = tf.keras.layers.LayerNormalization(name=f\"bn_0_lr_{name}\")\n",
        "          self.batchnorm_1 = tf.keras.layers.LayerNormalization(name=f\"bn_1_lr_{name}\")\n",
        "          self.batchnorm_prj = tf.keras.layers.LayerNormalization(name=f\"bn_prj_lr_{name}\")  #LayerNormalization()\n",
        "\n",
        "\n",
        "        self.layer_norm = tf.keras.layers.BatchNormalization(name=f\"bn_2_{name}\")\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout_rate,name=f\"do_0_{name}\")\n",
        "        self.add_layer = tf.keras.layers.Add(name=f\"add_0_{name}\")\n",
        "\n",
        "        self.attention=attention\n",
        "        self.attention_layer = tf.keras.layers.Attention(name=f\"attention_{name}\")\n",
        "\n",
        "        if activation==\"gelu\":\n",
        "          self.activation_0 = tf.keras.activations.gelu\n",
        "          self.activation_1 = tf.keras.activations.gelu\n",
        "          self.dense_0 = tf.keras.layers.Dense(units, kernel_initializer=\"he_normal\",\n",
        "                                               kernel_regularizer = tf.keras.regularizers.L2(l2=kr),\n",
        "                                               name=f\"dense_0_{name}\")\n",
        "        if activation==\"relu\":\n",
        "          self.activation_0 = tf.keras.activations.relu\n",
        "          self.activation_1 = tf.keras.activations.relu\n",
        "          self.dense_0 = tf.keras.layers.Dense(units, kernel_initializer=\"he_normal\",\n",
        "                                               kernel_regularizer = tf.keras.regularizers.L2(l2=kr),\n",
        "                                               name=f\"dense_0_{name}\")\n",
        "        if activation==\"elu\":\n",
        "          self.activation_0 = tf.keras.activations.elu\n",
        "          self.activation_1 = tf.keras.activations.elu\n",
        "          self.dense_0 = tf.keras.layers.Dense(units, kernel_initializer=\"he_normal\",\n",
        "                                               kernel_regularizer = tf.keras.regularizers.L2(l2=kr),\n",
        "                                               name=f\"dense_0_{name}\")\n",
        "        if activation==\"swish\":\n",
        "          self.activation_0 = tf.keras.activations.swish\n",
        "          self.activation_1 = tf.keras.activations.swish\n",
        "          self.dense_0 = tf.keras.layers.Dense(units, kernel_initializer=\"he_normal\",\n",
        "                                               kernel_regularizer = tf.keras.regularizers.L2(l2=kr),\n",
        "                                               name=f\"dense_0_{name}\")\n",
        "        if activation==\"selu\":\n",
        "          self.activation_0 = tf.keras.activations.selu\n",
        "          self.activation_1 = tf.keras.activations.selu\n",
        "          self.dense_0 = tf.keras.layers.Dense(units, kernel_initializer=\"lecun_normal\",\n",
        "                                               kernel_regularizer = tf.keras.regularizers.L2(l2=kr),\n",
        "                                               name=f\"dense_0_{name}\")\n",
        "        if activation==\"leaky_relu\":\n",
        "          self.activation_0 = tf.keras.layers.LeakyReLU()\n",
        "          self.activation_1 = tf.keras.layers.LeakyReLU()\n",
        "          self.dense_0 = tf.keras.layers.Dense(units, kernel_initializer=\"he_normal\",\n",
        "                                               kernel_regularizer = tf.keras.regularizers.L2(l2=kr),\n",
        "                                               name=f\"dense_0_{name}\")\n",
        "        if activation==\"prelu\":\n",
        "          self.activation_0 = tf.keras.layers.PReLU()\n",
        "          self.activation_1 = tf.keras.layers.PReLU()\n",
        "          self.dense_0 = tf.keras.layers.Dense(units, kernel_initializer=\"he_normal\",\n",
        "                                               kernel_regularizer = tf.keras.regularizers.L2(l2=kr),\n",
        "                                               name=f\"dense_0_{name}\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "\n",
        "        if self.attention==True:\n",
        "          attention = self.attention_layer([inputs, inputs])\n",
        "        else:\n",
        "          attention = inputs\n",
        "\n",
        "        x = self.dense_0(attention)\n",
        "        x = self.batchnorm_0(x)\n",
        "        x = self.activation_0(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.linear_dense(x)\n",
        "        x = self.batchnorm_1(x)\n",
        "\n",
        "        if attention.shape[-1] != self.units:\n",
        "            inputs = self.project(attention)\n",
        "            inputs = self.batchnorm_prj(inputs)\n",
        "\n",
        "        return self.add_layer([x, inputs])\n",
        "\n",
        "\n",
        "def create_mlp(hidden_units, dropout_rate, activation, normalization_layer, name=None):\n",
        "    mlp_layers = []\n",
        "    if activation==\"relu\":\n",
        "        activation_0 = tf.keras.activations.relu\n",
        "    if activation==\"prelu\":\n",
        "        activation_0 = tf.keras.layers.PReLU()\n",
        "    if activation==\"elu\":\n",
        "        activation_0 = tf.keras.activations.elu\n",
        "    if activation==\"swish\":\n",
        "        activation_0 = tf.keras.activations.swish\n",
        "    if activation==\"selu\":\n",
        "        activation_0 = tf.keras.activations.selu\n",
        "    if activation==\"leaky_relu\":\n",
        "        activation_0 = tf.keras.layers.LeakyReLU()\n",
        "    if activation==\"gelu\":\n",
        "        activation_0 = tf.keras.activations.gelu\n",
        "\n",
        "    for units in hidden_units:\n",
        "        mlp_layers.append(normalization_layer()),\n",
        "        if activation == \"selu\":\n",
        "          mlp_layers.append(layers.Dense(units, activation=activation_0, kernel_initializer=\"lecun_normal\"))\n",
        "          mlp_layers.append(layers.AlphaDropout(dropout_rate))\n",
        "        else:\n",
        "          if activation in [\"prelu\",\"leaky_relu\"]:\n",
        "            mlp_layers.append(layers.Dense(units))\n",
        "            mlp_layers.append(activation_0)\n",
        "            mlp_layers.append(layers.Dropout(dropout_rate))\n",
        "          else:\n",
        "            mlp_layers.append(layers.Dense(units,activation=activation_0))\n",
        "            mlp_layers.append(layers.Dropout(dropout_rate))\n",
        "\n",
        "    return keras.Sequential(mlp_layers, name=name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qbFk_8KI1pE"
      },
      "outputs": [],
      "source": [
        "def dataframe_to_dataset(dataframe, shuffle=False, batch_size=64):\n",
        "    dataframe = dataframe.copy()\n",
        "    labels = dataframe[\"FloodProbability\"]\n",
        "    dataframe = dataframe.drop(columns=[\"FloodProbability\"])\n",
        "    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
        "    if shuffle:\n",
        "      ds = ds.shuffle(buffer_size=len(dataframe))\n",
        "    ds = ds.batch(batch_size)\n",
        "    ds = ds.prefetch(batch_size)\n",
        "    return ds\n",
        "\n",
        "\n",
        "feat_used = ['MonsoonIntensity', 'TopographyDrainage', 'RiverManagement',\n",
        "       'Deforestation', 'Urbanization', 'ClimateChange', 'DamsQuality',\n",
        "       'Siltation', 'AgriculturalPractices', 'Encroachments',\n",
        "       'IneffectiveDisasterPreparedness', 'DrainageSystems',\n",
        "       'CoastalVulnerability', 'Landslides', 'Watersheds',\n",
        "       'DeterioratingInfrastructure', 'PopulationScore', 'WetlandLoss',\n",
        "       'InadequatePlanning', 'PoliticalFactors', 'col_11', 'col_12', 'col_8',\n",
        "       'average_score', 'median_score', 'hmean', 'delta', 'compound_effect',\n",
        "       'risk_mitigation_factors']\n",
        "\n",
        "def run_experiment(func_model, train, test_data, best_params, experiment_name=\"baseline_nn\", splits=5, n_repeats=5, rs=42, target=\"FloodProbability\",\n",
        "                   batch_size=64, num_epochs=200, learning_rate=0.005):\n",
        "\n",
        "  skf = RepeatedStratifiedKFold(n_splits=splits, n_repeats=n_repeats, random_state=rs)\n",
        "\n",
        "  test_predictions = np.zeros((len(test_df),1))\n",
        "  test_results_df = pd.DataFrame(index=test_data.index, columns=[target])\n",
        "\n",
        "  all_logloss = []\n",
        "  all_R2_pr = []\n",
        "  oof_results = np.empty(shape=(train.shape[0],1))\n",
        "\n",
        "  for i, (train_index, valid_index) in enumerate(skf.split(train,train[target])):\n",
        "\n",
        "    print(f\"\\nRunning CV {i}\\n\")\n",
        "    ########################################################################## Prepare the Dataset:\n",
        "    X_trn = train.iloc[train_index,:]\n",
        "    X_val = train.iloc[valid_index,:]\n",
        "\n",
        "    X = X_trn.drop(columns=[target]).copy()\n",
        "    y = X_trn[target].copy()\n",
        "\n",
        "    val_X = X_val.drop(columns=[target]).copy()\n",
        "    val_y = X_val[target].copy()\n",
        "\n",
        "    X_test = test_data.copy()\n",
        "    #################################################################### Prepare Datasets loaders:\n",
        "\n",
        "    train_dataset = dataframe_to_dataset(X_trn, batch_size=batch_size, shuffle=True)\n",
        "    valid_dataset = dataframe_to_dataset(X_val, batch_size=batch_size, shuffle=False)\n",
        "    test_dataset = dataframe_to_dataset(X_test, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    feature_space_dict = FeatureSpace(\n",
        "                                  features={**{a:FeatureSpace.float() for a in feat_used}},\n",
        "                                  output_mode=\"dict\"\n",
        "                                  )\n",
        "\n",
        "    train_ds_with_no_labels = train_dataset.map(lambda x, *_: x)\n",
        "    print(\"Adapting Features Space....\")\n",
        "    feature_space_dict.adapt(train_ds_with_no_labels)\n",
        "\n",
        "    preprocessed_train_ds = train_dataset.map(lambda x, y: (feature_space_dict(x), y), num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
        "    preprocessed_valid_ds = valid_dataset.map(lambda x, y: (feature_space_dict(x), y), num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
        "    preprocessed_test_ds = test_dataset.map(lambda x, y: (feature_space_dict(x), y), num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    ##################################################################### Relevant Folders\n",
        "    folders_experiment = f\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Models/S4E5_Flood/neural_networks/{experiment_name}/\"\n",
        "    folders_experiment_cv1 = f\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Models/S4E5_Flood/neural_networks/\"\n",
        "    folders_experiment_cv2= f\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Models/S4E5_Flood/neural_networks/{experiment_name}\"\n",
        "    folders_experiment_cv3 = f\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Models/S4E5_Flood/neural_networks/{experiment_name}/cv_{i}/\"\n",
        "    folder_data = f\"/content/drive/MyDrive/Exercises/Studies_Structured_Data//Data/S4E4_Abalone\"\n",
        "    list_directories = [folder_data,folders_experiment,folders_experiment_cv1,folders_experiment_cv2,folders_experiment_cv3]\n",
        "\n",
        "    for path in list_directories:\n",
        "      try:\n",
        "          os.mkdir(path)\n",
        "      except OSError as error:\n",
        "          print(f\"{path} already exists\")\n",
        "    ##################################################################### Generate and Fit Model\n",
        "    # Callbacks:\n",
        "    checkpoint_filepath = folders_experiment + f'checkpoint/{experiment_name}.weights.h5'\n",
        "\n",
        "    # Generate the Model:\n",
        "    model = func_model(feature_space_dict, name=experiment_name, learning_rate = learning_rate, **best_params)\n",
        "\n",
        "    print(\"Start training the model...\")\n",
        "    history = model.fit(preprocessed_train_ds,\n",
        "                        epochs=num_epochs,\n",
        "                        callbacks=[keras.callbacks.EarlyStopping(monitor='val_msle', patience=21, mode=\"min\",\n",
        "                                                  start_from_epoch=5,restore_best_weights=True),\n",
        "                                   keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath,\n",
        "                                                    save_weights_only=True,\n",
        "                                                    monitor=\"val_msle\",\n",
        "                                                    mode='min',\n",
        "                                                    save_best_only=True),\n",
        "                                   keras.callbacks.ReduceLROnPlateau(monitor='val_msle', factor=0.5,\n",
        "                                                          patience=5, min_lr=0.0001, mode=\"min\")],\n",
        "                        validation_data=preprocessed_valid_ds)\n",
        "    print(\"Model training finished\")\n",
        "\n",
        "    model.load_weights(checkpoint_filepath)\n",
        "    model.evaluate(preprocessed_valid_ds, verbose=0)\n",
        "\n",
        "    plot_training_session(history)\n",
        "\n",
        "    oof_res = model.predict(preprocessed_valid_ds)\n",
        "    test_prob = model.predict(preprocessed_test_ds)\n",
        "\n",
        "    oof_res = oof_res.clip(1,29)\n",
        "    test_prob = test_prob.clip(1,29)\n",
        "\n",
        "    print(f\"Out-of-Fold Shapes: {oof_results[valid_index].shape},{oof_res.shape}\")\n",
        "\n",
        "    oof_results[valid_index] += oof_res/n_repeats\n",
        "    r2_score = r2_score(val_y, oof_res)\n",
        "\n",
        "    test_predictions += test_prob/skf.get_n_splits()\n",
        "\n",
        "    ##################################################################### Save the Model\n",
        "    model.save(f\"{folders_experiment_cv3}/model_{experiment_name}.keras\")\n",
        "    feature_space_dict.save(f\"{folders_experiment_cv3}/myfeaturespace_{experiment_name}.keras\")\n",
        "\n",
        "    print(f\"Test R2 Score: {round(r2_score, 3)}%\")\n",
        "    all_R2_pr.append(round(r2_score, 3))\n",
        "\n",
        "    gc.collect()\n",
        "\n",
        "  ##################################################################### Create Model Output\n",
        "  test_results_df.loc[:,:] = test_predictions\n",
        "\n",
        "  print(f\"All Valuation R2: {all_R2_pr}\")\n",
        "\n",
        "  return test_results_df, oof_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8s7SoQk0nfmg"
      },
      "source": [
        "### 5.8 Baseline NN:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_on=True"
      ],
      "metadata": {
        "id": "dTckxv5Jjbx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VBhFcY2oDbX"
      },
      "outputs": [],
      "source": [
        "if train_on==True:\n",
        "  train_dataset = dataframe_to_dataset(X_train_df, batch_size=64, shuffle=True)\n",
        "  test_dataset = dataframe_to_dataset(X_test_df, batch_size=64, shuffle=False)\n",
        "\n",
        "  feature_space_dict = FeatureSpace(\n",
        "                                features={**{a:FeatureSpace.float() for a in feat_used}},\n",
        "                                output_mode=\"dict\"\n",
        "                                )\n",
        "\n",
        "  train_ds_with_no_labels = train_dataset.map(lambda x, *_: x)\n",
        "  print(\"Adapting Features Space....\")\n",
        "  feature_space_dict.adapt(train_ds_with_no_labels)\n",
        "\n",
        "  preprocessed_train_ds = train_dataset.map(lambda x, y: (feature_space_dict(x), y), num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
        "  preprocessed_test_ds = test_dataset.map(lambda x, y: (feature_space_dict(x), y), num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcuaioBpXOtp"
      },
      "outputs": [],
      "source": [
        "def create_spaghetti(fs, name=\"baseline_v0\",\n",
        "                     learning_rate = 0.001,\n",
        "                     main_activation=\"selu\",\n",
        "                     dropout=0.5,\n",
        "                     dense_blocks=5,\n",
        "                     units_dense=64,\n",
        "                     kr=0.00001):\n",
        "\n",
        "\n",
        "    encoded_features = fs.get_encoded_features()\n",
        "\n",
        "    list_inputs=[]\n",
        "    for k in encoded_features.keys():\n",
        "      list_inputs.append(encoded_features[k])\n",
        "\n",
        "    x = layers.concatenate(list_inputs, name=\"input_concat\")\n",
        "\n",
        "    for block in range(dense_blocks):\n",
        "      x0=dense_block(units_dense, dropout_rate=dropout, activation=main_activation, kr=kr, name=f\"drb_{block}\")(x)\n",
        "      x = tf.keras.layers.concatenate([x,x0])\n",
        "\n",
        "\n",
        "    output = tf.keras.layers.Dense(1, activation=\"sigmoid\", name=\"output_layer\")(x)\n",
        "\n",
        "\n",
        "    model = keras.Model(inputs=encoded_features, outputs=output)\n",
        "\n",
        "\n",
        "\n",
        "    model.compile(\n",
        "                  optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "                  loss=tf.keras.losses.MeanSquaredError(name='mse'),\n",
        "                  metrics=[tf.keras.metrics.MeanSquaredError(name=\"mse\"),\n",
        "                          tf.keras.metrics.R2Score(name=\"R2_Score\")]\n",
        "                )\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzX0PVCIXOpi"
      },
      "outputs": [],
      "source": [
        "if train_on==True:\n",
        "\n",
        "  model = create_spaghetti(feature_space_dict)\n",
        "  model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppMnTepYXOlS"
      },
      "outputs": [],
      "source": [
        "#plot_model(\n",
        "#            model,\n",
        "#            show_shapes=True,\n",
        "#            show_layer_names=True,\n",
        "#            rankdir=\"LR\",\n",
        "#            expand_nested=True,\n",
        "#            dpi=200,\n",
        "#            show_trainable=True\n",
        "#            )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUuDPs8XZE2T"
      },
      "source": [
        "##### **KerasTuner**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71B2rL07ZE2T"
      },
      "outputs": [],
      "source": [
        "tuning_on=False\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WiKFTRur5n-3"
      },
      "outputs": [],
      "source": [
        "if tuning_on==True:\n",
        "  X_train_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lRvG1S-ZE2T"
      },
      "outputs": [],
      "source": [
        "if tuning_on==True:\n",
        "  X_trn, X_val = train_test_split(X_train_df, random_state=1978, test_size=0.4, stratify=X_train_df[\"FloodProbability\"])\n",
        "  X_trn, X_val = train_test_split(X_trn, random_state=1978, test_size=0.3, stratify=X_trn[\"FloodProbability\"])\n",
        "\n",
        "  feature_space_dict = FeatureSpace(\n",
        "                                features={**{a:FeatureSpace.float() for a in feat_used}},\n",
        "                                output_mode=\"dict\"\n",
        "                                )\n",
        "\n",
        "\n",
        "  train_dataset = dataframe_to_dataset(X_trn, batch_size=256, shuffle=True)\n",
        "  valid_dataset = dataframe_to_dataset(X_val, batch_size=256, shuffle=False)\n",
        "\n",
        "  train_ds_with_no_labels = train_dataset.map(lambda x, *_: x)\n",
        "  feature_space_dict.adapt(train_ds_with_no_labels)\n",
        "\n",
        "  preprocessed_train_ds = train_dataset.map(lambda x, y: (feature_space_dict(x), y), num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
        "  preprocessed_valid_ds = valid_dataset.map(lambda x, y: (feature_space_dict(x), y), num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZ9-XtraZE2U"
      },
      "outputs": [],
      "source": [
        "if tuning_on==True:\n",
        "  # Define the hyperparameter search space: EXPERIMENT 1\n",
        "  hp = kt.HyperParameters()\n",
        "  hp.Choice('main_activation', [\"relu\",\"prelu\"])\n",
        "  hp.Float('dropout',0.15,0.40, step=0.025)\n",
        "  hp.Choice('dense_blocks', values=[4,5,6])\n",
        "  hp.Choice('units_dense', values=[128,256,512])\n",
        "  #hp.Float('kr', 0.00001, 1.0, step=10,sampling=\"log\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-zfpFO1raRE"
      },
      "source": [
        "    Best val_R2_Score So Far: 0.8667208552360535\n",
        "    Total elapsed time: 09h 46m 45s\n",
        "\n",
        "    Search: Running Trial #12\n",
        "\n",
        "    Value             |Best Value So Far |Hyperparameter\n",
        "    swish             |relu              |main_activation\n",
        "    0.175             |0.25              |dropout\n",
        "    4                 |4                 |dense_blocks\n",
        "    256               |256               |units_dense\n",
        "\n",
        "    Best val_R2_Score So Far: 0.8667084574699402\n",
        "    Total elapsed time: 21h 09m 30s\n",
        "\n",
        "    Search: Running Trial #33\n",
        "\n",
        "    Value             |Best Value So Far |Hyperparameter\n",
        "    prelu             |prelu             |main_activation\n",
        "    0.2               |0.2               |dropout\n",
        "    5                 |5                 |dense_blocks\n",
        "    256               |256               |units_dense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSZJWADkZE2U"
      },
      "outputs": [],
      "source": [
        "# TRAIN MODEL\n",
        "# Define Preprocessing steps:\n",
        "def create_turner_model(hp):\n",
        "\n",
        "  model = create_spaghetti(feature_space_dict,\n",
        "                           name=\"baseline\",\n",
        "                           learning_rate = 0.005,\n",
        "                           main_activation=hp.get('main_activation'),\n",
        "                           dropout=hp.get('dropout'),\n",
        "                           dense_blocks=hp.get('dense_blocks'),\n",
        "                           units_dense = hp.get('units_dense'),\n",
        "                           kr = 0)\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vx8bEKD5ZE2U"
      },
      "outputs": [],
      "source": [
        "if tuning_on==True:\n",
        "  # Create a tuner and search for the best hyperparameters\n",
        "  tuner = BayesianOptimization(create_turner_model,\n",
        "                              objective=kt.Objective(\"val_R2_Score\", \"max\"),\n",
        "                              hyperparameters=hp, max_trials=61, overwrite=True)\n",
        "\n",
        "  stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_R2_Score', patience=7, mode=\"max\", start_from_epoch=5)\n",
        "  reduce_ = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_R2_Score', factor=0.5, patience=3, min_lr=0.0001, mode=\"max\")\n",
        "\n",
        "  tuner.search(preprocessed_train_ds, validation_data=preprocessed_valid_ds, epochs=31, callbacks=[stop_early,reduce_])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5R0sF_tZE2V"
      },
      "outputs": [],
      "source": [
        "if tuning_on==True:\n",
        "  print(tuner.get_best_hyperparameters(4)[0].values)\n",
        "  print(tuner.get_best_hyperparameters(4)[1].values)\n",
        "  print(tuner.get_best_hyperparameters(4)[2].values)\n",
        "  print(tuner.get_best_hyperparameters(4)[3].values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hd8DbVmcBUkw"
      },
      "source": [
        "### 5.8 Baseline v1:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_on=False"
      ],
      "metadata": {
        "id": "3w5qtnLCkSa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ea9IsPvu-b1Q"
      },
      "outputs": [],
      "source": [
        "features = ['MonsoonIntensity', 'TopographyDrainage', 'RiverManagement',\n",
        "       'Deforestation', 'Urbanization', 'ClimateChange', 'DamsQuality',\n",
        "       'Siltation', 'AgriculturalPractices', 'Encroachments',\n",
        "       'IneffectiveDisasterPreparedness', 'DrainageSystems',\n",
        "       'CoastalVulnerability', 'Landslides', 'Watersheds',\n",
        "       'DeterioratingInfrastructure', 'PopulationScore', 'WetlandLoss',\n",
        "       'InadequatePlanning', 'PoliticalFactors', 'col_11', 'col_12', 'col_8',\n",
        "       'average_score', 'median_score', 'hmean', 'delta', 'compound_effect',\n",
        "       'risk_mitigation_factors']\n",
        "\n",
        "feat_used = ['MonsoonIntensity', 'TopographyDrainage', 'RiverManagement',\n",
        "       'Deforestation', 'Urbanization', 'ClimateChange', 'DamsQuality',\n",
        "       'Siltation', 'AgriculturalPractices', 'Encroachments',\n",
        "       'IneffectiveDisasterPreparedness', 'DrainageSystems',\n",
        "       'CoastalVulnerability', 'Landslides', 'Watersheds',\n",
        "       'DeterioratingInfrastructure', 'PopulationScore', 'WetlandLoss',\n",
        "       'InadequatePlanning', 'PoliticalFactors',\n",
        "       'average_score', 'median_score', 'hmean', 'delta', 'compound_effect',\n",
        "       'risk_mitigation_factors']\n",
        "cat_features = ['col_11', 'col_12', 'col_8']\n",
        "\n",
        "x_scaler = StandardScaler()\n",
        "X_train = train_df[features].copy()\n",
        "X_test = test_df[features].copy()\n",
        "\n",
        "X_train[feat_used] =  x_scaler.fit_transform(X_train[feat_used])\n",
        "X_test[feat_used] =  x_scaler.transform(X_test[feat_used])\n",
        "\n",
        "X_train_df = pd.DataFrame(index=train_df.index, columns=features, data=X_train)\n",
        "X_test_df = pd.DataFrame(index=test_df.index, columns=features, data=X_test)\n",
        "\n",
        "X_train_df[\"FloodProbability\"] = y_scaled\n",
        "X_test_df[\"FloodProbability\"] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OpWXH58vFVlG"
      },
      "outputs": [],
      "source": [
        "X_train_df.describe().T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmP-moTg-OS2"
      },
      "outputs": [],
      "source": [
        "Cat_Feat_Entries = {}\n",
        "for f in cat_features:\n",
        "  Cat_Feat_Entries[f]=list(X_train_df[f].unique())\n",
        "\n",
        "Cat_Feat_Entries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARj5NdvWAT0X"
      },
      "outputs": [],
      "source": [
        "def encode_inputs(inputs, add_dense_layer_cat = False, dense_dim_cat = 4, num_dense_exp=True, dense_dim_num = 4,\n",
        "                  list_categorical_nn=cat_features, Cat_Feat_Entries=Cat_Feat_Entries,  name=\"enc\"):\n",
        "\n",
        "    encoded_categorical_feature_list = []\n",
        "    numerical_feature_list = []\n",
        "\n",
        "    for feature_name in inputs:\n",
        "      if feature_name in list_categorical_nn:\n",
        "\n",
        "        vocabulary = Cat_Feat_Entries[feature_name]\n",
        "        #print(vocabulary,vocabulary.shape[1])\n",
        "\n",
        "        embedding = layers.Embedding(input_dim=len(vocabulary), output_dim=int(np.round(np.log1p(len(vocabulary))+1.1,0)), name=f\"{feature_name}_embedding\")\n",
        "        # Convert the index values to embedding representations.\n",
        "        encoded_categorical_feature = embedding(inputs[feature_name])\n",
        "\n",
        "        if add_dense_layer_cat==True:\n",
        "          encoded_categorical_feature = tf.keras.layers.Dense(dense_dim_cat, name=f\"cat_dense_{feature_name}_{name}\")(encoded_categorical_feature)\n",
        "\n",
        "        encoded_categorical_feature_list.append(encoded_categorical_feature)\n",
        "\n",
        "      else:\n",
        "        # Use the numerical features as-is.\n",
        "        numerical_feature = inputs[feature_name] #tf.expand_dims(inputs[feature_name], -1)\n",
        "\n",
        "        if num_dense_exp==True:\n",
        "          numerical_feature = keras.layers.Reshape((-1, 1))(numerical_feature)\n",
        "          numerical_feature = tf.keras.layers.Dense(dense_dim_num, name=f\"num_dense_{feature_name}_{name}\")(numerical_feature)\n",
        "\n",
        "        numerical_feature_list.append(numerical_feature)\n",
        "\n",
        "#    for feat in encoded_categorical_feature_list:\n",
        "#      print(feat.name, feat.shape)\n",
        "#    for feat in numerical_feature_list:\n",
        "#      print(feat.name, feat.shape)\n",
        "\n",
        "    return encoded_categorical_feature_list, numerical_feature_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9ItbJAkBUkx"
      },
      "outputs": [],
      "source": [
        "if train_on==True:\n",
        "  train_dataset = dataframe_to_dataset(X_train_df, batch_size=256, shuffle=True)\n",
        "  test_dataset = dataframe_to_dataset(X_test_df, batch_size=256, shuffle=False)\n",
        "\n",
        "  feature_space_dict = FeatureSpace(\n",
        "                                features={**{a:FeatureSpace.float() for a in feat_used},\n",
        "                                          **{a:FeatureSpace.integer_categorical(num_oov_indices=1, output_mode=\"one_hot\") for a in cat_features}},\n",
        "                                #crosses = [FeatureSpace.cross((\"col_11\",\"col_12\"), crossing_dim=32)],\n",
        "                                output_mode=\"dict\"\n",
        "                                )\n",
        "\n",
        "  train_ds_with_no_labels = train_dataset.map(lambda x, *_: x)\n",
        "  print(\"Adapting Features Space....\")\n",
        "  feature_space_dict.adapt(train_ds_with_no_labels)\n",
        "\n",
        "  preprocessed_train_ds = train_dataset.map(lambda x, y: (feature_space_dict(x), y), num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
        "  preprocessed_test_ds = test_dataset.map(lambda x, y: (feature_space_dict(x), y), num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMkNVT4THIih"
      },
      "outputs": [],
      "source": [
        "from keras import ops\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1N_w5keh4EtY"
      },
      "outputs": [],
      "source": [
        "def create_BASELINE(fs, name=\"baseline_v0\",\n",
        "                     learning_rate = 0.001,\n",
        "                     main_activation=\"selu\",\n",
        "                     trans_activation=\"swish\",\n",
        "                     dropout=0.5,\n",
        "                     sp_dropout=0.33,\n",
        "                     dense_blocks=5,\n",
        "                     units_dense=1024,\n",
        "                     final_dense=256,\n",
        "                     excite_dim=12,\n",
        "                     num_trans_cat_layers = 3,\n",
        "                     num_trans_num_layers = 3,\n",
        "                     num_heads_cat=8,\n",
        "                     num_heads_num=8,\n",
        "                     add_dense_layer_cat = False, #encoding option\n",
        "                     dense_dim_cat = 4, #encoding option\n",
        "                     num_dense_exp = False, #encoding option\n",
        "                     dense_dim_num = 4 #encoding option\n",
        "                    ):\n",
        "\n",
        "\n",
        "    encoded_features = fs.get_encoded_features()\n",
        "\n",
        "\n",
        "    cat,num = encode_inputs(inputs=encoded_features,\n",
        "                            add_dense_layer_cat = add_dense_layer_cat,\n",
        "                            dense_dim_cat = dense_dim_cat,\n",
        "                            num_dense_exp = num_dense_exp,\n",
        "                            dense_dim_num = dense_dim_num,\n",
        "                            list_categorical_nn=cat_features,\n",
        "                            Cat_Feat_Entries=Cat_Feat_Entries,\n",
        "                            name=\"enc\")\n",
        "\n",
        "  ################################Transformer#######################################\n",
        "    cat_reshaped_att = []\n",
        "\n",
        "    for num_feat, feat in enumerate(cat):\n",
        "      for lay_n in range(num_trans_cat_layers):\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "                                                    num_heads=num_heads_cat,\n",
        "                                                    key_dim=feat.shape[2],\n",
        "                                                    dropout=dropout,\n",
        "                                                    name=f\"multihead_attention_{cat_features[num_feat]}_{lay_n}\",\n",
        "                                                    )(feat, feat)\n",
        "\n",
        "        x = layers.Add(name=f\"skip_connection1_{cat_features[num_feat]}_{lay_n}\")([attention_output, feat])\n",
        "        # Layer normalization 1.\n",
        "        x = layers.LayerNormalization(name=f\"layer_norm1_{cat_features[num_feat]}_{lay_n}\", epsilon=1e-6)(x)\n",
        "          # Feedforward.\n",
        "        feedforward_output = create_mlp(hidden_units=[feat.shape[2]],\n",
        "                                      dropout_rate=dropout,\n",
        "                                      activation=trans_activation,\n",
        "                                      normalization_layer=partial(layers.LayerNormalization, epsilon=1e-6),  # using partial to provide keyword arguments before initialization\n",
        "                                      name=f\"feedforward_{cat_features[num_feat]}_{lay_n}\")(x)\n",
        "\n",
        "        x = layers.Add(name=f\"skip_connection2_{cat_features[num_feat]}_{lay_n}\")([feedforward_output, x])\n",
        "\n",
        "        feat = layers.LayerNormalization(name=f\"layer_norm2_{cat_features[num_feat]}_{lay_n}\", epsilon=1e-6)(x)\n",
        "\n",
        "      cat_reshaped_att.append(feat)\n",
        "\n",
        "\n",
        "    #### Concat\n",
        "    # Categorical\n",
        "\n",
        "    x_cat_pool = tf.keras.layers.Concatenate(name = \"categorical_conc_pool\")([tf.keras.layers.GlobalAveragePooling1D(data_format=\"channels_first\")(tf.keras.layers.SpatialDropout1D(sp_dropout)(cat_emb)) for cat_emb in cat_reshaped_att])\n",
        "    x_cat_concat = tf.keras.layers.Concatenate(name = \"categorical_conc_flat\")([tf.keras.layers.Flatten()(tf.keras.layers.SpatialDropout1D(sp_dropout)(cat_emb)) for cat_emb in cat_reshaped_att])\n",
        "    x_cat = tf.keras.layers.Concatenate(name = \"categorical_conc\")([x_cat_pool,x_cat_concat])\n",
        "\n",
        "    # Numerical\n",
        "    x_num_stack = ops.stack(num, axis=1)\n",
        "    x_num_concat = tf.keras.layers.Concatenate(name=\"x_num_concat\")([n for n in num])\n",
        "\n",
        "    # Excite Layer:\n",
        "    x_num_stack_excite = keras.layers.Dense(excite_dim, activation=\"linear\",kernel_regularizer='l2')(x_num_stack)\n",
        "\n",
        "    for lay_n in range(num_trans_num_layers):\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "                                                    num_heads=num_heads_cat,\n",
        "                                                    key_dim=x_num_stack_excite.shape[2],\n",
        "                                                    dropout=dropout,\n",
        "                                                    name=f\"multihead_attention_num_{lay_n}\",\n",
        "                                                    )(x_num_stack_excite, x_num_stack_excite)\n",
        "\n",
        "        x = layers.Add(name=f\"skip_connection1_{lay_n}_numeric_trans\")([attention_output, x_num_stack_excite])\n",
        "        # Layer normalization 1.\n",
        "        x = layers.LayerNormalization(name=f\"layer_norm_{lay_n}_numeric_trans\", epsilon=1e-6)(x)\n",
        "          # Feedforward.\n",
        "        feedforward_output = create_mlp(hidden_units=[x_num_stack_excite.shape[2]],\n",
        "                                      dropout_rate=dropout,\n",
        "                                      activation=trans_activation,\n",
        "                                      normalization_layer=partial(layers.LayerNormalization, epsilon=1e-6),  # using partial to provide keyword arguments before initialization\n",
        "                                      name=f\"feedforward_{lay_n}_numeric_trans\")(x)\n",
        "\n",
        "        x = layers.Add(name=f\"skip_connection2_{lay_n}_numeric_trans\")([feedforward_output, x])\n",
        "\n",
        "        x_num_stack_excite = layers.LayerNormalization(name=f\"layer_norm2_{lay_n}_numeric_trans\", epsilon=1e-6)(x)\n",
        "\n",
        "    exite_flatten = tf.keras.layers.Flatten()(tf.keras.layers.SpatialDropout1D(sp_dropout)(x_num_stack_excite))\n",
        "\n",
        "    x_num = tf.keras.layers.Concatenate(name=\"x_num_concat_final\")([x_num_concat,exite_flatten])\n",
        "\n",
        "    # Dense Layers:\n",
        "\n",
        "    x = tf.keras.layers.Concatenate(name=\"x_concat_final\")([x_num,x_cat])\n",
        "    x = layers.BatchNormalization(name=f\"batch_norm_final\", epsilon=1e-6)(x)\n",
        "    x = tf.keras.layers.Dropout(dropout,name=f\"dropout_final_concat\")(x)\n",
        "\n",
        "    for block_number, block in enumerate(range(dense_blocks)):\n",
        "      x = dense_residual_block(units=units_dense, dropout_rate=dropout, activation=main_activation, kr=0, attention=False, name=f\"drb_{block_number}\", norm=\"batch\")(x)\n",
        "\n",
        "    x=dense_block(final_dense, dropout_rate=dropout, activation=main_activation, name=f\"drb_final\")(x)\n",
        "    output = tf.keras.layers.Dense(1, activation=\"sigmoid\", name=\"output_layer\")(x)\n",
        "\n",
        "\n",
        "    model = keras.Model(inputs=encoded_features, outputs=output)\n",
        "\n",
        "\n",
        "\n",
        "    model.compile(\n",
        "                  optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "                  loss=tf.keras.losses.MeanSquaredError(name='mse'),\n",
        "                  metrics=[tf.keras.metrics.MeanSquaredError(name=\"mse\"),\n",
        "                          tf.keras.metrics.R2Score(name=\"R2_Score\")]\n",
        "                )\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRJT3h-M5MrT"
      },
      "outputs": [],
      "source": [
        "if train_on==True:\n",
        "  model = create_BASELINE(feature_space_dict)\n",
        "  model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJTgdRnXvVzb"
      },
      "outputs": [],
      "source": [
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SvxqDa135MrT"
      },
      "outputs": [],
      "source": [
        "#plot_model(\n",
        "#            model,\n",
        "#            show_shapes=True,\n",
        "#            show_layer_names=True,\n",
        "#            rankdir=\"LR\",\n",
        "#            expand_nested=True,\n",
        "#            dpi=200,\n",
        "#            show_trainable=True\n",
        "#            )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwDlNYeGBUk0"
      },
      "source": [
        "##### **KerasTuner**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sg7aS5V4BUk0"
      },
      "outputs": [],
      "source": [
        "tuning_on=False\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRMVBeatBUk1"
      },
      "outputs": [],
      "source": [
        "if tuning_on==True:\n",
        "  X_trn, X_val = train_test_split(X_train_df, random_state=1978, test_size=0.3, stratify=X_train_df[\"FloodProbability\"])\n",
        "\n",
        "  feature_space_dict = FeatureSpace(\n",
        "                                features={**{a:FeatureSpace.float() for a in feat_used},\n",
        "                                          **{a:FeatureSpace.integer_categorical(num_oov_indices=1, output_mode=\"one_hot\") for a in cat_features}},\n",
        "                                #crosses = [FeatureSpace.cross((\"col_11\",\"col_12\"), crossing_dim=32)],\n",
        "                                output_mode=\"dict\"\n",
        "                                )\n",
        "\n",
        "\n",
        "  train_dataset = dataframe_to_dataset(X_trn, batch_size=1024, shuffle=True)\n",
        "  valid_dataset = dataframe_to_dataset(X_val, batch_size=1024, shuffle=False)\n",
        "\n",
        "  train_ds_with_no_labels = train_dataset.map(lambda x, *_: x)\n",
        "  feature_space_dict.adapt(train_ds_with_no_labels)\n",
        "\n",
        "  preprocessed_train_ds = train_dataset.map(lambda x, y: (feature_space_dict(x), y), num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
        "  preprocessed_valid_ds = valid_dataset.map(lambda x, y: (feature_space_dict(x), y), num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lit7R2BTBUk1"
      },
      "outputs": [],
      "source": [
        "if tuning_on==True:\n",
        "  # Define the hyperparameter search space: EXPERIMENT 1\n",
        "  hp = kt.HyperParameters()\n",
        "  hp.Choice('main_activation', [\"prelu\",\"swish\",\"gelu\"])\n",
        "  hp.Choice('trans_activation', [\"prelu\",\"swish\",\"gelu\"])\n",
        "  hp.Float('dropout',0.15,0.40, step=0.025)\n",
        "  hp.Float('sp_dropout',0.15,0.50, step=0.025)\n",
        "  hp.Choice('dense_blocks', values=[2,3,4])\n",
        "  hp.Choice('units_dense', values=[128,256,512,1024])\n",
        "  hp.Choice('final_dense', values=[128,256,512])\n",
        "  hp.Choice('excite_dim', values=[6,12,24])\n",
        "  hp.Choice('num_trans_cat_layers', values=[6,12,24])\n",
        "  hp.Choice('num_trans_num_layers', values=[6,12,24])\n",
        "  hp.Choice('num_heads_cat', values=[4,8,12])\n",
        "  hp.Choice('num_heads_num', values=[8,12,16])\n",
        "  #hp.Float('kr', 0.00001, 1.0, step=10,sampling=\"log\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWN3dN01BUk3"
      },
      "outputs": [],
      "source": [
        "# TRAIN MODEL\n",
        "# Define Preprocessing steps:\n",
        "def create_turner_model(hp):\n",
        "\n",
        "  model = create_BASELINE(feature_space_dict,\n",
        "                           name=\"baseline_v1\",\n",
        "                           learning_rate = 0.005,\n",
        "                            main_activation=hp.get('main_activation'),\n",
        "                            trans_activation=hp.get('trans_activation'),\n",
        "                            dropout=hp.get('dropout'),\n",
        "                            sp_dropout=hp.get('sp_dropout'),\n",
        "                            dense_blocks=hp.get('dense_blocks'),\n",
        "                            units_dense=hp.get('units_dense'),\n",
        "                            final_dense=hp.get('final_dense'),\n",
        "                            excite_dim=hp.get('excite_dim'),\n",
        "                            num_trans_cat_layers = hp.get('num_trans_cat_layers'),\n",
        "                            num_trans_num_layers = hp.get('num_trans_num_layers'),\n",
        "                            num_heads_cat=hp.get('num_heads_cat'),\n",
        "                            num_heads_num=hp.get('num_heads_num')\n",
        "                          )\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4FfyXobBUk4"
      },
      "outputs": [],
      "source": [
        "if tuning_on==True:\n",
        "  # Create a tuner and search for the best hyperparameters\n",
        "  tuner = BayesianOptimization(create_turner_model,\n",
        "                              objective=kt.Objective(\"val_R2_Score\", \"max\"),\n",
        "                              hyperparameters=hp, max_trials=50, overwrite=True)\n",
        "\n",
        "  stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_R2_Score', patience=7, mode=\"max\", start_from_epoch=5)\n",
        "  reduce_ = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_R2_Score', factor=0.5, patience=3, min_lr=0.0001, mode=\"max\")\n",
        "\n",
        "  tuner.search(preprocessed_train_ds, validation_data=preprocessed_valid_ds, epochs=31, callbacks=[stop_early,reduce_])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36NuwexiBUk5"
      },
      "outputs": [],
      "source": [
        "if tuning_on==True:\n",
        "  print(tuner.get_best_hyperparameters(4)[0].values)\n",
        "  print(tuner.get_best_hyperparameters(4)[1].values)\n",
        "  print(tuner.get_best_hyperparameters(4)[2].values)\n",
        "  print(tuner.get_best_hyperparameters(4)[3].values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmD8Mcj3VD9B"
      },
      "source": [
        "### 5.8.3 Baseline v2:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.columns"
      ],
      "metadata": {
        "id": "NF8wZqFpmtz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OscsMZpEVD9J"
      },
      "outputs": [],
      "source": [
        "features = ['MonsoonIntensity', 'TopographyDrainage', 'RiverManagement',\n",
        "       'Deforestation', 'Urbanization', 'ClimateChange', 'DamsQuality',\n",
        "       'Siltation', 'AgriculturalPractices', 'Encroachments',\n",
        "       'IneffectiveDisasterPreparedness', 'DrainageSystems',\n",
        "       'CoastalVulnerability', 'Landslides', 'Watersheds',\n",
        "       'DeterioratingInfrastructure', 'PopulationScore', 'WetlandLoss',\n",
        "       'InadequatePlanning', 'PoliticalFactors', 'col_11', 'col_12', 'col_8',\n",
        "       'average_score', 'median_score', 'hmean', 'delta', 'compound_effect',\n",
        "       'risk_mitigation_factors']\n",
        "\n",
        "feat_used = ['MonsoonIntensity', 'TopographyDrainage', 'RiverManagement',\n",
        "       'Deforestation', 'Urbanization', 'ClimateChange', 'DamsQuality',\n",
        "       'Siltation', 'AgriculturalPractices', 'Encroachments',\n",
        "       'IneffectiveDisasterPreparedness', 'DrainageSystems',\n",
        "       'CoastalVulnerability', 'Landslides', 'Watersheds',\n",
        "       'DeterioratingInfrastructure', 'PopulationScore', 'WetlandLoss',\n",
        "       'InadequatePlanning', 'PoliticalFactors',\n",
        "       'average_score', 'median_score', 'hmean', 'delta', 'compound_effect',\n",
        "       'risk_mitigation_factors']\n",
        "cat_features = ['col_11', 'col_12', 'col_8']\n",
        "\n",
        "x_scaler = StandardScaler()\n",
        "X_train = train_df[features].copy()\n",
        "X_test = test_df[features].copy()\n",
        "\n",
        "X_train[feat_used] =  x_scaler.fit_transform(X_train[feat_used])\n",
        "X_test[feat_used] =  x_scaler.transform(X_test[feat_used])\n",
        "\n",
        "X_train_df = pd.DataFrame(index=train_df.index, columns=features, data=X_train)\n",
        "X_test_df = pd.DataFrame(index=test_df.index, columns=features, data=X_test)\n",
        "\n",
        "X_train_df[\"FloodProbability\"] = y_scaled\n",
        "X_test_df[\"FloodProbability\"] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8rzSxVjVD9K"
      },
      "outputs": [],
      "source": [
        "X_train_df.describe().T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uUhzMvbTVD9K"
      },
      "outputs": [],
      "source": [
        "Cat_Feat_Entries = {}\n",
        "for f in cat_features:\n",
        "  Cat_Feat_Entries[f]=list(X_train_df[f].unique())\n",
        "\n",
        "Cat_Feat_Entries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2oVO80RHVD9K"
      },
      "outputs": [],
      "source": [
        "def encode_inputs(inputs, add_dense_layer_cat = False, dense_dim_cat = 4, num_dense_exp=True, dense_dim_num = 4,\n",
        "                  list_categorical_nn=cat_features, Cat_Feat_Entries=Cat_Feat_Entries,  name=\"enc\"):\n",
        "\n",
        "    encoded_categorical_feature_list = []\n",
        "    numerical_feature_list = []\n",
        "\n",
        "    for feature_name in inputs:\n",
        "      if feature_name in list_categorical_nn:\n",
        "\n",
        "        vocabulary = Cat_Feat_Entries[feature_name]\n",
        "        #print(vocabulary,vocabulary.shape[1])\n",
        "\n",
        "        embedding = layers.Embedding(input_dim=len(vocabulary), output_dim=int(np.round(np.log1p(len(vocabulary))+1.1,0)), name=f\"{feature_name}_embedding\")\n",
        "        # Convert the index values to embedding representations.\n",
        "        encoded_categorical_feature = embedding(inputs[feature_name])\n",
        "\n",
        "        if add_dense_layer_cat==True:\n",
        "          encoded_categorical_feature = tf.keras.layers.Dense(dense_dim_cat, name=f\"cat_dense_{feature_name}_{name}\")(encoded_categorical_feature)\n",
        "\n",
        "        encoded_categorical_feature_list.append(encoded_categorical_feature)\n",
        "\n",
        "      else:\n",
        "        # Use the numerical features as-is.\n",
        "        numerical_feature = inputs[feature_name] #tf.expand_dims(inputs[feature_name], -1)\n",
        "\n",
        "        if num_dense_exp==True:\n",
        "          numerical_feature = keras.layers.Reshape((-1, 1))(numerical_feature)\n",
        "          numerical_feature = tf.keras.layers.Dense(dense_dim_num, name=f\"num_dense_{feature_name}_{name}\")(numerical_feature)\n",
        "\n",
        "        numerical_feature_list.append(numerical_feature)\n",
        "\n",
        "#    for feat in encoded_categorical_feature_list:\n",
        "#      print(feat.name, feat.shape)\n",
        "#    for feat in numerical_feature_list:\n",
        "#      print(feat.name, feat.shape)\n",
        "\n",
        "    return encoded_categorical_feature_list, numerical_feature_list"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_on=False"
      ],
      "metadata": {
        "id": "vuRlUa4Gkn5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGGYwjMhVD9K"
      },
      "outputs": [],
      "source": [
        "if train_on==True:\n",
        "\n",
        "  train_dataset = dataframe_to_dataset(X_train_df, batch_size=256, shuffle=True)\n",
        "  test_dataset = dataframe_to_dataset(X_test_df, batch_size=256, shuffle=False)\n",
        "\n",
        "  feature_space_dict = FeatureSpace(\n",
        "                                features={**{a:FeatureSpace.float() for a in feat_used},\n",
        "                                          **{a:FeatureSpace.integer_categorical(num_oov_indices=1, output_mode=\"one_hot\") for a in cat_features}},\n",
        "                                #crosses = [FeatureSpace.cross((\"col_11\",\"col_12\"), crossing_dim=32)],\n",
        "                                output_mode=\"dict\"\n",
        "                                )\n",
        "\n",
        "  train_ds_with_no_labels = train_dataset.map(lambda x, *_: x)\n",
        "  print(\"Adapting Features Space....\")\n",
        "  feature_space_dict.adapt(train_ds_with_no_labels)\n",
        "\n",
        "  preprocessed_train_ds = train_dataset.map(lambda x, y: (feature_space_dict(x), y), num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
        "  preprocessed_test_ds = test_dataset.map(lambda x, y: (feature_space_dict(x), y), num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wgjv-IsgVD9K"
      },
      "outputs": [],
      "source": [
        "from keras import ops\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "317XmZF5VD9L"
      },
      "outputs": [],
      "source": [
        "def create_BASELINE(fs, name=\"baseline_v0\",\n",
        "                     learning_rate = 0.001,\n",
        "                     main_activation=\"selu\",\n",
        "                     trans_activation=\"swish\",\n",
        "                     dropout=0.5,\n",
        "                     sp_dropout=0.33,\n",
        "                     dense_blocks=5,\n",
        "                     units_dense=128,\n",
        "                     final_dense=256,\n",
        "                     num_trans_cat_layers = 3,\n",
        "                     num_heads_cat=8,\n",
        "                     gn_noise=0.025,\n",
        "                     add_dense_layer_cat = False, #encoding option\n",
        "                     dense_dim_cat = 4, #encoding option\n",
        "                     num_dense_exp = False, #encoding option\n",
        "                     dense_dim_num = 4 #encoding option\n",
        "                    ):\n",
        "\n",
        "\n",
        "    encoded_features = fs.get_encoded_features()\n",
        "\n",
        "\n",
        "    cat,num = encode_inputs(inputs=encoded_features,\n",
        "                            add_dense_layer_cat = add_dense_layer_cat,\n",
        "                            dense_dim_cat = dense_dim_cat,\n",
        "                            num_dense_exp = num_dense_exp,\n",
        "                            dense_dim_num = dense_dim_num,\n",
        "                            list_categorical_nn=cat_features,\n",
        "                            Cat_Feat_Entries=Cat_Feat_Entries,\n",
        "                            name=\"enc\")\n",
        "\n",
        "  ################################Transformer#######################################\n",
        "    cat_reshaped_att = []\n",
        "\n",
        "    for num_feat, feat in enumerate(cat):\n",
        "      for lay_n in range(num_trans_cat_layers):\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "                                                    num_heads=num_heads_cat,\n",
        "                                                    key_dim=feat.shape[2],\n",
        "                                                    dropout=dropout,\n",
        "                                                    name=f\"multihead_attention_{cat_features[num_feat]}_{lay_n}\",\n",
        "                                                    )(feat, feat)\n",
        "\n",
        "        x = layers.Add(name=f\"skip_connection1_{cat_features[num_feat]}_{lay_n}\")([attention_output, feat])\n",
        "        # Layer normalization 1.\n",
        "        x = layers.LayerNormalization(name=f\"layer_norm1_{cat_features[num_feat]}_{lay_n}\", epsilon=1e-6)(x)\n",
        "          # Feedforward.\n",
        "        feedforward_output = create_mlp(hidden_units=[feat.shape[2]],\n",
        "                                      dropout_rate=dropout,\n",
        "                                      activation=trans_activation,\n",
        "                                      normalization_layer=partial(layers.LayerNormalization, epsilon=1e-6),  # using partial to provide keyword arguments before initialization\n",
        "                                      name=f\"feedforward_{cat_features[num_feat]}_{lay_n}\")(x)\n",
        "\n",
        "        x = layers.Add(name=f\"skip_connection2_{cat_features[num_feat]}_{lay_n}\")([feedforward_output, x])\n",
        "\n",
        "        feat = layers.LayerNormalization(name=f\"layer_norm2_{cat_features[num_feat]}_{lay_n}\", epsilon=1e-6)(x)\n",
        "\n",
        "      cat_reshaped_att.append(feat)\n",
        "\n",
        "\n",
        "    #### Concat\n",
        "    # Categorical\n",
        "\n",
        "    #x_cat_pool = tf.keras.layers.Concatenate(name = \"categorical_conc_pool\")([tf.keras.layers.GlobalAveragePooling1D(data_format=\"channels_first\")(tf.keras.layers.SpatialDropout1D(sp_dropout)(cat_emb)) for cat_emb in cat_reshaped_att])\n",
        "    x_cat = tf.keras.layers.Concatenate(name = \"categorical_conc_flat\")([tf.keras.layers.Flatten()(tf.keras.layers.SpatialDropout1D(sp_dropout)(cat_emb)) for cat_emb in cat_reshaped_att])\n",
        "    #x_cat = tf.keras.layers.Concatenate(name = \"categorical_conc\")([x_cat_pool,x_cat_concat])\n",
        "\n",
        "    # Numerical\n",
        "    #x_num_stack = ops.stack(num, axis=1)\n",
        "    x_num_concat = tf.keras.layers.Concatenate(name=\"x_num_concat\")([n for n in num])\n",
        "\n",
        "    # Excite Layer:\n",
        "    #x_num_stack_excite = keras.layers.Dense(excite_dim, activation=\"linear\",kernel_regularizer='l2')(x_num_stack)\n",
        "\n",
        "    #for lay_n in range(num_trans_num_layers):\n",
        "    #    attention_output = layers.MultiHeadAttention(\n",
        "    #                                                num_heads=num_heads_cat,\n",
        "    #                                                key_dim=x_num_stack_excite.shape[2],\n",
        "    #                                                dropout=dropout,\n",
        "    #                                                name=f\"multihead_attention_num_{lay_n}\",\n",
        "    #                                                )(x_num_stack_excite, x_num_stack_excite)#\n",
        "\n",
        "    #    x = layers.Add(name=f\"skip_connection1_{lay_n}_numeric_trans\")([attention_output, x_num_stack_excite])\n",
        "    #    # Layer normalization 1.\n",
        "     #   x = layers.LayerNormalization(name=f\"layer_norm_{lay_n}_numeric_trans\", epsilon=1e-6)(x)\n",
        "          # Feedforward.\n",
        "    #    feedforward_output = create_mlp(hidden_units=[x_num_stack_excite.shape[2]],\n",
        "    #                                  dropout_rate=dropout,\n",
        "    #                                  activation=trans_activation,\n",
        "    #                                  normalization_layer=partial(layers.LayerNormalization, epsilon=1e-6),  # using partial to provide keyword arguments before initialization\n",
        "    #                                  name=f\"feedforward_{lay_n}_numeric_trans\")(x)\n",
        "\n",
        "    #   x = layers.Add(name=f\"skip_connection2_{lay_n}_numeric_trans\")([feedforward_output, x])\n",
        "\n",
        "    #    x_num_stack_excite = layers.LayerNormalization(name=f\"layer_norm2_{lay_n}_numeric_trans\", epsilon=1e-6)(x)\n",
        "\n",
        "    #exite_flatten = tf.keras.layers.Flatten()(tf.keras.layers.SpatialDropout1D(sp_dropout)(x_num_stack_excite))\n",
        "\n",
        "    #x_num = tf.keras.layers.Concatenate(name=\"x_num_concat_final\")([x_num_concat,exite_flatten])\n",
        "\n",
        "    # Dense Layers:\n",
        "\n",
        "    x = tf.keras.layers.Concatenate(name=\"x_concat_final\")([x_num_concat,x_cat])\n",
        "    x = layers.BatchNormalization(name=f\"batch_norm_final\", epsilon=1e-6)(x)\n",
        "    x = tf.keras.layers.Dropout(dropout,name=f\"dropout_final_concat\")(x)\n",
        "    x = tf.keras.layers.GaussianNoise(stddev=gn_noise, name=f\"gsn_layer\")(x)\n",
        "\n",
        "    for block_number, block in enumerate(range(dense_blocks)):\n",
        "      x = dense_residual_block(units=units_dense, dropout_rate=dropout, activation=main_activation, kr=0, attention=False, name=f\"drb_{block_number}\", norm=\"batch\")(x)\n",
        "\n",
        "    x=dense_block(final_dense, dropout_rate=dropout, activation=main_activation, name=f\"drb_final\")(x)\n",
        "    output = tf.keras.layers.Dense(1, activation=\"sigmoid\", name=\"output_layer\")(x)\n",
        "\n",
        "\n",
        "    model = keras.Model(inputs=encoded_features, outputs=output)\n",
        "\n",
        "\n",
        "\n",
        "    model.compile(\n",
        "                  optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "                  loss=tf.keras.losses.MeanSquaredError(name='mse'),\n",
        "                  metrics=[tf.keras.metrics.MeanSquaredError(name=\"mse\"),\n",
        "                          tf.keras.metrics.R2Score(name=\"R2_Score\")]\n",
        "                )\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4BbGMAimVD9L"
      },
      "outputs": [],
      "source": [
        "if train_on==True:\n",
        "\n",
        "  model = create_BASELINE(feature_space_dict)\n",
        "  model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwQfzqNEVD9L"
      },
      "outputs": [],
      "source": [
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "saECYQw5VD9L"
      },
      "outputs": [],
      "source": [
        "#plot_model(\n",
        "#            model,\n",
        "#            show_shapes=True,\n",
        "#            show_layer_names=True,\n",
        "#            rankdir=\"LR\",\n",
        "#            expand_nested=True,\n",
        "#            dpi=200,\n",
        "#            show_trainable=True\n",
        "#            )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIz3SyOJVD9L"
      },
      "source": [
        "##### **KerasTuner**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdV-mOVRVD9M"
      },
      "outputs": [],
      "source": [
        "tuning_on=False\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r31fEIJSVD9M"
      },
      "outputs": [],
      "source": [
        "if tuning_on==True:\n",
        "  X_trn, X_val = train_test_split(X_train_df, random_state=1978, test_size=0.70, stratify=X_train_df[\"FloodProbability\"])\n",
        "  X_trn, X_val = train_test_split(X_trn, random_state=1978, test_size=0.3, stratify=X_trn[\"FloodProbability\"])\n",
        "\n",
        "  feature_space_dict = FeatureSpace(\n",
        "                                features={**{a:FeatureSpace.float() for a in feat_used},\n",
        "                                          **{a:FeatureSpace.integer_categorical(num_oov_indices=1, output_mode=\"one_hot\") for a in cat_features}},\n",
        "                                #crosses = [FeatureSpace.cross((\"col_11\",\"col_12\"), crossing_dim=32)],\n",
        "                                output_mode=\"dict\"\n",
        "                                )\n",
        "\n",
        "\n",
        "  train_dataset = dataframe_to_dataset(X_trn, batch_size=1024, shuffle=True)\n",
        "  valid_dataset = dataframe_to_dataset(X_val, batch_size=1024, shuffle=False)\n",
        "\n",
        "  train_ds_with_no_labels = train_dataset.map(lambda x, *_: x)\n",
        "  feature_space_dict.adapt(train_ds_with_no_labels)\n",
        "\n",
        "  preprocessed_train_ds = train_dataset.map(lambda x, y: (feature_space_dict(x), y), num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
        "  preprocessed_valid_ds = valid_dataset.map(lambda x, y: (feature_space_dict(x), y), num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if tuning_on==True:\n",
        "  print(X_trn.shape, X_val.shape)"
      ],
      "metadata": {
        "id": "wEl_ztUE3GrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iyoxlp7JVD9M"
      },
      "outputs": [],
      "source": [
        "if tuning_on==True:\n",
        "  # Define the hyperparameter search space: EXPERIMENT 1\n",
        "  hp = kt.HyperParameters()\n",
        "  hp.Choice('main_activation', [\"prelu\"])\n",
        "  hp.Choice('trans_activation', [\"prelu\",\"gelu\"])\n",
        "  hp.Float('dropout',0.10,0.20, step=0.01)\n",
        "  hp.Float('sp_dropout',0.10,0.50, step=0.025)\n",
        "  hp.Choice('dense_blocks', values=[4,5])\n",
        "  hp.Choice('units_dense', values=[64,128,256,512])\n",
        "  hp.Choice('final_dense', values=[64,128,256,512])\n",
        "  hp.Choice('num_trans_cat_layers', values=[1,3])\n",
        "  hp.Choice('num_heads_cat', values=[4,8])\n",
        "  hp.Float('gn_noise', 0.005,0.05, step=0.0025)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ioefLMwkVD9M"
      },
      "outputs": [],
      "source": [
        "# TRAIN MODEL\n",
        "# Define Preprocessing steps:\n",
        "def create_turner_model(hp):\n",
        "\n",
        "  model = create_BASELINE(feature_space_dict,\n",
        "                             name=\"baseline_v2\",\n",
        "                             learning_rate = 0.0025,\n",
        "                             main_activation=hp.get('main_activation'),\n",
        "                             trans_activation=hp.get('trans_activation'),\n",
        "                             dropout=hp.get('dropout'),\n",
        "                             sp_dropout=hp.get('sp_dropout'),\n",
        "                             dense_blocks=hp.get('dense_blocks'),\n",
        "                             units_dense=hp.get('units_dense'),\n",
        "                             final_dense=hp.get('final_dense'),\n",
        "                             num_trans_cat_layers = hp.get('num_trans_cat_layers'),\n",
        "                             num_heads_cat=hp.get('num_heads_cat'),\n",
        "                             gn_noise=hp.get('gn_noise')\n",
        "                             )\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqcG4aPXVD9M"
      },
      "outputs": [],
      "source": [
        "if tuning_on==True:\n",
        "  # Create a tuner and search for the best hyperparameters\n",
        "  tuner = BayesianOptimization(create_turner_model,\n",
        "                              objective=kt.Objective(\"val_R2_Score\", \"max\"),\n",
        "                              hyperparameters=hp, max_trials=101, overwrite=True)\n",
        "\n",
        "  stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_R2_Score', patience=7, mode=\"max\", start_from_epoch=3)\n",
        "  reduce_ = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_R2_Score', factor=0.5, patience=3, min_lr=0.0001, mode=\"max\")\n",
        "\n",
        "  tuner.search(preprocessed_train_ds, validation_data=preprocessed_valid_ds, epochs=31, callbacks=[stop_early,reduce_])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxJo0ii3VD9M"
      },
      "outputs": [],
      "source": [
        "if tuning_on==True:\n",
        "  print(tuner.get_best_hyperparameters(4)[0].values)\n",
        "  print(tuner.get_best_hyperparameters(4)[1].values)\n",
        "  print(tuner.get_best_hyperparameters(4)[2].values)\n",
        "  print(tuner.get_best_hyperparameters(4)[3].values)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "0. {'main_activation': 'prelu', 'trans_activation': 'prelu', 'dropout': 0.12, 'sp_dropout': 0.45, 'dense_blocks': 4, 'units_dense': 128, 'final_dense': 512, 'num_trans_cat_layers': 3, 'num_heads_cat': 8, 'gn_noise': 0.01} 0.8622984886169434\n",
        "1. {'main_activation': 'prelu', 'trans_activation': 'prelu', 'dropout': 0.15, 'sp_dropout': 0.15, 'dense_blocks': 4, 'units_dense': 256, 'final_dense': 64, 'num_trans_cat_layers': 1, 'num_heads_cat': 4, 'gn_noise': 0.0125} 0.8612829446792603\n",
        "2. {'main_activation': 'prelu', 'trans_activation': 'gelu', 'dropout': 0.15, 'sp_dropout': 0.475, 'dense_blocks': 4, 'units_dense': 256, 'final_dense': 64, 'num_trans_cat_layers': 1, 'num_heads_cat': 8, 'gn_noise': 0.005}\n",
        "3. {'main_activation': 'prelu', 'trans_activation': 'prelu', 'dropout': 0.15, 'sp_dropout': 0.475, 'dense_blocks': 4, 'units_dense': 256, 'final_dense': 256, 'num_trans_cat_layers': 3, 'num_heads_cat': 8, 'gn_noise': 0.05}\n",
        "4. {'main_activation': 'prelu', 'trans_activation': 'prelu', 'dropout': 0.15, 'sp_dropout': 0.15, 'dense_blocks': 4, 'units_dense': 64, 'final_dense': 256, 'num_trans_cat_layers': 1, 'num_heads_cat': 8, 'gn_noise': 0.05}"
      ],
      "metadata": {
        "id": "zD1UOKj9Z7Dh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cw5VpnEO44iA"
      },
      "source": [
        "### 5.8.4 Baseline v3:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_on=False"
      ],
      "metadata": {
        "id": "4rxgysM0k8Gq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.columns"
      ],
      "metadata": {
        "id": "FnY-u6-x44iB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nziJNl8S44iC"
      },
      "outputs": [],
      "source": [
        "features = ['MonsoonIntensity', 'TopographyDrainage', 'RiverManagement',\n",
        "       'Deforestation', 'Urbanization', 'ClimateChange', 'DamsQuality',\n",
        "       'Siltation', 'AgriculturalPractices', 'Encroachments',\n",
        "       'IneffectiveDisasterPreparedness', 'DrainageSystems',\n",
        "       'CoastalVulnerability', 'Landslides', 'Watersheds',\n",
        "       'DeterioratingInfrastructure', 'PopulationScore', 'WetlandLoss',\n",
        "       'InadequatePlanning', 'PoliticalFactors', 'col_11', 'col_12', 'col_8',\n",
        "       'average_score', 'median_score', 'hmean', 'delta', 'compound_effect',\n",
        "       'risk_mitigation_factors']\n",
        "\n",
        "feat_used = ['MonsoonIntensity', 'TopographyDrainage', 'RiverManagement',\n",
        "       'Deforestation', 'Urbanization', 'ClimateChange', 'DamsQuality',\n",
        "       'Siltation', 'AgriculturalPractices', 'Encroachments',\n",
        "       'IneffectiveDisasterPreparedness', 'DrainageSystems',\n",
        "       'CoastalVulnerability', 'Landslides', 'Watersheds',\n",
        "       'DeterioratingInfrastructure', 'PopulationScore', 'WetlandLoss',\n",
        "       'InadequatePlanning', 'PoliticalFactors',\n",
        "       'average_score', 'median_score', 'hmean', 'delta', 'compound_effect',\n",
        "       'risk_mitigation_factors']\n",
        "cat_features = ['col_11', 'col_12', 'col_8']\n",
        "\n",
        "x_scaler = StandardScaler()\n",
        "X_train = train_df[features].copy()\n",
        "X_test = test_df[features].copy()\n",
        "\n",
        "X_train[feat_used] =  x_scaler.fit_transform(X_train[feat_used])\n",
        "X_test[feat_used] =  x_scaler.transform(X_test[feat_used])\n",
        "\n",
        "X_train_df = pd.DataFrame(index=train_df.index, columns=features, data=X_train)\n",
        "X_test_df = pd.DataFrame(index=test_df.index, columns=features, data=X_test)\n",
        "\n",
        "X_train_df[\"FloodProbability\"] = y_scaled\n",
        "X_test_df[\"FloodProbability\"] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dekdtE8844iD"
      },
      "outputs": [],
      "source": [
        "X_train_df.describe().T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdWTQGwQ44iD"
      },
      "outputs": [],
      "source": [
        "Cat_Feat_Entries = {}\n",
        "for f in cat_features:\n",
        "  Cat_Feat_Entries[f]=list(X_train_df[f].unique())\n",
        "\n",
        "Cat_Feat_Entries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRd2VxmD44iE"
      },
      "outputs": [],
      "source": [
        "def encode_inputs(inputs, add_dense_layer_cat = False, dense_dim_cat = 4, num_dense_exp=True, dense_dim_num = 4,\n",
        "                  list_categorical_nn=cat_features, Cat_Feat_Entries=Cat_Feat_Entries,  name=\"enc\"):\n",
        "\n",
        "    encoded_categorical_feature_list = []\n",
        "    numerical_feature_list = []\n",
        "\n",
        "    for feature_name in inputs:\n",
        "      if feature_name in list_categorical_nn:\n",
        "\n",
        "        vocabulary = Cat_Feat_Entries[feature_name]\n",
        "        #print(vocabulary,vocabulary.shape[1])\n",
        "\n",
        "        embedding = layers.Embedding(input_dim=len(vocabulary), output_dim=int(np.round(np.log1p(len(vocabulary))+1.1,0)), name=f\"{feature_name}_embedding\")\n",
        "        # Convert the index values to embedding representations.\n",
        "        encoded_categorical_feature = embedding(inputs[feature_name])\n",
        "\n",
        "        if add_dense_layer_cat==True:\n",
        "          encoded_categorical_feature = tf.keras.layers.Dense(dense_dim_cat, name=f\"cat_dense_{feature_name}_{name}\")(encoded_categorical_feature)\n",
        "\n",
        "        encoded_categorical_feature_list.append(encoded_categorical_feature)\n",
        "\n",
        "      else:\n",
        "        # Use the numerical features as-is.\n",
        "        numerical_feature = inputs[feature_name] #tf.expand_dims(inputs[feature_name], -1)\n",
        "\n",
        "        if num_dense_exp==True:\n",
        "          numerical_feature = keras.layers.Reshape((-1, 1))(numerical_feature)\n",
        "          numerical_feature = tf.keras.layers.Dense(dense_dim_num, name=f\"num_dense_{feature_name}_{name}\")(numerical_feature)\n",
        "\n",
        "        numerical_feature_list.append(numerical_feature)\n",
        "\n",
        "#    for feat in encoded_categorical_feature_list:\n",
        "#      print(feat.name, feat.shape)\n",
        "#    for feat in numerical_feature_list:\n",
        "#      print(feat.name, feat.shape)\n",
        "\n",
        "    return encoded_categorical_feature_list, numerical_feature_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arnMLeGR44iE"
      },
      "outputs": [],
      "source": [
        "if train_on==True:\n",
        "  train_dataset = dataframe_to_dataset(X_train_df, batch_size=256, shuffle=True)\n",
        "  test_dataset = dataframe_to_dataset(X_test_df, batch_size=256, shuffle=False)\n",
        "\n",
        "  feature_space_dict = FeatureSpace(\n",
        "                                features={**{a:FeatureSpace.float() for a in feat_used},\n",
        "                                          **{a:FeatureSpace.integer_categorical(num_oov_indices=1, output_mode=\"one_hot\") for a in cat_features}},\n",
        "                                #crosses = [FeatureSpace.cross((\"col_11\",\"col_12\"), crossing_dim=32)],\n",
        "                                output_mode=\"dict\"\n",
        "                                )\n",
        "\n",
        "  train_ds_with_no_labels = train_dataset.map(lambda x, *_: x)\n",
        "  print(\"Adapting Features Space....\")\n",
        "  feature_space_dict.adapt(train_ds_with_no_labels)\n",
        "\n",
        "  preprocessed_train_ds = train_dataset.map(lambda x, y: (feature_space_dict(x), y), num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
        "  preprocessed_test_ds = test_dataset.map(lambda x, y: (feature_space_dict(x), y), num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abbffT5k44iF"
      },
      "outputs": [],
      "source": [
        "from keras import ops\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBVLxmY844iF"
      },
      "outputs": [],
      "source": [
        "def create_BASELINE_v3(fs, name=\"baseline_v3\",\n",
        "                       learning_rate = 0.001,\n",
        "                       main_activation=\"selu\",\n",
        "                       trans_activation=\"swish\",\n",
        "                       dropout=0.5,\n",
        "                       sp_dropout=0.33,\n",
        "                       dense_blocks=5,\n",
        "                       units_dense=128,\n",
        "                       final_dense=256,\n",
        "                       num_trans_cat_layers = 3,\n",
        "                       num_heads_cat=8,\n",
        "                       gn_noise=0.025,\n",
        "                       add_dense_layer_cat = False, #encoding option\n",
        "                       dense_dim_cat = 4, #encoding option\n",
        "                       num_dense_exp = False, #encoding option\n",
        "                       dense_dim_num = 4 #encoding option\n",
        "                       ):\n",
        "\n",
        "\n",
        "    encoded_features = fs.get_encoded_features()\n",
        "\n",
        "\n",
        "    cat,num = encode_inputs(inputs=encoded_features,\n",
        "                            add_dense_layer_cat = add_dense_layer_cat,\n",
        "                            dense_dim_cat = dense_dim_cat,\n",
        "                            num_dense_exp = num_dense_exp,\n",
        "                            dense_dim_num = dense_dim_num,\n",
        "                            list_categorical_nn=cat_features,\n",
        "                            Cat_Feat_Entries=Cat_Feat_Entries,\n",
        "                            name=\"enc\")\n",
        "\n",
        "  ################################Transformer#######################################\n",
        "    cat_reshaped_att = []\n",
        "\n",
        "    for num_feat, feat in enumerate(cat):\n",
        "      for lay_n in range(num_trans_cat_layers):\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "                                                    num_heads=num_heads_cat,\n",
        "                                                    key_dim=feat.shape[2],\n",
        "                                                    dropout=dropout,\n",
        "                                                    name=f\"multihead_attention_{cat_features[num_feat]}_{lay_n}\",\n",
        "                                                    )(feat, feat)\n",
        "\n",
        "        x = layers.Add(name=f\"skip_connection1_{cat_features[num_feat]}_{lay_n}\")([attention_output, feat])\n",
        "        # Layer normalization 1.\n",
        "        x = layers.LayerNormalization(name=f\"layer_norm1_{cat_features[num_feat]}_{lay_n}\", epsilon=1e-6)(x)\n",
        "          # Feedforward.\n",
        "        feedforward_output = create_mlp(hidden_units=[feat.shape[2]],\n",
        "                                      dropout_rate=dropout,\n",
        "                                      activation=trans_activation,\n",
        "                                      normalization_layer=partial(layers.LayerNormalization, epsilon=1e-6),  # using partial to provide keyword arguments before initialization\n",
        "                                      name=f\"feedforward_{cat_features[num_feat]}_{lay_n}\")(x)\n",
        "\n",
        "        x = layers.Add(name=f\"skip_connection2_{cat_features[num_feat]}_{lay_n}\")([feedforward_output, x])\n",
        "\n",
        "        feat = layers.LayerNormalization(name=f\"layer_norm2_{cat_features[num_feat]}_{lay_n}\", epsilon=1e-6)(x)\n",
        "\n",
        "      cat_reshaped_att.append(feat)\n",
        "\n",
        "\n",
        "    #### Concat\n",
        "    # Categorical\n",
        "    x_cat = tf.keras.layers.Concatenate(name = \"categorical_conc_flat\")([tf.keras.layers.Flatten()(tf.keras.layers.SpatialDropout1D(sp_dropout)(cat_emb)) for cat_emb in cat_reshaped_att])\n",
        "\n",
        "    # Numerical\n",
        "    x_num_concat = tf.keras.layers.Concatenate(name=\"x_num_concat\")([n for n in num])\n",
        "\n",
        "    # Dense Layers:\n",
        "\n",
        "    x_concat = tf.keras.layers.Concatenate(name=\"x_concat_final\")([x_num_concat,x_cat])\n",
        "    x = layers.BatchNormalization(name=f\"batch_norm_final\", epsilon=1e-6)(x_concat)\n",
        "    x = tf.keras.layers.Dropout(dropout,name=f\"dropout_final_concat\")(x)\n",
        "    x = tf.keras.layers.GaussianNoise(stddev=gn_noise, name=f\"gsn_layer\")(x)\n",
        "\n",
        "    for block_number, block in enumerate(range(dense_blocks)):\n",
        "      x = dense_residual_block(units=units_dense, dropout_rate=dropout, activation=main_activation, kr=0, attention=False, name=f\"drb_{block_number}\", norm=\"batch\")(x)\n",
        "\n",
        "    x=dense_block(final_dense, dropout_rate=dropout, activation=main_activation, name=f\"drb_final\")(x)\n",
        "    x = tf.keras.layers.Concatenate(name=\"x_concat_skip\")([x_concat,x])\n",
        "\n",
        "    output = tf.keras.layers.Dense(1, activation=\"sigmoid\", name=\"output_layer\")(x)\n",
        "\n",
        "\n",
        "    model = keras.Model(inputs=encoded_features, outputs=output)\n",
        "\n",
        "\n",
        "\n",
        "    model.compile(\n",
        "                  optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "                  loss=tf.keras.losses.MeanSquaredError(name='mse'),\n",
        "                  metrics=[tf.keras.metrics.MeanSquaredError(name=\"mse\"),\n",
        "                          tf.keras.metrics.R2Score(name=\"R2_Score\")]\n",
        "                )\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Btl90TJL44iG"
      },
      "outputs": [],
      "source": [
        "if train_on==True:\n",
        "\n",
        "  model = create_BASELINE_v3(feature_space_dict)\n",
        "  model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hp3egP5a44iG"
      },
      "outputs": [],
      "source": [
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjGb4dbH44iH"
      },
      "outputs": [],
      "source": [
        "#plot_model(\n",
        "#            model,\n",
        "#            show_shapes=True,\n",
        "#            show_layer_names=True,\n",
        "#            rankdir=\"LR\",\n",
        " #           expand_nested=True,\n",
        " #           dpi=200,\n",
        " #           show_trainable=True\n",
        " #           )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JwNd7MO44iJ"
      },
      "source": [
        "##### **KerasTuner**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-9-Wo1P44iK"
      },
      "outputs": [],
      "source": [
        "tuning_on=False\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbbHKxAP44iK"
      },
      "outputs": [],
      "source": [
        "if tuning_on==True:\n",
        "  X_trn, X_val = train_test_split(X_train_df, random_state=1978, test_size=0.70, stratify=X_train_df[\"FloodProbability\"])\n",
        "  X_trn, X_val = train_test_split(X_trn, random_state=1978, test_size=0.3, stratify=X_trn[\"FloodProbability\"])\n",
        "\n",
        "  feature_space_dict = FeatureSpace(\n",
        "                                features={**{a:FeatureSpace.float() for a in feat_used},\n",
        "                                          **{a:FeatureSpace.integer_categorical(num_oov_indices=1, output_mode=\"one_hot\") for a in cat_features}},\n",
        "                                #crosses = [FeatureSpace.cross((\"col_11\",\"col_12\"), crossing_dim=32)],\n",
        "                                output_mode=\"dict\"\n",
        "                                )\n",
        "\n",
        "\n",
        "  train_dataset = dataframe_to_dataset(X_trn, batch_size=1024, shuffle=True)\n",
        "  valid_dataset = dataframe_to_dataset(X_val, batch_size=1024, shuffle=False)\n",
        "\n",
        "  train_ds_with_no_labels = train_dataset.map(lambda x, *_: x)\n",
        "  feature_space_dict.adapt(train_ds_with_no_labels)\n",
        "\n",
        "  preprocessed_train_ds = train_dataset.map(lambda x, y: (feature_space_dict(x), y), num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
        "  preprocessed_valid_ds = valid_dataset.map(lambda x, y: (feature_space_dict(x), y), num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if tuning_on==True:\n",
        "  print(X_trn.shape, X_val.shape)"
      ],
      "metadata": {
        "id": "cZe32kgO44iL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLJFxzMp44iL"
      },
      "outputs": [],
      "source": [
        "if tuning_on==True:\n",
        "  # Define the hyperparameter search space: EXPERIMENT 1\n",
        "  hp = kt.HyperParameters()\n",
        "  hp.Choice('main_activation', [\"prelu\"])\n",
        "  hp.Choice('trans_activation', [\"prelu\",\"gelu\"])\n",
        "  hp.Float('dropout',0.10,0.21, step=0.01)\n",
        "  hp.Float('sp_dropout',0.15,0.30, step=0.01)\n",
        "  hp.Choice('dense_blocks', values=[4,5])\n",
        "  hp.Choice('units_dense', values=[128,256,512])\n",
        "  hp.Choice('final_dense', values=[64,128,256,512])\n",
        "  hp.Choice('num_trans_cat_layers', values=[1,3])\n",
        "  hp.Choice('num_heads_cat', values=[4,8])\n",
        "  hp.Float('gn_noise', 0.01,0.05, step=0.0025)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOzDM_q944iL"
      },
      "outputs": [],
      "source": [
        "# TRAIN MODEL\n",
        "# Define Preprocessing steps:\n",
        "def create_turner_model(hp):\n",
        "\n",
        "  model = create_BASELINE_v3(feature_space_dict,\n",
        "                             name=\"baseline_v1\",\n",
        "                             learning_rate = 0.005,\n",
        "                             main_activation=hp.get('main_activation'),\n",
        "                             trans_activation=hp.get('trans_activation'),\n",
        "                             dropout=hp.get('dropout'),\n",
        "                             sp_dropout=hp.get('sp_dropout'),\n",
        "                             dense_blocks=hp.get('dense_blocks'),\n",
        "                             units_dense=hp.get('units_dense'),\n",
        "                             final_dense=hp.get('final_dense'),\n",
        "                             num_trans_cat_layers = hp.get('num_trans_cat_layers'),\n",
        "                             num_heads_cat=hp.get('num_heads_cat'),\n",
        "                             gn_noise=hp.get('gn_noise')\n",
        "                             )\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MlKVu47u44iM"
      },
      "outputs": [],
      "source": [
        "if tuning_on==True:\n",
        "  # Create a tuner and search for the best hyperparameters\n",
        "  tuner = BayesianOptimization(create_turner_model,\n",
        "                              objective=kt.Objective(\"val_R2_Score\", \"max\"),\n",
        "                              hyperparameters=hp, max_trials=26, overwrite=True)\n",
        "\n",
        "  stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_R2_Score', patience=7, mode=\"max\", start_from_epoch=3)\n",
        "  reduce_ = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_R2_Score', factor=0.5, patience=3, min_lr=0.0001, mode=\"max\")\n",
        "\n",
        "  tuner.search(preprocessed_train_ds, validation_data=preprocessed_valid_ds, epochs=31, callbacks=[stop_early,reduce_])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8R9h3gZ44iM"
      },
      "outputs": [],
      "source": [
        "if tuning_on==True:\n",
        "  print(tuner.get_best_hyperparameters(4)[0].values)\n",
        "  print(tuner.get_best_hyperparameters(4)[1].values)\n",
        "  print(tuner.get_best_hyperparameters(4)[2].values)\n",
        "  print(tuner.get_best_hyperparameters(4)[3].values)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trial 76 Complete [00h 18m 11s]\n",
        "val_R2_Score: 0.864101767539978\n",
        "\n",
        "- Best val_R2_Score So Far: 0.8656777739524841\n",
        "Total elapsed time: 21h 08m 56s\n",
        "\n",
        "Search: Running Trial #77 0.8647\n",
        "\n",
        "    Value             |Best Value So Far |Hyperparameter\n",
        "    prelu             |prelu             |main_activation\n",
        "    prelu             |prelu             |trans_activation\n",
        "    0.1               |0.1               |dropout\n",
        "    0.225             |0.225             |sp_dropout\n",
        "    5                 |5                 |dense_blocks\n",
        "    256               |128               |units_dense\n",
        "    128               |128               |final_dense\n",
        "    3                 |3                 |num_trans_cat_layers\n",
        "    4                 |8                 |num_heads_cat\n",
        "    0.04              |0.0425            |gn_noise"
      ],
      "metadata": {
        "id": "U9QG2eZC9KyT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3o8PxG8IBWP5"
      },
      "source": [
        "## 6.0 A Analyze Forecas for linear Models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jM96xSHDLM_"
      },
      "outputs": [],
      "source": [
        "# Containers for results\n",
        "oof, test_pred = {}, {}\n",
        "y_pred_test_final_dict = {}\n",
        "COMPUTE_TEST_PRED=True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUUEbZAzBhHq"
      },
      "outputs": [],
      "source": [
        "rs_list=[17,3,78,18,20,42,38,25,1978,1981]\n",
        "\n",
        "def cross_validate(model, label,  train, test, target_feat=\"FloodProbability\", features=[], n_repeats=1, rs_list=rs_list, es=True, model_type=\"other\", n_splits=5):\n",
        "    \"\"\"Compute out-of-fold and test predictions for a given model.\n",
        "\n",
        "    Out-of-fold and test predictions are stored in the global variables\n",
        "    oof and test_pred, respectively.\n",
        "\n",
        "    If n_repeats > 1, the model is trained several times with different seeds.\n",
        "\n",
        "    All predictions are clipped to the interval [1, 29].\n",
        "    \"\"\"\n",
        "    scores = []\n",
        "    oof_preds = np.full_like(train[target_feat], np.nan, dtype=float)\n",
        "    y_pred_test_final = np.zeros_like(test[features[0]], dtype=float)\n",
        "\n",
        "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=1)\n",
        "    for fold, (idx_tr, idx_va) in enumerate(kf.split(train, train[target_feat])):\n",
        "        X_tr = train.iloc[idx_tr][features]\n",
        "        X_va = train.iloc[idx_va][features]\n",
        "        y_tr = train.iloc[idx_tr][target_feat]\n",
        "        y_va = train.iloc[idx_va][target_feat]\n",
        "\n",
        "        y_pred = np.zeros_like(y_va, dtype=float)\n",
        "        y_pred_test = np.zeros_like(test[features[0]], dtype=float)\n",
        "\n",
        "        for i in range(n_repeats):\n",
        "            m = clone(model)\n",
        "            if n_repeats > 1:\n",
        "                try:\n",
        "                  m.set_params(random_state=rs_list[i])\n",
        "                except:\n",
        "                  pass\n",
        "\n",
        "            if 'lgbm' in model_type:\n",
        "                fit_params={\"eval_set\":(X_va,y_va),\n",
        "                            \"callbacks\":[]}\n",
        "                if es==True:\n",
        "                    fit_params[\"callbacks\"].append(early_stopping(stopping_rounds=51))\n",
        "\n",
        "                m.fit(X_tr, y_tr,**fit_params)\n",
        "\n",
        "            if 'xgb' in model_type:\n",
        "\n",
        "                fit_params={\"eval_set\":[(X_va,y_va)],\n",
        "                            \"callbacks\":[]}\n",
        "\n",
        "                if es==True:\n",
        "                    fit_params[\"callbacks\"].append(xgb.callback.EarlyStopping(rounds=101, save_best=True, metric_name='rmse'))\n",
        "\n",
        "                m.fit(X_tr, y_tr, verbose=False, **fit_params)\n",
        "\n",
        "            if 'cat' in model_type:\n",
        "\n",
        "                fit_params={\"eval_set\":[(X_va,y_va)]}\n",
        "                m.fit(X_tr, y_tr, **fit_params)\n",
        "\n",
        "            else:\n",
        "                m.fit(X_tr, y_tr)\n",
        "\n",
        "            y_pred += m.predict(X_va)\n",
        "            y_pred_test += m.predict(test[features])/n_repeats\n",
        "        y_pred /= n_repeats\n",
        "        y_pred_test_final += y_pred_test/n_splits\n",
        "\n",
        "        plot_regression_scatter(y_va,y_pred)\n",
        "\n",
        "        score = r2_score(y_va, y_pred)\n",
        "        print(\"#################################\")\n",
        "        print(f\"# Fold {fold}: R2={score:.5f}\")\n",
        "        print(\"#################################\")\n",
        "        scores.append(score)\n",
        "        oof_preds[idx_va] = y_pred\n",
        "    print(f\"{Fore.GREEN}# Overall: {np.array(scores).mean():.5f} {label}{Style.RESET_ALL}\")\n",
        "\n",
        "    oof[label] = oof_preds\n",
        "    y_pred_test_final_dict[label] =  y_pred_test_final"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **NN Model Dictionary**"
      ],
      "metadata": {
        "id": "1J4SP2Dc-r_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_nn_models = [\"spag_v0\",\"spag_v1\",\"spag_v2\",\"spag_v3\",\"spag_v4\",\"baseline_v0\",\"baseline_v1\",\"baseline_v2\",\"baseline_v3\",\"baseline_v4\"]"
      ],
      "metadata": {
        "id": "b4JjUrfj-raz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment(list_nn_models, train, test_data, experiment_name=\"baseline_nn\", splits=5, rs=42, target=\"FloodProbability\",batch_size=128,\n",
        "                   num_epochs=31, learning_rate=0.0025):\n",
        "\n",
        "  skf = KFold(n_splits=splits, random_state=rs, shuffle=True)\n",
        "\n",
        "  test_predictions = np.zeros((len(test_df),1))\n",
        "\n",
        "  test_results_df = pd.DataFrame(index=test_data.index, columns=[f\"{name}_{z}\" for name in list_nn_models for z in range(splits)])\n",
        "  oof_results_df = pd.DataFrame(index=train.index, columns=list_nn_models)\n",
        "\n",
        "  all_logloss = []\n",
        "  all_R2_pr = []\n",
        "  oof_results = np.empty(shape=(train.shape[0],1))\n",
        "\n",
        "  for i, (train_index, valid_index) in enumerate(skf.split(train,train[target])):\n",
        "\n",
        "    print(f\"\\nRunning CV {i}\\n\")\n",
        "    ########################################################################## Prepare the Dataset:\n",
        "    X_trn = train.iloc[train_index,:]\n",
        "    X_val = train.iloc[valid_index,:]\n",
        "\n",
        "    X = X_trn.drop(columns=[target]).copy()\n",
        "    y = X_trn[target].copy()\n",
        "\n",
        "    val_X = X_val.drop(columns=[target]).copy()\n",
        "    val_y = X_val[target].copy()\n",
        "\n",
        "    X_test = test_data.copy()\n",
        "    #################################################################### Prepare Datasets loaders:\n",
        "\n",
        "    train_dataset = dataframe_to_dataset(X_trn, batch_size=batch_size, shuffle=True)\n",
        "    valid_dataset = dataframe_to_dataset(X_val, batch_size=batch_size, shuffle=False)\n",
        "    test_dataset = dataframe_to_dataset(X_test, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    feature_space_dict_base = FeatureSpace(\n",
        "                                  features={**{a:FeatureSpace.float() for a in feat_used},\n",
        "                                            **{a:FeatureSpace.integer_categorical(num_oov_indices=1, output_mode=\"one_hot\") for a in cat_features}},\n",
        "                                  output_mode=\"dict\"\n",
        "                                  )\n",
        "    feature_space_dict_spag = FeatureSpace(\n",
        "                                  features={**{a:FeatureSpace.float() for a in feat_used}},\n",
        "                                  output_mode=\"dict\"\n",
        "                                  )\n",
        "\n",
        "    train_ds_with_no_labels = train_dataset.map(lambda x, *_: x)\n",
        "    print(\"Adapting Features Space....\")\n",
        "    feature_space_dict.adapt(train_ds_with_no_labels)\n",
        "\n",
        "    preprocessed_train_ds_base = train_dataset.map(lambda x, y: (feature_space_dict_base(x), y), num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
        "    preprocessed_valid_ds_base = valid_dataset.map(lambda x, y: (feature_space_dict_base(x), y), num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
        "    preprocessed_test_ds_base = test_dataset.map(lambda x, y: (feature_space_dict_base(x), y), num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    preprocessed_train_ds_spag = train_dataset.map(lambda x, y: (feature_space_dict_spag(x), y), num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
        "    preprocessed_valid_ds_spag = valid_dataset.map(lambda x, y: (feature_space_dict_spag(x), y), num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
        "    preprocessed_test_ds_spag = test_dataset.map(lambda x, y: (feature_space_dict_spag(x), y), num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    #################################################################################### Models:\n",
        "    dict_nn_models = {\"spag_v0\": create_spaghetti(feature_space_dict_spag,name=\"spag_v0\",learning_rate = 0.005,main_activation=\"relu\",\n",
        "                                              dropout=0.25,dense_blocks=4,units_dense = 256,kr = 0),\n",
        "                      \"spag_v1\": create_spaghetti(feature_space_dict_spag,name=\"spag_v1\",learning_rate = 0.005,main_activation=\"relu\",\n",
        "                                                  dropout=0.20,dense_blocks=5,units_dense = 256,kr = 0),\n",
        "                      \"spag_v2\": create_spaghetti(feature_space_dict_spag,name=\"spag_v2\",learning_rate = 0.005,main_activation=\"prelu\",\n",
        "                                                  dropout=0.15,dense_blocks=5,units_dense = 128,kr = 0),\n",
        "                      \"spag_v3\": create_spaghetti(feature_space_dict_spag,name=\"spag_v3\",learning_rate = 0.005,main_activation=\"relu\",\n",
        "                                                  dropout=0.15,dense_blocks=6,units_dense = 128,kr = 0),\n",
        "                      \"spag_v4\": create_spaghetti(feature_space_dict_spag,name=\"spag_v4\",learning_rate = 0.005,main_activation=\"prelu\",\n",
        "                                                  dropout=0.2,dense_blocks=4,units_dense = 256,kr = 0),\n",
        "                      \"baseline_v0\": create_BASELINE(feature_space_dict_base,name=\"baseline_v0\",learning_rate = 0.0025,\n",
        "                                                    main_activation= 'prelu', trans_activation='prelu',dropout= 0.12,\n",
        "                                                    sp_dropout=0.45, dense_blocks=4, units_dense= 128,final_dense= 512,\n",
        "                                                    num_trans_cat_layers= 3, num_heads_cat= 8, gn_noise=0.01),\n",
        "                      \"baseline_v1\": create_BASELINE(feature_space_dict_base,name=\"baseline_v1\",learning_rate = 0.0025,\n",
        "                                                    main_activation= 'prelu', trans_activation='prelu',dropout= 0.15,\n",
        "                                                    sp_dropout=0.15, dense_blocks=4, units_dense= 256,final_dense= 64,\n",
        "                                                    num_trans_cat_layers= 1, num_heads_cat= 4, gn_noise=0.0125),\n",
        "                      \"baseline_v2\": create_BASELINE_v3(feature_space_dict_base,name=\"baseline_v2\",learning_rate = 0.0025,\n",
        "                                                        main_activation=\"prelu\",trans_activation=\"prelu\",dropout=0.1,sp_dropout=0.225,\n",
        "                                                        dense_blocks=5,units_dense=128,final_dense=128,num_trans_cat_layers = 3,\n",
        "                                                        num_heads_cat=8,gn_noise=0.0425),\n",
        "                      \"baseline_v3\": create_BASELINE_v3(feature_space_dict_base,name=\"baseline_v3\",learning_rate = 0.0025,\n",
        "                                                        main_activation=\"prelu\",trans_activation=\"prelu\",dropout=0.1,sp_dropout=0.225,\n",
        "                                                        dense_blocks=5,units_dense=256,final_dense=128,num_trans_cat_layers = 3,\n",
        "                                                        num_heads_cat=4,gn_noise=0.04),\n",
        "                      \"baseline_v4\": create_BASELINE_v3(feature_space_dict_base,name=\"baseline_v4\",learning_rate = 0.0025,\n",
        "                                                        main_activation=\"prelu\",trans_activation=\"prelu\",dropout=0.1,sp_dropout=0.45,\n",
        "                                                        dense_blocks=4,units_dense=512,final_dense=128,num_trans_cat_layers = 3,\n",
        "                                                        num_heads_cat=8,gn_noise=0.015)}\n",
        "    ##################################################################### Relevant Folders\n",
        "    folders_experiment = f\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Models/S4E4_Abalone/neural_networks/{experiment_name}/\"\n",
        "    folders_experiment_cv1 = f\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Models/S4E4_Abalone/neural_networks/\"\n",
        "    folders_experiment_cv2= f\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Models/S4E4_Abalone/neural_networks/{experiment_name}\"\n",
        "    folders_experiment_cv3 = f\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Models/S4E4_Abalone/neural_networks/{experiment_name}/cv_{i}/\"\n",
        "    folder_data = f\"/content/drive/MyDrive/Exercises/Studies_Structured_Data//Data/S4E4_Abalone\"\n",
        "    list_directories = [folder_data,folders_experiment,folders_experiment_cv1,folders_experiment_cv2,folders_experiment_cv3]\n",
        "\n",
        "    for path in list_directories:\n",
        "      try:\n",
        "          os.mkdir(path)\n",
        "      except OSError as error:\n",
        "          print(f\"{path} already exists\")\n",
        "    ##################################################################### Generate and Fit Model\n",
        "    # Callbacks Folder:\n",
        "    checkpoint_filepath = folders_experiment + f'checkpoint/{experiment_name}.weights.h5'\n",
        "\n",
        "    # Generate the Model:\n",
        "\n",
        "    for mod in dict_nn_models.keys():\n",
        "      if \"spag\"  in mod:\n",
        "\n",
        "        train_dataset = dataframe_to_dataset(X_trn, batch_size=batch_size, shuffle=True)\n",
        "        valid_dataset = dataframe_to_dataset(X_val, batch_size=batch_size, shuffle=False)\n",
        "        test_dataset = dataframe_to_dataset(X_test, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "        feature_space_dict = FeatureSpace(\n",
        "                                features={**{a:FeatureSpace.float() for a in feat_used},\n",
        "                                          **{a:FeatureSpace.integer_categorical(num_oov_indices=1, output_mode=\"one_hot\") for a in cat_features}},\n",
        "                                output_mode=\"dict\"\n",
        "                                )\n",
        "\n",
        "        train_ds_with_no_labels = train_dataset.map(lambda x, *_: x)\n",
        "        print(\"Adapting Features Space....\")\n",
        "        feature_space_dict.adapt(train_ds_with_no_labels)\n",
        "\n",
        "        preprocessed_train_ds = train_dataset.map(lambda x, y: (feature_space_dict(x), y), num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
        "        preprocessed_valid_ds = valid_dataset.map(lambda x, y: (feature_space_dict(x), y), num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
        "        preprocessed_test_ds = test_dataset.map(lambda x, y: (feature_space_dict(x), y), num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "        k = randint(0, 1)\n",
        "        rep = randint(0, 10)\n",
        "        if k==0:\n",
        "          xt = X_trn.sample(frac=0.15, random_state=rs_list[rep])\n",
        "        else:\n",
        "          xt, xv = train_test_split(X_trn,random_state=rs_list[rep], train_size=0.15, shuffle=True, stratify=X_trn[target])\n",
        "\n",
        "        train_dataset_subset = dataframe_to_dataset(xt, batch_size=batch_size, shuffle=True)\n",
        "        preprocessed_train_ds_subset = train_dataset_subset.map(lambda x, y: (feature_space_dict(x), y), num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "        model = dict_nn_models[mod]\n",
        "\n",
        "        print(f\"Start training the model: CV-{i} and Model: {mod}\")\n",
        "        history = model.fit(preprocessed_train_ds_subset,\n",
        "                            epochs=num_epochs,\n",
        "                            callbacks=[keras.callbacks.EarlyStopping(monitor='val_R2_Score', patience=5, mode=\"max\",\n",
        "                                                      start_from_epoch=5,restore_best_weights=True),\n",
        "                                      keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath,\n",
        "                                                        save_weights_only=True,\n",
        "                                                        monitor=\"val_R2_Score\",\n",
        "                                                        mode='max',\n",
        "                                                        save_best_only=True),\n",
        "                                      keras.callbacks.ReduceLROnPlateau(monitor='val_R2_Score', factor=0.5,\n",
        "                                                              patience=3, min_lr=0.000025, mode=\"max\")],\n",
        "                            validation_data=preprocessed_valid_ds)\n",
        "        print(\"Model training finished\")\n",
        "\n",
        "        model.load_weights(checkpoint_filepath)\n",
        "        model.evaluate(preprocessed_valid_ds, verbose=0)\n",
        "\n",
        "        plot_training_session(history)\n",
        "\n",
        "        oof_res = model.predict(preprocessed_valid_ds)\n",
        "        test_prob = model.predict(preprocessed_test_ds)\n",
        "\n",
        "        test_results_df.loc[:,f\"{mod}_{i}\"] = test_prob\n",
        "        oof_results_df.loc[valid_index,mod] = oof_res\n",
        "\n",
        "        score = r2_score(val_y, oof_res)\n",
        "\n",
        "        print(f\"Model {mod} - CV {i} R2 Score: {score}\")\n",
        "\n",
        "        all_R2_pr.append(round(score, 3))\n",
        "        gc.collect()\n",
        "\n",
        "      else:\n",
        "        train_dataset = dataframe_to_dataset(X_trn, batch_size=batch_size, shuffle=True)\n",
        "        valid_dataset = dataframe_to_dataset(X_val, batch_size=batch_size, shuffle=False)\n",
        "        test_dataset = dataframe_to_dataset(X_test, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "        feature_space_dict = FeatureSpace(\n",
        "                                features={**{a:FeatureSpace.float() for a in feat_used},\n",
        "                                          **{a:FeatureSpace.integer_categorical(num_oov_indices=1, output_mode=\"one_hot\") for a in cat_features}},\n",
        "                                output_mode=\"dict\"\n",
        "                                )\n",
        "\n",
        "        train_ds_with_no_labels = train_dataset.map(lambda x, *_: x)\n",
        "        print(\"Adapting Features Space....\")\n",
        "        feature_space_dict.adapt(train_ds_with_no_labels)\n",
        "\n",
        "        preprocessed_train_ds = train_dataset.map(lambda x, y: (feature_space_dict(x), y), num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
        "        preprocessed_valid_ds = valid_dataset.map(lambda x, y: (feature_space_dict(x), y), num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
        "        preprocessed_test_ds = test_dataset.map(lambda x, y: (feature_space_dict(x), y), num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "        k = randint(0, 1)\n",
        "        rep = randint(0, 10)\n",
        "        if k==0:\n",
        "          xt = X_trn.sample(frac=0.15, random_state=rs_list[rep])\n",
        "        else:\n",
        "          xt, xv = train_test_split(X_trn,random_state=rs_list[rep], train_size=0.15, shuffle=True, stratify=X_trn[target])\n",
        "\n",
        "        train_dataset_subset = dataframe_to_dataset(xt, batch_size=batch_size, shuffle=True)\n",
        "        preprocessed_train_ds_subset = train_dataset_subset.map(lambda x, y: (feature_space_dict(x), y), num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "        model = dict_nn_models[mod]\n",
        "\n",
        "        print(f\"Start training the model: CV-{i} and Model: {mod}\")\n",
        "        history = model.fit(preprocessed_train_ds_subset,\n",
        "                            epochs=num_epochs,\n",
        "                            callbacks=[keras.callbacks.EarlyStopping(monitor='val_R2_Score', patience=5, mode=\"max\",\n",
        "                                                      start_from_epoch=5,restore_best_weights=True),\n",
        "                                      keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath,\n",
        "                                                        save_weights_only=True,\n",
        "                                                        monitor=\"val_R2_Score\",\n",
        "                                                        mode='max',\n",
        "                                                        save_best_only=True),\n",
        "                                      keras.callbacks.ReduceLROnPlateau(monitor='val_R2_Score', factor=0.5,\n",
        "                                                              patience=3, min_lr=0.000025, mode=\"max\")],\n",
        "                            validation_data=preprocessed_valid_ds)\n",
        "        print(\"Model training finished\")\n",
        "\n",
        "        model.load_weights(checkpoint_filepath)\n",
        "        model.evaluate(preprocessed_valid_ds, verbose=0)\n",
        "\n",
        "        plot_training_session(history)\n",
        "\n",
        "        oof_res = model.predict(preprocessed_valid_ds)\n",
        "        test_prob = model.predict(preprocessed_test_ds)\n",
        "\n",
        "        test_results_df.loc[:,f\"{mod}_{i}\"] = test_prob\n",
        "        oof_results_df.loc[valid_index,mod] = oof_res\n",
        "\n",
        "        score = r2_score(val_y, oof_res)\n",
        "\n",
        "        print(f\"Model {mod} - CV {i} R2 Score: {score}\")\n",
        "\n",
        "        all_R2_pr.append(round(score, 3))\n",
        "        gc.collect()\n",
        "\n",
        "  print(f\"All Valuation R2: {all_R2_pr}\")\n",
        "\n",
        "  return test_results_df, oof_results"
      ],
      "metadata": {
        "id": "JIRoprHaATlq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ap706haDZo5_"
      },
      "source": [
        "#### Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHFIJk6kRxEl"
      },
      "outputs": [],
      "source": [
        "params_ransac={'min_samples': 0.45}\n",
        "params_huber={'alpha': 0.3671905753537766, 'epsilon': 4.5}\n",
        "params_lgbm = {\"learning_rate\":0.01,'n_estimators':2_000,\n",
        "               'random_state':42,\n",
        "               'device':\"cpu\",\n",
        "               'num_leaves': 108,\n",
        "               'min_child_samples': 60,\n",
        "               'min_child_weight': 0.009869114516750276,\n",
        "               'reg_alpha': 0.005445305113175027,\n",
        "               'reg_lambda': 0.0011418268475368125,\n",
        "               'max_depth': 8,\n",
        "               'bagging_freq': 5,\n",
        "               'max_bin': 481,\n",
        "               'feature_fraction': 1.0,\n",
        "               'bagging_fraction': 0.8200000000000001}\n",
        "\n",
        "params_lgbm = {\"learning_rate\":0.01,\n",
        "               'n_estimators':2_000,\n",
        "               'random_state':42,\n",
        "               'device':\"cpu\",\n",
        "               'num_leaves': 108,\n",
        "               'min_child_samples': 63,\n",
        "               'min_child_weight': 0.036102158869668395,\n",
        "               'reg_alpha': 0.0004888277171563409,\n",
        "               'reg_lambda': 0.004923185364540501,\n",
        "               'max_depth': 9,\n",
        "               'bagging_freq': 7,\n",
        "               'max_bin': 501,\n",
        "               'feature_fraction': 1.0,\n",
        "               'bagging_fraction': 0.75}\n",
        "\n",
        "params_xgb = {'grow_policy':\"lossguide\",\n",
        "              'objective':'reg:squarederror',\n",
        "              'tree_method':'hist',\n",
        "              'device':\"cpu\",\n",
        "              'enable_categorical': True,\n",
        "              'verbosity':0,\n",
        "              'n_estimators' :2000,\n",
        "              'eta' :0.0025,\n",
        "              'booster' : \"gbtree\",\n",
        "              'max_depth': 8,\n",
        "              'subsample': 0.85,\n",
        "              'colsample_bylevel': 0.65,\n",
        "              'gamma': 0,\n",
        "              'min_child_weight': 10.695814388584468,\n",
        "              'reg_lambda': 0.14948338043917855,\n",
        "              'reg_alpha': 0.025900915752743607,\n",
        "              'max_bin': 476}\n",
        "\n",
        "params_cat = {'grow_policy':         \"SymmetricTree\",\n",
        "              'loss_function':       'RMSE',\n",
        "              'eval_metric':         'RMSE',\n",
        "              'task_type':           'CPU',\n",
        "              'boosting_type':        'Plain',\n",
        "              'verbose':              0,\n",
        "              'n_estimators' :        1000,\n",
        "              'learning_rate' :       0.015,\n",
        "              'early_stopping_rounds': 101,\n",
        "              'boost_from_average':   True,\n",
        "              'colsample_bylevel':    0.91,\n",
        "              'max_depth':            16,\n",
        "              'l2_leaf_reg':          2.893709030040562,\n",
        "              'min_data_in_leaf':     50,\n",
        "              'random_strength':      1.0,\n",
        "              'max_bin':              401,\n",
        "              'bootstrap_type':       'Bernoulli',\n",
        "              'subsample':            0.8772046302417569}\n",
        "\n",
        "model_1 = RANSACRegressor(estimator=Ridge(alpha=2.0), loss=\"squared_error\",**params_ransac)\n",
        "model_2 = HuberRegressor(**params_huber)\n",
        "model_3 = LGBMRegressor(**params_lgbm)\n",
        "model_4 = XGBRegressor(**params_xgb)\n",
        "model_5 = CatBoostRegressor(**params_cat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUHu759tOzpd"
      },
      "source": [
        "#### Train Control:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9rq7Z9WOzds"
      },
      "outputs": [],
      "source": [
        "control_train={\n",
        "              \"ransac\":False,\n",
        "              \"huber\":False,\n",
        "              \"lgbm_v0\":False,\n",
        "              \"xgb_v0\":False,\n",
        "              \"cat_v0\":False,\n",
        "              \"nn_v0\":False\n",
        "              }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5i8iUZqYZsho"
      },
      "source": [
        "##### Ransac"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbVN-4BHSRAF"
      },
      "outputs": [],
      "source": [
        "if control_train[\"ransac\"]==True:\n",
        "  cross_validate(model_1, label=\"Ransac\",  train=X_train_df, test=X_test_df,features=features, target_feat=\"FloodProbability\",  n_repeats=1, rs_list=rs_list, es=True, model_type=\"other\", n_splits=5)\n",
        "else:\n",
        "  oof[\"Ransac\"] = np.squeeze(pd.read_csv(\"results_ensemble/Ransac_train.csv\", index_col=0).values)\n",
        "  y_pred_test_final_dict[\"Ransac\"] =  np.squeeze(pd.read_csv(\"results_ensemble/Ransac_test.csv\", index_col=0).values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7KItClVT3lI"
      },
      "outputs": [],
      "source": [
        "oof[\"Ransac\"].shape,y_pred_test_final_dict[\"Ransac\"].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAmn_fFEAky_"
      },
      "source": [
        "**FINAL SCORE**\n",
        "\n",
        "Overall: 0.84903 Ransac"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uf0bVe-6ZvZi"
      },
      "source": [
        "##### Huber"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0WQQ-SuSx3-"
      },
      "outputs": [],
      "source": [
        "if control_train[\"huber\"]==True:\n",
        "  cross_validate(model_2, label=\"Huber\",  train=X_train_df, test=X_test_df,features=features, target_feat=\"FloodProbability\",  n_repeats=1, rs_list=rs_list, es=True, model_type=\"other\", n_splits=5)\n",
        "else:\n",
        "  oof[\"Huber\"] = np.squeeze(pd.read_csv(\"results_ensemble/Huber_train.csv\", index_col=0).values)\n",
        "  y_pred_test_final_dict[\"Huber\"] =  np.squeeze(pd.read_csv(\"results_ensemble/Huber_test.csv\", index_col=0).values)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "coHnEtr8cP13"
      },
      "outputs": [],
      "source": [
        "oof[\"Huber\"].shape,y_pred_test_final_dict[\"Huber\"].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsDzdRTSArR5"
      },
      "source": [
        "**FINAL SCORE**\n",
        "\n",
        "Overall: 0.84903 Ransac"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFF8S9JTZx1j"
      },
      "source": [
        "##### LGBM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3m70so9Q_a3C"
      },
      "outputs": [],
      "source": [
        "if control_train[\"lgbm_v0\"]==True:\n",
        "  cross_validate(model_3, label=\"LGBM_v1\",  train=X_train_df, test=X_test_df,features=features, target_feat=\"FloodProbability\",  n_repeats=1, rs_list=rs_list, es=True, model_type=\"lgbm\", n_splits=5)\n",
        "else:\n",
        "  oof[\"LGBM_v0\"] = np.squeeze(pd.read_csv(\"results_ensemble/LGBM_v0_train.csv\", index_col=0).values)\n",
        "  y_pred_test_final_dict[\"LGBM_v0\"] =  np.squeeze(pd.read_csv(\"results_ensemble/LGBM_v0_test.csv\", index_col=0).values)\n",
        "  oof[\"LGBM_v1\"] = np.squeeze(pd.read_csv(\"results_ensemble/LGBM_v1_train.csv\", index_col=0).values)\n",
        "  y_pred_test_final_dict[\"LGBM_v1\"] =  np.squeeze(pd.read_csv(\"results_ensemble/LGBM_v1_test.csv\", index_col=0).values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0Y9L5M9cXHO"
      },
      "outputs": [],
      "source": [
        "oof[\"LGBM_v1\"].shape,y_pred_test_final_dict[\"LGBM_v1\"].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuhCXkzlAu6X"
      },
      "source": [
        "**FINAL SCORE**\n",
        "\n",
        "Overall: 0.87107 LGBM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yrt89khTX7p6"
      },
      "source": [
        "##### XGB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUlu5bFgX7qH"
      },
      "outputs": [],
      "source": [
        "if control_train[\"xgb_v0\"]==True:\n",
        "  cross_validate(model_4, label=\"xgb_v0\",  train=X_train_df, test=X_test_df,features=features, target_feat=\"FloodProbability\",  n_repeats=1, rs_list=rs_list, es=True, model_type=\"xgb\",\n",
        "                 n_splits=5)\n",
        "else:\n",
        "  oof[\"xgb_v0\"] = np.squeeze(pd.read_csv(\"results_ensemble/xgb_v0_train.csv\", index_col=0).values)\n",
        "  y_pred_test_final_dict[\"xgb_v0\"] =  np.squeeze(pd.read_csv(\"results_ensemble/xgb_v0_test.csv\", index_col=0).values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0EO-Oo2X7qI"
      },
      "outputs": [],
      "source": [
        "oof[\"xgb_v0\"].shape,y_pred_test_final_dict[\"xgb_v0\"].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5Scolf4X7qI"
      },
      "source": [
        "**FINAL SCORE**\n",
        "\n",
        "Overall: 0.87095 XGB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRkGY_IZacIa"
      },
      "source": [
        "##### CATBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGj3v6dXacIj"
      },
      "outputs": [],
      "source": [
        "if control_train[\"cat_v0\"]==True:\n",
        "  cross_validate(model_5, label=\"cat_v0\",  train=X_train_df, test=X_test_df,features=features, target_feat=\"FloodProbability\",  n_repeats=1, rs_list=rs_list, es=True, model_type=\"cat\",\n",
        "                 n_splits=5)\n",
        "else:\n",
        "  oof[\"cat_v0\"] = np.squeeze(pd.read_csv(\"results_ensemble/cat_v0_train.csv\", index_col=0).values)\n",
        "  y_pred_test_final_dict[\"cat_v0\"] =  np.squeeze(pd.read_csv(\"results_ensemble/cat_v0_test.csv\", index_col=0).values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LsGFRR-bacIj"
      },
      "outputs": [],
      "source": [
        "oof[\"cat_v0\"].shape,y_pred_test_final_dict[\"cat_v0\"].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Qd7CXXIacIj"
      },
      "source": [
        "**FINAL SCORE**\n",
        "\n",
        "Overall: 0.87030 CAT"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### NN Models Ensemble:"
      ],
      "metadata": {
        "id": "9_sbzyO8f7vX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = ['MonsoonIntensity', 'TopographyDrainage', 'RiverManagement',\n",
        "       'Deforestation', 'Urbanization', 'ClimateChange', 'DamsQuality',\n",
        "       'Siltation', 'AgriculturalPractices', 'Encroachments',\n",
        "       'IneffectiveDisasterPreparedness', 'DrainageSystems',\n",
        "       'CoastalVulnerability', 'Landslides', 'Watersheds',\n",
        "       'DeterioratingInfrastructure', 'PopulationScore', 'WetlandLoss',\n",
        "       'InadequatePlanning', 'PoliticalFactors', 'col_11', 'col_12', 'col_8',\n",
        "       'average_score', 'median_score', 'hmean', 'delta', 'compound_effect',\n",
        "       'risk_mitigation_factors']\n",
        "\n",
        "feat_used = ['MonsoonIntensity', 'TopographyDrainage', 'RiverManagement',\n",
        "       'Deforestation', 'Urbanization', 'ClimateChange', 'DamsQuality',\n",
        "       'Siltation', 'AgriculturalPractices', 'Encroachments',\n",
        "       'IneffectiveDisasterPreparedness', 'DrainageSystems',\n",
        "       'CoastalVulnerability', 'Landslides', 'Watersheds',\n",
        "       'DeterioratingInfrastructure', 'PopulationScore', 'WetlandLoss',\n",
        "       'InadequatePlanning', 'PoliticalFactors',\n",
        "       'average_score', 'median_score', 'hmean', 'delta', 'compound_effect',\n",
        "       'risk_mitigation_factors']\n",
        "cat_features = ['col_11', 'col_12', 'col_8']\n",
        "\n",
        "x_scaler = StandardScaler()\n",
        "X_train = train_df[features].copy()\n",
        "X_test = test_df[features].copy()\n",
        "\n",
        "X_train[feat_used] =  x_scaler.fit_transform(X_train[feat_used])\n",
        "X_test[feat_used] =  x_scaler.transform(X_test[feat_used])\n",
        "\n",
        "X_train_df = pd.DataFrame(index=train_df.index, columns=features, data=X_train)\n",
        "X_test_df = pd.DataFrame(index=test_df.index, columns=features, data=X_test)\n",
        "\n",
        "X_train_df[\"FloodProbability\"] = y_scaled\n",
        "X_test_df[\"FloodProbability\"] = 0"
      ],
      "metadata": {
        "id": "jukYSMivhD4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_results_df, oof_results = run_experiment(list_nn_models, X_train_df, X_test_df, experiment_name=\"baseline_nn\", splits=5, rs=42, target=\"FloodProbability\",\n",
        "                                              batch_size=128,num_epochs=31, learning_rate=0.0025)"
      ],
      "metadata": {
        "id": "aYZBGE8uf7ia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iUkbTRolt0v"
      },
      "source": [
        "## 7.0 Store and Evaluate Results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIoli9zHZ33q"
      },
      "outputs": [],
      "source": [
        "for k in y_pred_test_final_dict.keys():\n",
        "  print(k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCPpvaMghoq_"
      },
      "outputs": [],
      "source": [
        "for k in y_pred_test_final_dict.keys():\n",
        "  print(f\"OOF Shape {k}: {oof[k].shape}\")\n",
        "  print(f\"{k} Nan: {np.isnan(y_pred_test_final_dict[k]).sum()}\")\n",
        "  print(f\"{k} Min: {y_pred_test_final_dict[k].min()}\")\n",
        "\n",
        "members = [name for name in oof.keys() if 'Stack' not in name]\n",
        "X = np.column_stack([oof[name] for name in members])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klYwawn8i5ii"
      },
      "outputs": [],
      "source": [
        "sub_res_final={}\n",
        "for k in y_pred_test_final_dict.keys():\n",
        "  print(f\"OOF Shape {k}: {oof[k].shape}\")\n",
        "  print(f\"Test {k} Nan: {np.isnan(y_pred_test_final_dict[k]).sum()}\")\n",
        "  sub_res_final[k] = create_sub_files(df_results=y_pred_test_final_dict[k], scaler=y_scaler, oof_results=oof[k], experiment_name = f\"{k}\", folder_data = f\"/content/drive/MyDrive/Exercises/Studies_Structured_Data//Data/S4E5_Flood\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mrAYUJB9e_s"
      },
      "outputs": [],
      "source": [
        "target_groups = train_df.copy()\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(data=sub_res_final[\"LGBM_v1\"], x='FloodProbability', kde=True,\n",
        "             stat=\"percent\"\n",
        "              )\n",
        "plt.title('Distribution of FloodProbability', fontsize=15)\n",
        "plt.xlabel('FloodProbability')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(True)\n",
        "#plt.yscale('log')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMHUJ7tCk28D"
      },
      "outputs": [],
      "source": [
        "plt.scatter(y_pred_test_final_dict[\"LGBM_v1\"], y_pred_test_final_dict[\"Stack_ridge_v1\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-8hWUUVaEM4"
      },
      "source": [
        "##### * **RIDGE V1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uubEJpf63BTo"
      },
      "outputs": [],
      "source": [
        "members = [name for name in oof.keys() if 'Stack' not in name]\n",
        "\n",
        "X = np.column_stack([oof[name] for name in members])\n",
        "model_r1 = Ridge(positive=True,alpha=1)\n",
        "\n",
        "model_r1.fit(X, X_train_df.FloodProbability)\n",
        "print('Ensemble weights')\n",
        "weights = pd.Series(model_r1.coef_, index=members)\n",
        "print(weights)\n",
        "print('Total weight:', weights.sum())\n",
        "print('Intercept:', model_r1.intercept_)\n",
        "oof['Stack_ridge'] = model_r1.predict(X) # not really out-of-fold...\n",
        "print(f\"Score: {r2_score(X_train_df.FloodProbability, oof['Stack_ridge']):.5f}\")\n",
        "\n",
        "# Pie chart\n",
        "weights = weights[weights > 0]\n",
        "plt.pie(weights, labels=weights.index, autopct=\"%.0f%%\")\n",
        "plt.title('Ensemble weights')\n",
        "plt.show()\n",
        "\n",
        "# Test predictions\n",
        "if COMPUTE_TEST_PRED:\n",
        "    X = np.column_stack([y_pred_test_final_dict[name] for name in members])\n",
        "    y_pred_test_final_dict['Stack_ridge_v1'] = model_r1.predict(X)\n",
        "\n",
        "del weights\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kE92G-5S9A-k"
      },
      "outputs": [],
      "source": [
        "y_pred_test_final_dict.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9ikEz0W_c7J"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "toc_visible": true,
      "provenance": [],
      "gpuType": "L4",
      "authorship_tag": "ABX9TyNxi7/Mhy24sZWri8XY7ZrF",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}